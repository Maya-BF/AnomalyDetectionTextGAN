{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import helper\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "import math\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading params\n",
    "# FLAGS are command-line arguments to our program.\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "#tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "#tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_boolean(\"enable_word_embeddings\", True, \"Enable/disable the word embedding (default: True)\")\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.4, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 9.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 30, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "tf.flags.DEFINE_float(\"decay_coefficient\", 2.5, \"Decay coefficient (default: 2.5)\")\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "# load config file\n",
    "with open(\"config.yml\", 'r') as ymlfile:\n",
    "    cfg = yaml.load(ymlfile)\n",
    "    \n",
    "dataset_name = cfg[\"datasets\"][\"default\"]\n",
    "if FLAGS.enable_word_embeddings and cfg['word_embeddings']['default'] is not None:\n",
    "    embedding_name = cfg['word_embeddings']['default']\n",
    "    embedding_dimension = cfg['word_embeddings'][embedding_name]['dimension']\n",
    "else:\n",
    "    embedding_dimension = FLAGS.embedding_dim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    #x_text, y = helper.load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "    \n",
    "    datasets = None\n",
    "    if dataset_name == \"mrpolarity\":\n",
    "        datasets = helper.get_datasets_mrpolarity(cfg[\"datasets\"][dataset_name][\"positive_data_file\"][\"path\"],\n",
    "                                                        cfg[\"datasets\"][dataset_name][\"negative_data_file\"][\"path\"])\n",
    "    elif dataset_name == \"20newsgroup\":\n",
    "        datasets = helper.get_datasets_20newsgroup(subset=\"train\", \n",
    "                                                         categories=cfg[\"datasets\"][dataset_name][\"categories\"],\n",
    "                                                         shuffle=cfg[\"datasets\"][dataset_name][\"shuffle\"],\n",
    "                                                         random_state=cfg[\"datasets\"][dataset_name][\"random_state\"])\n",
    "    elif dataset_name == \"localdata\":\n",
    "        datasets = helper.get_datasets_localdata(container_path=cfg[\"datasets\"][dataset_name][\"container_path\"],\n",
    "                                                         categories=cfg[\"datasets\"][dataset_name][\"categories\"],\n",
    "                                                         shuffle=cfg[\"datasets\"][dataset_name][\"shuffle\"],\n",
    "                                                         random_state=cfg[\"datasets\"][dataset_name][\"random_state\"])\n",
    "    x_text, y = helper.load_data_labels(datasets)\n",
    "\n",
    "    # Build vocabulary\n",
    "    max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "    x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "    # Randomly shuffle data\n",
    "    np.random.seed(10)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "    x_shuffled = x[shuffle_indices]\n",
    "    y_shuffled = y[shuffle_indices]\n",
    "\n",
    "    # Split train/test set\n",
    "    # TODO: This is very crude, should use cross-validation\n",
    "    dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "    x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "    y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "\n",
    "    del x, y, x_shuffled, y_shuffled\n",
    "\n",
    "    print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, y_train, vocab_processor, x_dev, y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, vocab_processor, x_dev, y_dev):\n",
    "    # Training\n",
    "    # ==================================================\n",
    "    graph = tf.Graph()\n",
    "    gpu_options=tf.GPUOptions()\n",
    "    gpu_options.allow_growth = True\n",
    "    with graph.as_default():\n",
    "        session_conf = tf.ConfigProto(\n",
    "          allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "          log_device_placement=FLAGS.log_device_placement,\n",
    "          gpu_options=gpu_options)\n",
    "\n",
    "        sess = tf.Session(config= session_conf)\n",
    "        with sess.as_default():\n",
    "            cnn = TextCNN(\n",
    "                sequence_length= x_train.shape[1],\n",
    "                num_classes= y_train.shape[1],\n",
    "                vocab_size= len(vocab_processor.vocabulary_),\n",
    "                #embedding_size=FLAGS.embedding_dim,\n",
    "                embedding_size= embedding_dimension,\n",
    "                filter_sizes= list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                num_filters= FLAGS.num_filters,\n",
    "                l2_reg_lambda= FLAGS.l2_reg_lambda)\n",
    "\n",
    "            # Define Training procedure\n",
    "            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "            optimizer = tf.train.AdamOptimizer(cnn.learning_rate)\n",
    "            grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "            \n",
    "\n",
    "            # Keep track of gradient values and sparsity (optional)\n",
    "            grad_summaries = []\n",
    "            for g, v in grads_and_vars:\n",
    "                if g is not None:\n",
    "                    grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                    sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                    grad_summaries.append(grad_hist_summary)\n",
    "                    grad_summaries.append(sparsity_summary)\n",
    "            grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "            \n",
    "             # Output directory for models and summaries\n",
    "            timestamp = str(int(time.time()))\n",
    "            out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "            print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "            # Summaries for loss and accuracy\n",
    "            loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "            acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "        \n",
    "            # Train Summaries\n",
    "            train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "            train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "            train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "            # Dev summaries'\n",
    "            dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "            dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "            dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "            # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "            checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "            checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "            if not os.path.exists(checkpoint_dir):\n",
    "                os.makedirs(checkpoint_dir)\n",
    "            saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "            # Write vocabulary\n",
    "            vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "            # Initialize all variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Set the embedding\n",
    "            if FLAGS.enable_word_embeddings and cfg['word_embeddings']['default'] is not None:\n",
    "                vocabulary = vocab_processor.vocabulary_\n",
    "                initW = None\n",
    "                if embedding_name == 'word2vec':\n",
    "                    # load embedding vectors from the word2vec\n",
    "                    print(\"Load word2vec file {}\".format(cfg['word_embeddings']['word2vec']['path']))\n",
    "                    initW = helper.load_embedding_vectors_word2vec(vocabulary,\n",
    "                                                                         cfg['word_embeddings']['word2vec']['path'],\n",
    "                                                                         cfg['word_embeddings']['word2vec']['binary'])\n",
    "                    print(\"word2vec file has been loaded\")\n",
    "                elif embedding_name == 'glove':\n",
    "                    # load embedding vectors from the glove\n",
    "                    print(\"Load glove file {}\".format(cfg['word_embeddings']['glove']['path']))\n",
    "                    initW = helper.load_embedding_vectors_glove(vocabulary,\n",
    "                                                                      cfg['word_embeddings']['glove']['path'],\n",
    "                                                                      embedding_dimension)\n",
    "                    print(\"glove file has been loaded\\n\")\n",
    "\n",
    "                sess.run(cnn.W.assign(initW))\n",
    "            \n",
    "            \n",
    "            #def train_step(x_batch, y_batch):\n",
    "            def train_step(x_batch, y_batch, learning_rate):\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: FLAGS.dropout_keep_prob,\n",
    "                  cnn.learning_rate: learning_rate\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy= sess.run(\n",
    "                    [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "\n",
    "               \n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}, learning_rate {:g}\" \n",
    "                      .format(time_str, step, loss, accuracy, learning_rate))\n",
    "                \n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            def dev_step(x_batch, y_batch, writer=None):\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch,\n",
    "                  cnn.input_y: y_batch,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy= sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "                    \n",
    "            # Generate batches\n",
    "            batches = helper.batch_iter(\n",
    "                list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "             # It uses dynamic learning rate with a high value at the beginning to speed up the training\n",
    "            #max_learning_rate = 0.005\n",
    "            #min_learning_rate = 0.0001\n",
    "            max_learning_rate = 0.005\n",
    "            min_learning_rate = 0.0001\n",
    "            decay_speed = FLAGS.decay_coefficient*len(y_train)/FLAGS.batch_size\n",
    "            # Training loop. For each batch...\n",
    "            counter = 0\n",
    "            for batch in batches:\n",
    "                learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-counter/decay_speed)\n",
    "                counter += 1\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                #train_step(x_batch, y_batch)\n",
    "                train_step(x_batch, y_batch, learning_rate)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                \n",
    "                if current_step % FLAGS.evaluate_every == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % FLAGS.checkpoint_every == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    x_train, y_train, vocab_processor, x_dev, y_dev = preprocess()\n",
    "    train(x_train, y_train, vocab_processor, x_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "WARNING:tensorflow:From <ipython-input-3-136d936db4dc>:25: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n",
      "WARNING:tensorflow:From /root/My_Text_Classification_v3/text_cnn.py:111: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/filter_matrix:0/grad/hist is illegal; using conv-maxpool-3/filter_matrix_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/filter_matrix:0/grad/sparsity is illegal; using conv-maxpool-3/filter_matrix_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/filter_matrix:0/grad/hist is illegal; using conv-maxpool-4/filter_matrix_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/filter_matrix:0/grad/sparsity is illegal; using conv-maxpool-4/filter_matrix_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/filter_matrix:0/grad/hist is illegal; using conv-maxpool-5/filter_matrix_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/filter_matrix:0/grad/sparsity is illegal; using conv-maxpool-5/filter_matrix_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to /root/My_Text_Classification_v3/runs/1548728574\n",
      "\n",
      "Load word2vec file data/word_embeddings/GoogleNews-vectors-negative300.bin\n",
      "word2vec file has been loaded\n",
      "2019-01-29T02:23:25.389906: step 1, loss 18.9163, acc 0.4375, learning_rate 0.005\n",
      "2019-01-29T02:23:25.494602: step 2, loss 16.7997, acc 0.46875, learning_rate 0.00498695\n",
      "2019-01-29T02:23:25.625321: step 3, loss 14.9211, acc 0.4375, learning_rate 0.00497393\n",
      "2019-01-29T02:23:25.712101: step 4, loss 13.1543, acc 0.515625, learning_rate 0.00496094\n",
      "2019-01-29T02:23:25.831977: step 5, loss 11.3322, acc 0.5625, learning_rate 0.00494799\n",
      "2019-01-29T02:23:25.956950: step 6, loss 9.90476, acc 0.65625, learning_rate 0.00493507\n",
      "2019-01-29T02:23:26.079079: step 7, loss 8.94007, acc 0.453125, learning_rate 0.00492219\n",
      "2019-01-29T02:23:26.200536: step 8, loss 7.51413, acc 0.609375, learning_rate 0.00490934\n",
      "2019-01-29T02:23:26.286331: step 9, loss 6.51201, acc 0.65625, learning_rate 0.00489653\n",
      "2019-01-29T02:23:26.382317: step 10, loss 5.6414, acc 0.671875, learning_rate 0.00488375\n",
      "2019-01-29T02:23:26.503497: step 11, loss 4.78007, acc 0.640625, learning_rate 0.00487101\n",
      "2019-01-29T02:23:26.582437: step 12, loss 4.18023, acc 0.671875, learning_rate 0.0048583\n",
      "2019-01-29T02:23:26.710650: step 13, loss 3.69395, acc 0.53125, learning_rate 0.00484562\n",
      "2019-01-29T02:23:26.833141: step 14, loss 3.0385, acc 0.640625, learning_rate 0.00483298\n",
      "2019-01-29T02:23:26.958214: step 15, loss 2.6289, acc 0.6875, learning_rate 0.00482037\n",
      "2019-01-29T02:23:27.046100: step 16, loss 2.2663, acc 0.625, learning_rate 0.00480779\n",
      "2019-01-29T02:23:27.171680: step 17, loss 1.96382, acc 0.65625, learning_rate 0.00479525\n",
      "2019-01-29T02:23:27.301002: step 18, loss 1.7621, acc 0.53125, learning_rate 0.00478274\n",
      "2019-01-29T02:23:27.387833: step 19, loss 1.54966, acc 0.5625, learning_rate 0.00477026\n",
      "2019-01-29T02:23:27.476181: step 20, loss 1.38004, acc 0.609375, learning_rate 0.00475782\n",
      "2019-01-29T02:23:27.604539: step 21, loss 1.20721, acc 0.65625, learning_rate 0.00474541\n",
      "2019-01-29T02:23:27.730641: step 22, loss 1.22516, acc 0.5625, learning_rate 0.00473303\n",
      "2019-01-29T02:23:27.853597: step 23, loss 1.1607, acc 0.453125, learning_rate 0.00472069\n",
      "2019-01-29T02:23:27.943891: step 24, loss 1.0033, acc 0.53125, learning_rate 0.00470838\n",
      "2019-01-29T02:23:28.060881: step 25, loss 0.946989, acc 0.59375, learning_rate 0.0046961\n",
      "2019-01-29T02:23:28.187318: step 26, loss 0.934495, acc 0.578125, learning_rate 0.00468386\n",
      "2019-01-29T02:23:28.314559: step 27, loss 0.911934, acc 0.609375, learning_rate 0.00467164\n",
      "2019-01-29T02:23:28.446170: step 28, loss 0.952398, acc 0.53125, learning_rate 0.00465946\n",
      "2019-01-29T02:23:28.572698: step 29, loss 0.96128, acc 0.484375, learning_rate 0.00464732\n",
      "2019-01-29T02:23:28.696034: step 30, loss 0.909026, acc 0.46875, learning_rate 0.0046352\n",
      "2019-01-29T02:23:28.829416: step 31, loss 0.866582, acc 0.515625, learning_rate 0.00462312\n",
      "2019-01-29T02:23:28.953269: step 32, loss 0.886022, acc 0.5625, learning_rate 0.00461107\n",
      "2019-01-29T02:23:29.081718: step 33, loss 1.06316, acc 0.421875, learning_rate 0.00459905\n",
      "2019-01-29T02:23:29.198849: step 34, loss 1.02772, acc 0.453125, learning_rate 0.00458706\n",
      "2019-01-29T02:23:29.288376: step 35, loss 0.906068, acc 0.453125, learning_rate 0.00457511\n",
      "2019-01-29T02:23:29.414642: step 36, loss 0.913388, acc 0.34375, learning_rate 0.00456319\n",
      "2019-01-29T02:23:29.541596: step 37, loss 0.819231, acc 0.609375, learning_rate 0.0045513\n",
      "2019-01-29T02:23:29.667859: step 38, loss 0.937099, acc 0.5, learning_rate 0.00453944\n",
      "2019-01-29T02:23:29.798249: step 39, loss 0.968874, acc 0.46875, learning_rate 0.00452761\n",
      "2019-01-29T02:23:29.925682: step 40, loss 0.892889, acc 0.46875, learning_rate 0.00451581\n",
      "2019-01-29T02:23:30.049141: step 41, loss 0.838992, acc 0.5, learning_rate 0.00450405\n",
      "2019-01-29T02:23:30.176910: step 42, loss 0.898076, acc 0.4375, learning_rate 0.00449231\n",
      "2019-01-29T02:23:30.303747: step 43, loss 0.885951, acc 0.515625, learning_rate 0.00448061\n",
      "2019-01-29T02:23:30.426843: step 44, loss 0.84702, acc 0.53125, learning_rate 0.00446894\n",
      "2019-01-29T02:23:30.552218: step 45, loss 0.836495, acc 0.46875, learning_rate 0.0044573\n",
      "2019-01-29T02:23:30.640333: step 46, loss 0.797532, acc 0.46875, learning_rate 0.00444569\n",
      "2019-01-29T02:23:30.766828: step 47, loss 0.758272, acc 0.640625, learning_rate 0.00443411\n",
      "2019-01-29T02:23:30.889285: step 48, loss 0.843587, acc 0.453125, learning_rate 0.00442257\n",
      "2019-01-29T02:23:31.002793: step 49, loss 0.811869, acc 0.4375, learning_rate 0.00441105\n",
      "2019-01-29T02:23:31.126729: step 50, loss 0.798951, acc 0.421875, learning_rate 0.00439957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:23:31.246797: step 51, loss 0.748306, acc 0.625, learning_rate 0.00438811\n",
      "2019-01-29T02:23:31.376475: step 52, loss 0.713672, acc 0.640625, learning_rate 0.00437669\n",
      "2019-01-29T02:23:31.505947: step 53, loss 0.808519, acc 0.40625, learning_rate 0.00436529\n",
      "2019-01-29T02:23:31.592396: step 54, loss 0.758749, acc 0.5, learning_rate 0.00435393\n",
      "2019-01-29T02:23:31.712962: step 55, loss 0.751516, acc 0.5, learning_rate 0.0043426\n",
      "2019-01-29T02:23:31.836430: step 56, loss 0.730365, acc 0.515625, learning_rate 0.00433129\n",
      "2019-01-29T02:23:31.958955: step 57, loss 0.692783, acc 0.734375, learning_rate 0.00432002\n",
      "2019-01-29T02:23:32.083924: step 58, loss 0.773755, acc 0.5, learning_rate 0.00430878\n",
      "2019-01-29T02:23:32.214298: step 59, loss 0.80088, acc 0.484375, learning_rate 0.00429756\n",
      "2019-01-29T02:23:32.314874: step 60, loss 0.705145, acc 0.625, learning_rate 0.00428638\n",
      "2019-01-29T02:23:32.399505: step 61, loss 0.691355, acc 0.625, learning_rate 0.00427523\n",
      "2019-01-29T02:23:32.498721: step 62, loss 0.683531, acc 0.625, learning_rate 0.0042641\n",
      "2019-01-29T02:23:32.626799: step 63, loss 0.640021, acc 0.78125, learning_rate 0.00425301\n",
      "2019-01-29T02:23:32.711336: step 64, loss 0.654831, acc 0.625, learning_rate 0.00424194\n",
      "2019-01-29T02:23:32.836548: step 65, loss 0.701238, acc 0.59375, learning_rate 0.00423091\n",
      "2019-01-29T02:23:32.967312: step 66, loss 0.753398, acc 0.53125, learning_rate 0.0042199\n",
      "2019-01-29T02:23:33.057009: step 67, loss 0.822163, acc 0.453125, learning_rate 0.00420893\n",
      "2019-01-29T02:23:33.175049: step 68, loss 0.704528, acc 0.53125, learning_rate 0.00419798\n",
      "2019-01-29T02:23:33.301378: step 69, loss 0.666861, acc 0.75, learning_rate 0.00418706\n",
      "2019-01-29T02:23:33.431048: step 70, loss 0.686207, acc 0.625, learning_rate 0.00417617\n",
      "2019-01-29T02:23:33.559496: step 71, loss 0.674491, acc 0.65625, learning_rate 0.00416531\n",
      "2019-01-29T02:23:33.690479: step 72, loss 0.748397, acc 0.53125, learning_rate 0.00415448\n",
      "2019-01-29T02:23:33.817539: step 73, loss 0.768155, acc 0.5, learning_rate 0.00414368\n",
      "2019-01-29T02:23:33.948108: step 74, loss 0.731511, acc 0.625, learning_rate 0.00413291\n",
      "2019-01-29T02:23:34.079090: step 75, loss 0.666533, acc 0.71875, learning_rate 0.00412216\n",
      "2019-01-29T02:23:34.205308: step 76, loss 0.633876, acc 0.703125, learning_rate 0.00411145\n",
      "2019-01-29T02:23:34.299301: step 77, loss 0.665213, acc 0.609375, learning_rate 0.00410076\n",
      "2019-01-29T02:23:34.425635: step 78, loss 0.732523, acc 0.546875, learning_rate 0.0040901\n",
      "2019-01-29T02:23:34.548213: step 79, loss 0.726293, acc 0.515625, learning_rate 0.00407947\n",
      "2019-01-29T02:23:34.671941: step 80, loss 0.695811, acc 0.609375, learning_rate 0.00406887\n",
      "2019-01-29T02:23:34.794177: step 81, loss 0.673007, acc 0.6875, learning_rate 0.00405829\n",
      "2019-01-29T02:23:34.917514: step 82, loss 0.633792, acc 0.765625, learning_rate 0.00404775\n",
      "2019-01-29T02:23:35.040395: step 83, loss 0.637051, acc 0.703125, learning_rate 0.00403723\n",
      "2019-01-29T02:23:35.165192: step 84, loss 0.72983, acc 0.5, learning_rate 0.00402674\n",
      "2019-01-29T02:23:35.289567: step 85, loss 0.702405, acc 0.59375, learning_rate 0.00401628\n",
      "2019-01-29T02:23:35.418311: step 86, loss 0.702929, acc 0.625, learning_rate 0.00400584\n",
      "2019-01-29T02:23:35.547501: step 87, loss 0.646715, acc 0.71875, learning_rate 0.00399544\n",
      "2019-01-29T02:23:35.674926: step 88, loss 0.60691, acc 0.796875, learning_rate 0.00398506\n",
      "2019-01-29T02:23:35.800409: step 89, loss 0.643283, acc 0.734375, learning_rate 0.00397471\n",
      "2019-01-29T02:23:35.925156: step 90, loss 0.670619, acc 0.625, learning_rate 0.00396439\n",
      "2019-01-29T02:23:36.045118: step 91, loss 0.667302, acc 0.65625, learning_rate 0.00395409\n",
      "2019-01-29T02:23:36.172746: step 92, loss 0.8159, acc 0.484375, learning_rate 0.00394382\n",
      "2019-01-29T02:23:36.300451: step 93, loss 0.666961, acc 0.671875, learning_rate 0.00393358\n",
      "2019-01-29T02:23:36.388576: step 94, loss 0.636678, acc 0.734375, learning_rate 0.00392337\n",
      "2019-01-29T02:23:36.513390: step 95, loss 0.613901, acc 0.8125, learning_rate 0.00391318\n",
      "2019-01-29T02:23:36.635620: step 96, loss 0.677909, acc 0.625, learning_rate 0.00390302\n",
      "2019-01-29T02:23:36.733837: step 97, loss 0.670206, acc 0.625, learning_rate 0.00389289\n",
      "2019-01-29T02:23:36.861030: step 98, loss 0.706605, acc 0.609375, learning_rate 0.00388279\n",
      "2019-01-29T02:23:36.986233: step 99, loss 0.610928, acc 0.671875, learning_rate 0.00387271\n",
      "2019-01-29T02:23:37.115558: step 100, loss 0.597643, acc 0.78125, learning_rate 0.00386266\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:23:37.398294: step 100, loss 0.62451, acc 0.736398\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-100\n",
      "\n",
      "2019-01-29T02:23:37.815351: step 101, loss 0.626932, acc 0.765625, learning_rate 0.00385263\n",
      "2019-01-29T02:23:37.936200: step 102, loss 0.585949, acc 0.734375, learning_rate 0.00384263\n",
      "2019-01-29T02:23:38.064840: step 103, loss 0.630938, acc 0.65625, learning_rate 0.00383266\n",
      "2019-01-29T02:23:38.191993: step 104, loss 0.624159, acc 0.6875, learning_rate 0.00382272\n",
      "2019-01-29T02:23:38.313229: step 105, loss 0.569832, acc 0.765625, learning_rate 0.0038128\n",
      "2019-01-29T02:23:38.441071: step 106, loss 0.579848, acc 0.8125, learning_rate 0.00380291\n",
      "2019-01-29T02:23:38.562303: step 107, loss 0.595243, acc 0.765625, learning_rate 0.00379304\n",
      "2019-01-29T02:23:38.688954: step 108, loss 0.580559, acc 0.828125, learning_rate 0.0037832\n",
      "2019-01-29T02:23:38.815756: step 109, loss 0.583927, acc 0.8125, learning_rate 0.00377339\n",
      "2019-01-29T02:23:38.934939: step 110, loss 0.624136, acc 0.71875, learning_rate 0.0037636\n",
      "2019-01-29T02:23:39.019206: step 111, loss 0.576209, acc 0.75, learning_rate 0.00375384\n",
      "2019-01-29T02:23:39.105331: step 112, loss 0.640886, acc 0.6875, learning_rate 0.00374411\n",
      "2019-01-29T02:23:39.231549: step 113, loss 0.598681, acc 0.75, learning_rate 0.0037344\n",
      "2019-01-29T02:23:39.355008: step 114, loss 0.549612, acc 0.734375, learning_rate 0.00372472\n",
      "2019-01-29T02:23:39.483109: step 115, loss 0.621166, acc 0.6875, learning_rate 0.00371506\n",
      "2019-01-29T02:23:39.610341: step 116, loss 0.581268, acc 0.71875, learning_rate 0.00370543\n",
      "2019-01-29T02:23:39.739238: step 117, loss 0.701405, acc 0.578125, learning_rate 0.00369582\n",
      "2019-01-29T02:23:39.865373: step 118, loss 0.616862, acc 0.6875, learning_rate 0.00368624\n",
      "2019-01-29T02:23:39.991836: step 119, loss 0.647759, acc 0.6875, learning_rate 0.00367669\n",
      "2019-01-29T02:23:40.119775: step 120, loss 0.584448, acc 0.78125, learning_rate 0.00366716\n",
      "2019-01-29T02:23:40.207714: step 121, loss 0.552069, acc 0.828125, learning_rate 0.00365766\n",
      "2019-01-29T02:23:40.308789: step 122, loss 0.603227, acc 0.71875, learning_rate 0.00364818\n",
      "2019-01-29T02:23:40.438422: step 123, loss 0.610816, acc 0.71875, learning_rate 0.00363872\n",
      "2019-01-29T02:23:40.564589: step 124, loss 0.546492, acc 0.796875, learning_rate 0.0036293\n",
      "2019-01-29T02:23:40.686705: step 125, loss 0.60144, acc 0.71875, learning_rate 0.00361989\n",
      "2019-01-29T02:23:40.774416: step 126, loss 0.586811, acc 0.71875, learning_rate 0.00361052\n",
      "2019-01-29T02:23:40.902920: step 127, loss 0.570645, acc 0.828125, learning_rate 0.00360116\n",
      "2019-01-29T02:23:41.001453: step 128, loss 0.617579, acc 0.734375, learning_rate 0.00359183\n",
      "2019-01-29T02:23:41.130396: step 129, loss 0.591317, acc 0.65625, learning_rate 0.00358253\n",
      "2019-01-29T02:23:41.221021: step 130, loss 0.598713, acc 0.75, learning_rate 0.00357325\n",
      "2019-01-29T02:23:41.350765: step 131, loss 0.579208, acc 0.78125, learning_rate 0.003564\n",
      "2019-01-29T02:23:41.556203: step 132, loss 0.651765, acc 0.71875, learning_rate 0.00355477\n",
      "2019-01-29T02:23:41.679758: step 133, loss 0.557237, acc 0.8125, learning_rate 0.00354557\n",
      "2019-01-29T02:23:41.806651: step 134, loss 0.576058, acc 0.703125, learning_rate 0.00353639\n",
      "2019-01-29T02:23:41.935101: step 135, loss 0.553687, acc 0.796875, learning_rate 0.00352723\n",
      "2019-01-29T02:23:42.062603: step 136, loss 0.535974, acc 0.75, learning_rate 0.0035181\n",
      "2019-01-29T02:23:42.186648: step 137, loss 0.491842, acc 0.8125, learning_rate 0.00350899\n",
      "2019-01-29T02:23:42.310081: step 138, loss 0.57606, acc 0.796875, learning_rate 0.00349991\n",
      "2019-01-29T02:23:42.438349: step 139, loss 0.567893, acc 0.796875, learning_rate 0.00349085\n",
      "2019-01-29T02:23:42.568189: step 140, loss 0.477805, acc 0.828125, learning_rate 0.00348182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:23:42.693031: step 141, loss 0.601335, acc 0.703125, learning_rate 0.00347281\n",
      "2019-01-29T02:23:42.823091: step 142, loss 0.534288, acc 0.765625, learning_rate 0.00346382\n",
      "2019-01-29T02:23:42.948680: step 143, loss 0.626747, acc 0.703125, learning_rate 0.00345486\n",
      "2019-01-29T02:23:43.080232: step 144, loss 0.566446, acc 0.765625, learning_rate 0.00344592\n",
      "2019-01-29T02:23:43.212349: step 145, loss 0.553222, acc 0.75, learning_rate 0.00343701\n",
      "2019-01-29T02:23:43.301728: step 146, loss 0.548275, acc 0.796875, learning_rate 0.00342812\n",
      "2019-01-29T02:23:43.428921: step 147, loss 0.563537, acc 0.75, learning_rate 0.00341925\n",
      "2019-01-29T02:23:43.557194: step 148, loss 0.570066, acc 0.75, learning_rate 0.00341041\n",
      "2019-01-29T02:23:43.681350: step 149, loss 0.591084, acc 0.734375, learning_rate 0.00340159\n",
      "2019-01-29T02:23:43.906152: step 150, loss 0.505481, acc 0.833333, learning_rate 0.00339279\n",
      "2019-01-29T02:23:44.025025: step 151, loss 0.52145, acc 0.84375, learning_rate 0.00338402\n",
      "2019-01-29T02:23:44.105122: step 152, loss 0.430798, acc 0.875, learning_rate 0.00337527\n",
      "2019-01-29T02:23:44.234486: step 153, loss 0.481646, acc 0.859375, learning_rate 0.00336655\n",
      "2019-01-29T02:23:44.321926: step 154, loss 0.53549, acc 0.796875, learning_rate 0.00335784\n",
      "2019-01-29T02:23:44.446576: step 155, loss 0.445475, acc 0.875, learning_rate 0.00334916\n",
      "2019-01-29T02:23:44.554912: step 156, loss 0.457909, acc 0.90625, learning_rate 0.00334051\n",
      "2019-01-29T02:23:44.677855: step 157, loss 0.441136, acc 0.859375, learning_rate 0.00333187\n",
      "2019-01-29T02:23:44.799677: step 158, loss 0.408908, acc 0.90625, learning_rate 0.00332326\n",
      "2019-01-29T02:23:44.925086: step 159, loss 0.431802, acc 0.875, learning_rate 0.00331467\n",
      "2019-01-29T02:23:45.046003: step 160, loss 0.482548, acc 0.8125, learning_rate 0.00330611\n",
      "2019-01-29T02:23:45.132810: step 161, loss 0.445563, acc 0.828125, learning_rate 0.00329757\n",
      "2019-01-29T02:23:45.255508: step 162, loss 0.406349, acc 0.859375, learning_rate 0.00328905\n",
      "2019-01-29T02:23:45.376688: step 163, loss 0.417999, acc 0.90625, learning_rate 0.00328055\n",
      "2019-01-29T02:23:45.495108: step 164, loss 0.430116, acc 0.890625, learning_rate 0.00327208\n",
      "2019-01-29T02:23:45.588737: step 165, loss 0.448971, acc 0.828125, learning_rate 0.00326363\n",
      "2019-01-29T02:23:45.675199: step 166, loss 0.576784, acc 0.796875, learning_rate 0.0032552\n",
      "2019-01-29T02:23:45.804722: step 167, loss 0.441728, acc 0.84375, learning_rate 0.00324679\n",
      "2019-01-29T02:23:45.924324: step 168, loss 0.440538, acc 0.828125, learning_rate 0.00323841\n",
      "2019-01-29T02:23:46.061562: step 169, loss 0.423371, acc 0.84375, learning_rate 0.00323005\n",
      "2019-01-29T02:23:46.193037: step 170, loss 0.447495, acc 0.875, learning_rate 0.00322171\n",
      "2019-01-29T02:23:46.318949: step 171, loss 0.445406, acc 0.859375, learning_rate 0.00321339\n",
      "2019-01-29T02:23:46.440455: step 172, loss 0.513651, acc 0.765625, learning_rate 0.0032051\n",
      "2019-01-29T02:23:46.536580: step 173, loss 0.455582, acc 0.84375, learning_rate 0.00319682\n",
      "2019-01-29T02:23:46.621185: step 174, loss 0.426677, acc 0.859375, learning_rate 0.00318857\n",
      "2019-01-29T02:23:46.707983: step 175, loss 0.425467, acc 0.84375, learning_rate 0.00318035\n",
      "2019-01-29T02:23:46.839005: step 176, loss 0.485172, acc 0.859375, learning_rate 0.00317214\n",
      "2019-01-29T02:23:46.927570: step 177, loss 0.362064, acc 0.890625, learning_rate 0.00316395\n",
      "2019-01-29T02:23:47.048236: step 178, loss 0.449203, acc 0.8125, learning_rate 0.00315579\n",
      "2019-01-29T02:23:47.175627: step 179, loss 0.425472, acc 0.859375, learning_rate 0.00314765\n",
      "2019-01-29T02:23:47.300998: step 180, loss 0.41301, acc 0.859375, learning_rate 0.00313953\n",
      "2019-01-29T02:23:47.387769: step 181, loss 0.38063, acc 0.921875, learning_rate 0.00313143\n",
      "2019-01-29T02:23:47.475108: step 182, loss 0.438177, acc 0.828125, learning_rate 0.00312336\n",
      "2019-01-29T02:23:47.596343: step 183, loss 0.371809, acc 0.953125, learning_rate 0.0031153\n",
      "2019-01-29T02:23:47.722544: step 184, loss 0.366319, acc 0.90625, learning_rate 0.00310727\n",
      "2019-01-29T02:23:47.848491: step 185, loss 0.461996, acc 0.828125, learning_rate 0.00309926\n",
      "2019-01-29T02:23:47.970568: step 186, loss 0.400709, acc 0.875, learning_rate 0.00309126\n",
      "2019-01-29T02:23:48.101792: step 187, loss 0.429394, acc 0.828125, learning_rate 0.0030833\n",
      "2019-01-29T02:23:48.226988: step 188, loss 0.473644, acc 0.875, learning_rate 0.00307535\n",
      "2019-01-29T02:23:48.349970: step 189, loss 0.404709, acc 0.875, learning_rate 0.00306742\n",
      "2019-01-29T02:23:48.470516: step 190, loss 0.436014, acc 0.875, learning_rate 0.00305951\n",
      "2019-01-29T02:23:48.596027: step 191, loss 0.421399, acc 0.859375, learning_rate 0.00305163\n",
      "2019-01-29T02:23:48.722124: step 192, loss 0.516664, acc 0.84375, learning_rate 0.00304377\n",
      "2019-01-29T02:23:48.809314: step 193, loss 0.395271, acc 0.875, learning_rate 0.00303592\n",
      "2019-01-29T02:23:48.935576: step 194, loss 0.432425, acc 0.828125, learning_rate 0.0030281\n",
      "2019-01-29T02:23:49.056073: step 195, loss 0.371998, acc 0.90625, learning_rate 0.0030203\n",
      "2019-01-29T02:23:49.179978: step 196, loss 0.379443, acc 0.859375, learning_rate 0.00301252\n",
      "2019-01-29T02:23:49.300176: step 197, loss 0.392259, acc 0.890625, learning_rate 0.00300476\n",
      "2019-01-29T02:23:49.425412: step 198, loss 0.464893, acc 0.78125, learning_rate 0.00299702\n",
      "2019-01-29T02:23:49.554074: step 199, loss 0.334001, acc 0.921875, learning_rate 0.0029893\n",
      "2019-01-29T02:23:49.685793: step 200, loss 0.399459, acc 0.84375, learning_rate 0.0029816\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:23:49.712828: step 200, loss 0.530107, acc 0.782364\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-200\n",
      "\n",
      "2019-01-29T02:23:50.011286: step 201, loss 0.476896, acc 0.765625, learning_rate 0.00297393\n",
      "2019-01-29T02:23:50.098442: step 202, loss 0.527745, acc 0.78125, learning_rate 0.00296627\n",
      "2019-01-29T02:23:50.226805: step 203, loss 0.462353, acc 0.828125, learning_rate 0.00295863\n",
      "2019-01-29T02:23:50.350046: step 204, loss 0.463377, acc 0.859375, learning_rate 0.00295102\n",
      "2019-01-29T02:23:50.474770: step 205, loss 0.440061, acc 0.84375, learning_rate 0.00294342\n",
      "2019-01-29T02:23:50.602446: step 206, loss 0.407248, acc 0.890625, learning_rate 0.00293585\n",
      "2019-01-29T02:23:50.729572: step 207, loss 0.354911, acc 0.90625, learning_rate 0.00292829\n",
      "2019-01-29T02:23:50.855983: step 208, loss 0.396812, acc 0.875, learning_rate 0.00292076\n",
      "2019-01-29T02:23:50.987453: step 209, loss 0.443945, acc 0.8125, learning_rate 0.00291324\n",
      "2019-01-29T02:23:51.115380: step 210, loss 0.494774, acc 0.796875, learning_rate 0.00290575\n",
      "2019-01-29T02:23:51.204917: step 211, loss 0.370809, acc 0.890625, learning_rate 0.00289827\n",
      "2019-01-29T02:23:51.328059: step 212, loss 0.426853, acc 0.859375, learning_rate 0.00289082\n",
      "2019-01-29T02:23:51.454224: step 213, loss 0.483408, acc 0.828125, learning_rate 0.00288338\n",
      "2019-01-29T02:23:51.557517: step 214, loss 0.561223, acc 0.75, learning_rate 0.00287597\n",
      "2019-01-29T02:23:51.645999: step 215, loss 0.422051, acc 0.8125, learning_rate 0.00286857\n",
      "2019-01-29T02:23:51.771876: step 216, loss 0.401826, acc 0.84375, learning_rate 0.00286119\n",
      "2019-01-29T02:23:51.902588: step 217, loss 0.398486, acc 0.859375, learning_rate 0.00285384\n",
      "2019-01-29T02:23:51.994029: step 218, loss 0.505494, acc 0.796875, learning_rate 0.0028465\n",
      "2019-01-29T02:23:52.123258: step 219, loss 0.469744, acc 0.8125, learning_rate 0.00283918\n",
      "2019-01-29T02:23:52.213568: step 220, loss 0.435875, acc 0.859375, learning_rate 0.00283188\n",
      "2019-01-29T02:23:52.340855: step 221, loss 0.553788, acc 0.765625, learning_rate 0.00282461\n",
      "2019-01-29T02:23:52.466295: step 222, loss 0.478972, acc 0.78125, learning_rate 0.00281735\n",
      "2019-01-29T02:23:52.570642: step 223, loss 0.397795, acc 0.890625, learning_rate 0.00281011\n",
      "2019-01-29T02:23:52.700878: step 224, loss 0.435661, acc 0.84375, learning_rate 0.00280289\n",
      "2019-01-29T02:23:52.823558: step 225, loss 0.506137, acc 0.78125, learning_rate 0.00279569\n",
      "2019-01-29T02:23:52.947403: step 226, loss 0.406274, acc 0.875, learning_rate 0.0027885\n",
      "2019-01-29T02:23:53.075348: step 227, loss 0.439969, acc 0.828125, learning_rate 0.00278134\n",
      "2019-01-29T02:23:53.161958: step 228, loss 0.471575, acc 0.84375, learning_rate 0.0027742\n",
      "2019-01-29T02:23:53.284491: step 229, loss 0.452667, acc 0.796875, learning_rate 0.00276707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:23:53.407419: step 230, loss 0.293404, acc 0.953125, learning_rate 0.00275997\n",
      "2019-01-29T02:23:53.532529: step 231, loss 0.468007, acc 0.8125, learning_rate 0.00275288\n",
      "2019-01-29T02:23:53.618584: step 232, loss 0.448845, acc 0.84375, learning_rate 0.00274581\n",
      "2019-01-29T02:23:53.717664: step 233, loss 0.403467, acc 0.921875, learning_rate 0.00273876\n",
      "2019-01-29T02:23:53.841982: step 234, loss 0.415909, acc 0.84375, learning_rate 0.00273173\n",
      "2019-01-29T02:23:53.973085: step 235, loss 0.491647, acc 0.828125, learning_rate 0.00272472\n",
      "2019-01-29T02:23:54.097442: step 236, loss 0.56031, acc 0.78125, learning_rate 0.00271773\n",
      "2019-01-29T02:23:54.187579: step 237, loss 0.346636, acc 0.890625, learning_rate 0.00271076\n",
      "2019-01-29T02:23:54.315091: step 238, loss 0.450873, acc 0.8125, learning_rate 0.0027038\n",
      "2019-01-29T02:23:54.438560: step 239, loss 0.381205, acc 0.890625, learning_rate 0.00269686\n",
      "2019-01-29T02:23:54.564781: step 240, loss 0.42063, acc 0.828125, learning_rate 0.00268994\n",
      "2019-01-29T02:23:54.691251: step 241, loss 0.383444, acc 0.875, learning_rate 0.00268304\n",
      "2019-01-29T02:23:54.815592: step 242, loss 0.459022, acc 0.8125, learning_rate 0.00267616\n",
      "2019-01-29T02:23:54.949495: step 243, loss 0.382597, acc 0.84375, learning_rate 0.0026693\n",
      "2019-01-29T02:23:55.038244: step 244, loss 0.333519, acc 0.921875, learning_rate 0.00266245\n",
      "2019-01-29T02:23:55.161988: step 245, loss 0.405669, acc 0.828125, learning_rate 0.00265563\n",
      "2019-01-29T02:23:55.283616: step 246, loss 0.435347, acc 0.84375, learning_rate 0.00264882\n",
      "2019-01-29T02:23:55.374098: step 247, loss 0.476275, acc 0.84375, learning_rate 0.00264203\n",
      "2019-01-29T02:23:55.493589: step 248, loss 0.448434, acc 0.828125, learning_rate 0.00263525\n",
      "2019-01-29T02:23:55.617515: step 249, loss 0.484656, acc 0.8125, learning_rate 0.0026285\n",
      "2019-01-29T02:23:55.740479: step 250, loss 0.455276, acc 0.859375, learning_rate 0.00262176\n",
      "2019-01-29T02:23:55.871991: step 251, loss 0.344545, acc 0.9375, learning_rate 0.00261504\n",
      "2019-01-29T02:23:55.995467: step 252, loss 0.495527, acc 0.84375, learning_rate 0.00260834\n",
      "2019-01-29T02:23:56.120163: step 253, loss 0.452627, acc 0.84375, learning_rate 0.00260166\n",
      "2019-01-29T02:23:56.252063: step 254, loss 0.342857, acc 0.90625, learning_rate 0.002595\n",
      "2019-01-29T02:23:56.376568: step 255, loss 0.32581, acc 0.890625, learning_rate 0.00258835\n",
      "2019-01-29T02:23:56.503292: step 256, loss 0.383824, acc 0.890625, learning_rate 0.00258172\n",
      "2019-01-29T02:23:56.629222: step 257, loss 0.357098, acc 0.921875, learning_rate 0.00257511\n",
      "2019-01-29T02:23:56.750975: step 258, loss 0.471432, acc 0.796875, learning_rate 0.00256851\n",
      "2019-01-29T02:23:56.843998: step 259, loss 0.461413, acc 0.8125, learning_rate 0.00256194\n",
      "2019-01-29T02:23:56.972453: step 260, loss 0.341218, acc 0.890625, learning_rate 0.00255538\n",
      "2019-01-29T02:23:57.099089: step 261, loss 0.496281, acc 0.8125, learning_rate 0.00254884\n",
      "2019-01-29T02:23:57.224167: step 262, loss 0.360362, acc 0.859375, learning_rate 0.00254231\n",
      "2019-01-29T02:23:57.348700: step 263, loss 0.414815, acc 0.859375, learning_rate 0.00253581\n",
      "2019-01-29T02:23:57.471906: step 264, loss 0.433419, acc 0.78125, learning_rate 0.00252932\n",
      "2019-01-29T02:23:57.596551: step 265, loss 0.537162, acc 0.796875, learning_rate 0.00252284\n",
      "2019-01-29T02:23:57.719733: step 266, loss 0.380989, acc 0.84375, learning_rate 0.00251639\n",
      "2019-01-29T02:23:57.852037: step 267, loss 0.452908, acc 0.8125, learning_rate 0.00250995\n",
      "2019-01-29T02:23:57.978543: step 268, loss 0.37965, acc 0.859375, learning_rate 0.00250353\n",
      "2019-01-29T02:23:58.108088: step 269, loss 0.367817, acc 0.875, learning_rate 0.00249713\n",
      "2019-01-29T02:23:58.232437: step 270, loss 0.4964, acc 0.75, learning_rate 0.00249074\n",
      "2019-01-29T02:23:58.360723: step 271, loss 0.45122, acc 0.796875, learning_rate 0.00248437\n",
      "2019-01-29T02:23:58.490788: step 272, loss 0.493411, acc 0.78125, learning_rate 0.00247802\n",
      "2019-01-29T02:23:58.611527: step 273, loss 0.380228, acc 0.859375, learning_rate 0.00247168\n",
      "2019-01-29T02:23:58.740275: step 274, loss 0.481182, acc 0.8125, learning_rate 0.00246536\n",
      "2019-01-29T02:23:58.867897: step 275, loss 0.427287, acc 0.828125, learning_rate 0.00245906\n",
      "2019-01-29T02:23:58.994499: step 276, loss 0.371639, acc 0.890625, learning_rate 0.00245278\n",
      "2019-01-29T02:23:59.093382: step 277, loss 0.398493, acc 0.90625, learning_rate 0.00244651\n",
      "2019-01-29T02:23:59.213668: step 278, loss 0.551844, acc 0.8125, learning_rate 0.00244026\n",
      "2019-01-29T02:23:59.336586: step 279, loss 0.425629, acc 0.875, learning_rate 0.00243402\n",
      "2019-01-29T02:23:59.465539: step 280, loss 0.439819, acc 0.828125, learning_rate 0.0024278\n",
      "2019-01-29T02:23:59.587357: step 281, loss 0.491361, acc 0.8125, learning_rate 0.0024216\n",
      "2019-01-29T02:23:59.711532: step 282, loss 0.504609, acc 0.78125, learning_rate 0.00241542\n",
      "2019-01-29T02:23:59.800788: step 283, loss 0.548005, acc 0.78125, learning_rate 0.00240925\n",
      "2019-01-29T02:23:59.929058: step 284, loss 0.438966, acc 0.859375, learning_rate 0.0024031\n",
      "2019-01-29T02:24:00.055279: step 285, loss 0.430589, acc 0.859375, learning_rate 0.00239696\n",
      "2019-01-29T02:24:00.140790: step 286, loss 0.427766, acc 0.859375, learning_rate 0.00239084\n",
      "2019-01-29T02:24:00.262828: step 287, loss 0.461081, acc 0.828125, learning_rate 0.00238474\n",
      "2019-01-29T02:24:00.390944: step 288, loss 0.515486, acc 0.8125, learning_rate 0.00237865\n",
      "2019-01-29T02:24:00.477655: step 289, loss 0.456426, acc 0.875, learning_rate 0.00237258\n",
      "2019-01-29T02:24:00.565637: step 290, loss 0.436111, acc 0.84375, learning_rate 0.00236652\n",
      "2019-01-29T02:24:00.686983: step 291, loss 0.402688, acc 0.828125, learning_rate 0.00236049\n",
      "2019-01-29T02:24:00.817567: step 292, loss 0.367892, acc 0.890625, learning_rate 0.00235446\n",
      "2019-01-29T02:24:00.938569: step 293, loss 0.44658, acc 0.828125, learning_rate 0.00234846\n",
      "2019-01-29T02:24:01.064116: step 294, loss 0.584192, acc 0.765625, learning_rate 0.00234247\n",
      "2019-01-29T02:24:01.183264: step 295, loss 0.380814, acc 0.859375, learning_rate 0.00233649\n",
      "2019-01-29T02:24:01.307315: step 296, loss 0.408473, acc 0.921875, learning_rate 0.00233053\n",
      "2019-01-29T02:24:01.436924: step 297, loss 0.401989, acc 0.859375, learning_rate 0.00232459\n",
      "2019-01-29T02:24:01.526859: step 298, loss 0.504911, acc 0.828125, learning_rate 0.00231866\n",
      "2019-01-29T02:24:01.657600: step 299, loss 0.469263, acc 0.8125, learning_rate 0.00231275\n",
      "2019-01-29T02:24:01.779512: step 300, loss 0.341791, acc 0.9, learning_rate 0.00230686\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:24:01.807172: step 300, loss 0.50991, acc 0.788931\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-300\n",
      "\n",
      "2019-01-29T02:24:02.128118: step 301, loss 0.247494, acc 0.953125, learning_rate 0.00230098\n",
      "2019-01-29T02:24:02.257911: step 302, loss 0.273118, acc 0.953125, learning_rate 0.00229511\n",
      "2019-01-29T02:24:02.344145: step 303, loss 0.262026, acc 0.953125, learning_rate 0.00228927\n",
      "2019-01-29T02:24:02.471017: step 304, loss 0.259399, acc 0.9375, learning_rate 0.00228343\n",
      "2019-01-29T02:24:02.596374: step 305, loss 0.241257, acc 0.984375, learning_rate 0.00227762\n",
      "2019-01-29T02:24:02.727282: step 306, loss 0.258121, acc 0.96875, learning_rate 0.00227181\n",
      "2019-01-29T02:24:02.848427: step 307, loss 0.204094, acc 0.984375, learning_rate 0.00226603\n",
      "2019-01-29T02:24:02.977904: step 308, loss 0.272226, acc 0.953125, learning_rate 0.00226026\n",
      "2019-01-29T02:24:03.106118: step 309, loss 0.297277, acc 0.953125, learning_rate 0.0022545\n",
      "2019-01-29T02:24:03.240516: step 310, loss 0.24211, acc 0.9375, learning_rate 0.00224876\n",
      "2019-01-29T02:24:03.329768: step 311, loss 0.273858, acc 0.90625, learning_rate 0.00224304\n",
      "2019-01-29T02:24:03.454559: step 312, loss 0.299965, acc 0.9375, learning_rate 0.00223733\n",
      "2019-01-29T02:24:03.581029: step 313, loss 0.278377, acc 0.953125, learning_rate 0.00223163\n",
      "2019-01-29T02:24:03.669013: step 314, loss 0.338561, acc 0.953125, learning_rate 0.00222595\n",
      "2019-01-29T02:24:03.799950: step 315, loss 0.367459, acc 0.828125, learning_rate 0.00222029\n",
      "2019-01-29T02:24:03.926654: step 316, loss 0.220207, acc 0.984375, learning_rate 0.00221464\n",
      "2019-01-29T02:24:04.014706: step 317, loss 0.269327, acc 0.9375, learning_rate 0.00220901\n",
      "2019-01-29T02:24:04.139555: step 318, loss 0.291954, acc 0.921875, learning_rate 0.00220339\n",
      "2019-01-29T02:24:04.275070: step 319, loss 0.217987, acc 0.96875, learning_rate 0.00219778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:24:04.405965: step 320, loss 0.298715, acc 0.921875, learning_rate 0.0021922\n",
      "2019-01-29T02:24:04.522702: step 321, loss 0.251153, acc 0.9375, learning_rate 0.00218662\n",
      "2019-01-29T02:24:04.644314: step 322, loss 0.270959, acc 0.953125, learning_rate 0.00218106\n",
      "2019-01-29T02:24:04.771196: step 323, loss 0.237219, acc 0.96875, learning_rate 0.00217552\n",
      "2019-01-29T02:24:04.895456: step 324, loss 0.220145, acc 0.96875, learning_rate 0.00216999\n",
      "2019-01-29T02:24:05.021726: step 325, loss 0.305169, acc 0.921875, learning_rate 0.00216447\n",
      "2019-01-29T02:24:05.152231: step 326, loss 0.257592, acc 0.96875, learning_rate 0.00215897\n",
      "2019-01-29T02:24:05.281962: step 327, loss 0.266631, acc 0.953125, learning_rate 0.00215349\n",
      "2019-01-29T02:24:05.409909: step 328, loss 0.190816, acc 1, learning_rate 0.00214802\n",
      "2019-01-29T02:24:05.542903: step 329, loss 0.195713, acc 1, learning_rate 0.00214256\n",
      "2019-01-29T02:24:05.668480: step 330, loss 0.353765, acc 0.90625, learning_rate 0.00213712\n",
      "2019-01-29T02:24:05.794926: step 331, loss 0.24086, acc 0.953125, learning_rate 0.00213169\n",
      "2019-01-29T02:24:05.918207: step 332, loss 0.2767, acc 0.90625, learning_rate 0.00212628\n",
      "2019-01-29T02:24:06.050115: step 333, loss 0.221529, acc 0.953125, learning_rate 0.00212088\n",
      "2019-01-29T02:24:06.175486: step 334, loss 0.244374, acc 0.9375, learning_rate 0.0021155\n",
      "2019-01-29T02:24:06.298679: step 335, loss 0.270753, acc 0.90625, learning_rate 0.00211013\n",
      "2019-01-29T02:24:06.426031: step 336, loss 0.288038, acc 0.921875, learning_rate 0.00210477\n",
      "2019-01-29T02:24:06.549167: step 337, loss 0.286724, acc 0.921875, learning_rate 0.00209943\n",
      "2019-01-29T02:24:06.673879: step 338, loss 0.263366, acc 0.90625, learning_rate 0.0020941\n",
      "2019-01-29T02:24:06.761823: step 339, loss 0.315949, acc 0.921875, learning_rate 0.00208879\n",
      "2019-01-29T02:24:06.850766: step 340, loss 0.29384, acc 0.9375, learning_rate 0.00208349\n",
      "2019-01-29T02:24:06.974941: step 341, loss 0.33843, acc 0.921875, learning_rate 0.00207821\n",
      "2019-01-29T02:24:07.101106: step 342, loss 0.219749, acc 0.953125, learning_rate 0.00207294\n",
      "2019-01-29T02:24:07.191828: step 343, loss 0.265548, acc 0.953125, learning_rate 0.00206768\n",
      "2019-01-29T02:24:07.316894: step 344, loss 0.292338, acc 0.90625, learning_rate 0.00206244\n",
      "2019-01-29T02:24:07.440365: step 345, loss 0.285148, acc 0.921875, learning_rate 0.00205721\n",
      "2019-01-29T02:24:07.564962: step 346, loss 0.237352, acc 0.96875, learning_rate 0.00205199\n",
      "2019-01-29T02:24:07.693381: step 347, loss 0.224153, acc 0.96875, learning_rate 0.00204679\n",
      "2019-01-29T02:24:07.818996: step 348, loss 0.223129, acc 0.953125, learning_rate 0.00204161\n",
      "2019-01-29T02:24:07.944983: step 349, loss 0.249664, acc 0.953125, learning_rate 0.00203643\n",
      "2019-01-29T02:24:08.071709: step 350, loss 0.24834, acc 0.90625, learning_rate 0.00203128\n",
      "2019-01-29T02:24:08.159453: step 351, loss 0.193605, acc 0.984375, learning_rate 0.00202613\n",
      "2019-01-29T02:24:08.288097: step 352, loss 0.243411, acc 0.953125, learning_rate 0.002021\n",
      "2019-01-29T02:24:08.414270: step 353, loss 0.269148, acc 0.921875, learning_rate 0.00201588\n",
      "2019-01-29T02:24:08.547643: step 354, loss 0.349696, acc 0.875, learning_rate 0.00201078\n",
      "2019-01-29T02:24:08.677225: step 355, loss 0.217711, acc 0.984375, learning_rate 0.00200569\n",
      "2019-01-29T02:24:08.765160: step 356, loss 0.297752, acc 0.90625, learning_rate 0.00200061\n",
      "2019-01-29T02:24:08.851627: step 357, loss 0.270763, acc 0.9375, learning_rate 0.00199554\n",
      "2019-01-29T02:24:08.937215: step 358, loss 0.273383, acc 0.90625, learning_rate 0.00199049\n",
      "2019-01-29T02:24:09.058199: step 359, loss 0.215779, acc 0.9375, learning_rate 0.00198546\n",
      "2019-01-29T02:24:09.178367: step 360, loss 0.309122, acc 0.921875, learning_rate 0.00198043\n",
      "2019-01-29T02:24:09.303919: step 361, loss 0.236884, acc 0.96875, learning_rate 0.00197542\n",
      "2019-01-29T02:24:09.435794: step 362, loss 0.230206, acc 0.984375, learning_rate 0.00197043\n",
      "2019-01-29T02:24:09.564220: step 363, loss 0.3073, acc 0.921875, learning_rate 0.00196544\n",
      "2019-01-29T02:24:09.689876: step 364, loss 0.311013, acc 0.875, learning_rate 0.00196047\n",
      "2019-01-29T02:24:09.815073: step 365, loss 0.30812, acc 0.890625, learning_rate 0.00195552\n",
      "2019-01-29T02:24:09.945850: step 366, loss 0.320574, acc 0.875, learning_rate 0.00195057\n",
      "2019-01-29T02:24:10.071454: step 367, loss 0.286102, acc 0.9375, learning_rate 0.00194564\n",
      "2019-01-29T02:24:10.159240: step 368, loss 0.261956, acc 0.9375, learning_rate 0.00194073\n",
      "2019-01-29T02:24:10.280455: step 369, loss 0.323556, acc 0.890625, learning_rate 0.00193582\n",
      "2019-01-29T02:24:10.404282: step 370, loss 0.223562, acc 0.953125, learning_rate 0.00193093\n",
      "2019-01-29T02:24:10.529560: step 371, loss 0.234599, acc 0.9375, learning_rate 0.00192605\n",
      "2019-01-29T02:24:10.619475: step 372, loss 0.206832, acc 1, learning_rate 0.00192119\n",
      "2019-01-29T02:24:10.740166: step 373, loss 0.25118, acc 0.96875, learning_rate 0.00191634\n",
      "2019-01-29T02:24:10.860235: step 374, loss 0.258964, acc 0.953125, learning_rate 0.0019115\n",
      "2019-01-29T02:24:10.978579: step 375, loss 0.363333, acc 0.890625, learning_rate 0.00190667\n",
      "2019-01-29T02:24:11.100455: step 376, loss 0.269918, acc 0.96875, learning_rate 0.00190186\n",
      "2019-01-29T02:24:11.230086: step 377, loss 0.306217, acc 0.890625, learning_rate 0.00189706\n",
      "2019-01-29T02:24:11.353893: step 378, loss 0.263798, acc 0.953125, learning_rate 0.00189227\n",
      "2019-01-29T02:24:11.476403: step 379, loss 0.250837, acc 0.921875, learning_rate 0.00188749\n",
      "2019-01-29T02:24:11.600592: step 380, loss 0.251548, acc 0.953125, learning_rate 0.00188273\n",
      "2019-01-29T02:24:11.722761: step 381, loss 0.2167, acc 0.984375, learning_rate 0.00187798\n",
      "2019-01-29T02:24:11.850614: step 382, loss 0.258079, acc 0.921875, learning_rate 0.00187325\n",
      "2019-01-29T02:24:11.970994: step 383, loss 0.3358, acc 0.890625, learning_rate 0.00186852\n",
      "2019-01-29T02:24:12.059058: step 384, loss 0.257854, acc 0.96875, learning_rate 0.00186381\n",
      "2019-01-29T02:24:12.190209: step 385, loss 0.246423, acc 0.953125, learning_rate 0.00185911\n",
      "2019-01-29T02:24:12.317871: step 386, loss 0.271277, acc 0.9375, learning_rate 0.00185442\n",
      "2019-01-29T02:24:12.439153: step 387, loss 0.377917, acc 0.9375, learning_rate 0.00184975\n",
      "2019-01-29T02:24:12.562286: step 388, loss 0.30405, acc 0.890625, learning_rate 0.00184509\n",
      "2019-01-29T02:24:12.685431: step 389, loss 0.222883, acc 0.953125, learning_rate 0.00184044\n",
      "2019-01-29T02:24:12.810852: step 390, loss 0.261284, acc 0.921875, learning_rate 0.0018358\n",
      "2019-01-29T02:24:12.938600: step 391, loss 0.300313, acc 0.921875, learning_rate 0.00183118\n",
      "2019-01-29T02:24:13.069045: step 392, loss 0.267049, acc 0.9375, learning_rate 0.00182657\n",
      "2019-01-29T02:24:13.195139: step 393, loss 0.206156, acc 0.953125, learning_rate 0.00182197\n",
      "2019-01-29T02:24:13.318429: step 394, loss 0.299301, acc 0.921875, learning_rate 0.00181738\n",
      "2019-01-29T02:24:13.445181: step 395, loss 0.286862, acc 0.921875, learning_rate 0.0018128\n",
      "2019-01-29T02:24:13.565470: step 396, loss 0.368005, acc 0.890625, learning_rate 0.00180824\n",
      "2019-01-29T02:24:13.692100: step 397, loss 0.191367, acc 0.96875, learning_rate 0.00180369\n",
      "2019-01-29T02:24:13.814017: step 398, loss 0.263597, acc 0.921875, learning_rate 0.00179915\n",
      "2019-01-29T02:24:13.905742: step 399, loss 0.39135, acc 0.875, learning_rate 0.00179462\n",
      "2019-01-29T02:24:14.025804: step 400, loss 0.282942, acc 0.90625, learning_rate 0.00179011\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:24:14.053473: step 400, loss 0.512766, acc 0.786116\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-400\n",
      "\n",
      "2019-01-29T02:24:14.341007: step 401, loss 0.287204, acc 0.859375, learning_rate 0.0017856\n",
      "2019-01-29T02:24:14.463562: step 402, loss 0.219608, acc 0.953125, learning_rate 0.00178111\n",
      "2019-01-29T02:24:14.584233: step 403, loss 0.274092, acc 0.921875, learning_rate 0.00177663\n",
      "2019-01-29T02:24:14.708414: step 404, loss 0.359519, acc 0.9375, learning_rate 0.00177217\n",
      "2019-01-29T02:24:14.836888: step 405, loss 0.236803, acc 0.953125, learning_rate 0.00176771\n",
      "2019-01-29T02:24:14.964772: step 406, loss 0.173225, acc 0.984375, learning_rate 0.00176327\n",
      "2019-01-29T02:24:15.089555: step 407, loss 0.278413, acc 0.890625, learning_rate 0.00175884\n",
      "2019-01-29T02:24:15.216486: step 408, loss 0.303369, acc 0.921875, learning_rate 0.00175442\n",
      "2019-01-29T02:24:15.345077: step 409, loss 0.37567, acc 0.921875, learning_rate 0.00175001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:24:15.467648: step 410, loss 0.270222, acc 0.96875, learning_rate 0.00174561\n",
      "2019-01-29T02:24:15.593094: step 411, loss 0.267501, acc 0.953125, learning_rate 0.00174123\n",
      "2019-01-29T02:24:15.726121: step 412, loss 0.204891, acc 0.96875, learning_rate 0.00173686\n",
      "2019-01-29T02:24:15.848345: step 413, loss 0.247153, acc 0.953125, learning_rate 0.0017325\n",
      "2019-01-29T02:24:15.965483: step 414, loss 0.2847, acc 0.9375, learning_rate 0.00172815\n",
      "2019-01-29T02:24:16.095784: step 415, loss 0.316583, acc 0.953125, learning_rate 0.00172381\n",
      "2019-01-29T02:24:16.220497: step 416, loss 0.264584, acc 0.953125, learning_rate 0.00171948\n",
      "2019-01-29T02:24:16.346004: step 417, loss 0.262999, acc 0.90625, learning_rate 0.00171517\n",
      "2019-01-29T02:24:16.467228: step 418, loss 0.22117, acc 0.984375, learning_rate 0.00171087\n",
      "2019-01-29T02:24:16.599229: step 419, loss 0.255209, acc 0.9375, learning_rate 0.00170657\n",
      "2019-01-29T02:24:16.727283: step 420, loss 0.313579, acc 0.921875, learning_rate 0.00170229\n",
      "2019-01-29T02:24:16.854112: step 421, loss 0.27257, acc 0.90625, learning_rate 0.00169802\n",
      "2019-01-29T02:24:16.979949: step 422, loss 0.334543, acc 0.921875, learning_rate 0.00169377\n",
      "2019-01-29T02:24:17.106690: step 423, loss 0.270868, acc 0.9375, learning_rate 0.00168952\n",
      "2019-01-29T02:24:17.197475: step 424, loss 0.215634, acc 0.953125, learning_rate 0.00168529\n",
      "2019-01-29T02:24:17.326763: step 425, loss 0.278626, acc 0.921875, learning_rate 0.00168106\n",
      "2019-01-29T02:24:17.412812: step 426, loss 0.258364, acc 0.90625, learning_rate 0.00167685\n",
      "2019-01-29T02:24:17.539748: step 427, loss 0.378126, acc 0.890625, learning_rate 0.00167265\n",
      "2019-01-29T02:24:17.667579: step 428, loss 0.25117, acc 0.921875, learning_rate 0.00166846\n",
      "2019-01-29T02:24:17.793488: step 429, loss 0.346578, acc 0.921875, learning_rate 0.00166428\n",
      "2019-01-29T02:24:17.919961: step 430, loss 0.233005, acc 0.953125, learning_rate 0.00166011\n",
      "2019-01-29T02:24:18.021598: step 431, loss 0.2823, acc 0.9375, learning_rate 0.00165596\n",
      "2019-01-29T02:24:18.149960: step 432, loss 0.260928, acc 0.921875, learning_rate 0.00165181\n",
      "2019-01-29T02:24:18.274911: step 433, loss 0.189529, acc 0.984375, learning_rate 0.00164768\n",
      "2019-01-29T02:24:18.397184: step 434, loss 0.314828, acc 0.890625, learning_rate 0.00164355\n",
      "2019-01-29T02:24:18.523333: step 435, loss 0.327057, acc 0.890625, learning_rate 0.00163944\n",
      "2019-01-29T02:24:18.651683: step 436, loss 0.350103, acc 0.890625, learning_rate 0.00163534\n",
      "2019-01-29T02:24:18.778686: step 437, loss 0.26625, acc 0.9375, learning_rate 0.00163125\n",
      "2019-01-29T02:24:18.906081: step 438, loss 0.312261, acc 0.890625, learning_rate 0.00162717\n",
      "2019-01-29T02:24:19.029843: step 439, loss 0.247638, acc 0.921875, learning_rate 0.0016231\n",
      "2019-01-29T02:24:19.155928: step 440, loss 0.294845, acc 0.953125, learning_rate 0.00161904\n",
      "2019-01-29T02:24:19.277473: step 441, loss 0.318309, acc 0.875, learning_rate 0.001615\n",
      "2019-01-29T02:24:19.364628: step 442, loss 0.203438, acc 0.953125, learning_rate 0.00161096\n",
      "2019-01-29T02:24:19.496334: step 443, loss 0.27913, acc 0.890625, learning_rate 0.00160693\n",
      "2019-01-29T02:24:19.623386: step 444, loss 0.255498, acc 0.890625, learning_rate 0.00160292\n",
      "2019-01-29T02:24:19.743647: step 445, loss 0.422718, acc 0.90625, learning_rate 0.00159892\n",
      "2019-01-29T02:24:19.870143: step 446, loss 0.267156, acc 0.9375, learning_rate 0.00159492\n",
      "2019-01-29T02:24:19.996219: step 447, loss 0.254607, acc 0.96875, learning_rate 0.00159094\n",
      "2019-01-29T02:24:20.120743: step 448, loss 0.227025, acc 0.96875, learning_rate 0.00158697\n",
      "2019-01-29T02:24:20.254314: step 449, loss 0.320291, acc 0.90625, learning_rate 0.00158301\n",
      "2019-01-29T02:24:20.383029: step 450, loss 0.283521, acc 0.916667, learning_rate 0.00157905\n",
      "2019-01-29T02:24:20.515224: step 451, loss 0.171235, acc 0.984375, learning_rate 0.00157511\n",
      "2019-01-29T02:24:20.600884: step 452, loss 0.284643, acc 0.921875, learning_rate 0.00157118\n",
      "2019-01-29T02:24:20.714271: step 453, loss 0.241009, acc 0.953125, learning_rate 0.00156726\n",
      "2019-01-29T02:24:20.804469: step 454, loss 0.203861, acc 0.953125, learning_rate 0.00156335\n",
      "2019-01-29T02:24:20.929109: step 455, loss 0.148687, acc 0.984375, learning_rate 0.00155946\n",
      "2019-01-29T02:24:21.056086: step 456, loss 0.270881, acc 0.90625, learning_rate 0.00155557\n",
      "2019-01-29T02:24:21.174425: step 457, loss 0.269741, acc 0.9375, learning_rate 0.00155169\n",
      "2019-01-29T02:24:21.301210: step 458, loss 0.19394, acc 0.984375, learning_rate 0.00154782\n",
      "2019-01-29T02:24:21.391402: step 459, loss 0.186551, acc 0.984375, learning_rate 0.00154396\n",
      "2019-01-29T02:24:21.518365: step 460, loss 0.181418, acc 0.96875, learning_rate 0.00154012\n",
      "2019-01-29T02:24:21.606558: step 461, loss 0.202163, acc 0.96875, learning_rate 0.00153628\n",
      "2019-01-29T02:24:21.697016: step 462, loss 0.16286, acc 0.984375, learning_rate 0.00153245\n",
      "2019-01-29T02:24:21.823034: step 463, loss 0.207126, acc 0.953125, learning_rate 0.00152864\n",
      "2019-01-29T02:24:21.907907: step 464, loss 0.217141, acc 0.96875, learning_rate 0.00152483\n",
      "2019-01-29T02:24:21.994187: step 465, loss 0.153803, acc 1, learning_rate 0.00152104\n",
      "2019-01-29T02:24:22.084974: step 466, loss 0.160998, acc 0.96875, learning_rate 0.00151725\n",
      "2019-01-29T02:24:22.211515: step 467, loss 0.162524, acc 1, learning_rate 0.00151347\n",
      "2019-01-29T02:24:22.338312: step 468, loss 0.205104, acc 0.96875, learning_rate 0.00150971\n",
      "2019-01-29T02:24:22.467696: step 469, loss 0.186325, acc 0.953125, learning_rate 0.00150595\n",
      "2019-01-29T02:24:22.594292: step 470, loss 0.211145, acc 0.953125, learning_rate 0.00150221\n",
      "2019-01-29T02:24:22.681573: step 471, loss 0.264059, acc 0.9375, learning_rate 0.00149847\n",
      "2019-01-29T02:24:22.808697: step 472, loss 0.203814, acc 0.984375, learning_rate 0.00149475\n",
      "2019-01-29T02:24:22.929111: step 473, loss 0.202741, acc 0.96875, learning_rate 0.00149103\n",
      "2019-01-29T02:24:23.056318: step 474, loss 0.202652, acc 0.96875, learning_rate 0.00148732\n",
      "2019-01-29T02:24:23.186875: step 475, loss 0.137839, acc 1, learning_rate 0.00148363\n",
      "2019-01-29T02:24:23.308303: step 476, loss 0.196976, acc 0.9375, learning_rate 0.00147994\n",
      "2019-01-29T02:24:23.398276: step 477, loss 0.20825, acc 0.9375, learning_rate 0.00147626\n",
      "2019-01-29T02:24:23.487937: step 478, loss 0.202313, acc 0.9375, learning_rate 0.0014726\n",
      "2019-01-29T02:24:23.591813: step 479, loss 0.20762, acc 0.96875, learning_rate 0.00146894\n",
      "2019-01-29T02:24:23.716526: step 480, loss 0.161828, acc 0.984375, learning_rate 0.00146529\n",
      "2019-01-29T02:24:23.805878: step 481, loss 0.156163, acc 0.984375, learning_rate 0.00146166\n",
      "2019-01-29T02:24:23.899971: step 482, loss 0.175576, acc 0.984375, learning_rate 0.00145803\n",
      "2019-01-29T02:24:24.025307: step 483, loss 0.209348, acc 0.96875, learning_rate 0.00145441\n",
      "2019-01-29T02:24:24.155000: step 484, loss 0.224225, acc 0.96875, learning_rate 0.0014508\n",
      "2019-01-29T02:24:24.286389: step 485, loss 0.218879, acc 0.9375, learning_rate 0.0014472\n",
      "2019-01-29T02:24:24.408070: step 486, loss 0.177593, acc 0.96875, learning_rate 0.00144361\n",
      "2019-01-29T02:24:24.497084: step 487, loss 0.229522, acc 0.9375, learning_rate 0.00144003\n",
      "2019-01-29T02:24:24.626131: step 488, loss 0.179057, acc 0.96875, learning_rate 0.00143646\n",
      "2019-01-29T02:24:24.757391: step 489, loss 0.229038, acc 0.9375, learning_rate 0.0014329\n",
      "2019-01-29T02:24:24.880335: step 490, loss 0.173364, acc 1, learning_rate 0.00142935\n",
      "2019-01-29T02:24:25.010007: step 491, loss 0.224848, acc 0.953125, learning_rate 0.00142581\n",
      "2019-01-29T02:24:25.140608: step 492, loss 0.192923, acc 0.984375, learning_rate 0.00142228\n",
      "2019-01-29T02:24:25.230639: step 493, loss 0.181837, acc 1, learning_rate 0.00141876\n",
      "2019-01-29T02:24:25.357534: step 494, loss 0.191277, acc 0.984375, learning_rate 0.00141524\n",
      "2019-01-29T02:24:25.489448: step 495, loss 0.233829, acc 0.953125, learning_rate 0.00141174\n",
      "2019-01-29T02:24:25.613810: step 496, loss 0.16046, acc 0.984375, learning_rate 0.00140824\n",
      "2019-01-29T02:24:25.740403: step 497, loss 0.249462, acc 0.9375, learning_rate 0.00140476\n",
      "2019-01-29T02:24:25.866830: step 498, loss 0.2243, acc 0.9375, learning_rate 0.00140128\n",
      "2019-01-29T02:24:25.997529: step 499, loss 0.18708, acc 0.96875, learning_rate 0.00139781\n",
      "2019-01-29T02:24:26.124217: step 500, loss 0.22843, acc 0.9375, learning_rate 0.00139436\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:24:26.151081: step 500, loss 0.556438, acc 0.770169\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-500\n",
      "\n",
      "2019-01-29T02:24:26.472882: step 501, loss 0.172841, acc 0.984375, learning_rate 0.00139091\n",
      "2019-01-29T02:24:26.599893: step 502, loss 0.158359, acc 0.96875, learning_rate 0.00138747\n",
      "2019-01-29T02:24:26.726455: step 503, loss 0.211276, acc 0.953125, learning_rate 0.00138404\n",
      "2019-01-29T02:24:26.814783: step 504, loss 0.133305, acc 1, learning_rate 0.00138062\n",
      "2019-01-29T02:24:26.906065: step 505, loss 0.165403, acc 0.984375, learning_rate 0.00137721\n",
      "2019-01-29T02:24:27.027059: step 506, loss 0.196599, acc 0.9375, learning_rate 0.0013738\n",
      "2019-01-29T02:24:27.158085: step 507, loss 0.193795, acc 0.984375, learning_rate 0.00137041\n",
      "2019-01-29T02:24:27.281397: step 508, loss 0.206468, acc 0.953125, learning_rate 0.00136703\n",
      "2019-01-29T02:24:27.370685: step 509, loss 0.125005, acc 1, learning_rate 0.00136365\n",
      "2019-01-29T02:24:27.491285: step 510, loss 0.170451, acc 0.984375, learning_rate 0.00136028\n",
      "2019-01-29T02:24:27.746210: step 511, loss 0.159946, acc 0.96875, learning_rate 0.00135693\n",
      "2019-01-29T02:24:27.872087: step 512, loss 0.178404, acc 0.984375, learning_rate 0.00135358\n",
      "2019-01-29T02:24:27.997375: step 513, loss 0.241006, acc 0.921875, learning_rate 0.00135024\n",
      "2019-01-29T02:24:28.121542: step 514, loss 0.212959, acc 0.953125, learning_rate 0.00134691\n",
      "2019-01-29T02:24:28.232097: step 515, loss 0.192196, acc 0.96875, learning_rate 0.00134358\n",
      "2019-01-29T02:24:28.317841: step 516, loss 0.143707, acc 1, learning_rate 0.00134027\n",
      "2019-01-29T02:24:28.443927: step 517, loss 0.202187, acc 0.953125, learning_rate 0.00133697\n",
      "2019-01-29T02:24:28.569161: step 518, loss 0.179103, acc 0.96875, learning_rate 0.00133367\n",
      "2019-01-29T02:24:28.694460: step 519, loss 0.155471, acc 0.984375, learning_rate 0.00133038\n",
      "2019-01-29T02:24:28.820179: step 520, loss 0.232953, acc 0.953125, learning_rate 0.00132711\n",
      "2019-01-29T02:24:28.907693: step 521, loss 0.167363, acc 0.984375, learning_rate 0.00132384\n",
      "2019-01-29T02:24:29.033913: step 522, loss 0.183599, acc 0.953125, learning_rate 0.00132058\n",
      "2019-01-29T02:24:29.157013: step 523, loss 0.154153, acc 1, learning_rate 0.00131732\n",
      "2019-01-29T02:24:29.244100: step 524, loss 0.25078, acc 0.953125, learning_rate 0.00131408\n",
      "2019-01-29T02:24:29.369665: step 525, loss 0.158493, acc 1, learning_rate 0.00131085\n",
      "2019-01-29T02:24:29.498143: step 526, loss 0.244754, acc 0.953125, learning_rate 0.00130762\n",
      "2019-01-29T02:24:29.617303: step 527, loss 0.224367, acc 0.9375, learning_rate 0.0013044\n",
      "2019-01-29T02:24:29.743055: step 528, loss 0.179091, acc 0.984375, learning_rate 0.00130119\n",
      "2019-01-29T02:24:29.859320: step 529, loss 0.178898, acc 0.953125, learning_rate 0.00129799\n",
      "2019-01-29T02:24:29.986177: step 530, loss 0.207945, acc 0.953125, learning_rate 0.0012948\n",
      "2019-01-29T02:24:30.112505: step 531, loss 0.202537, acc 0.96875, learning_rate 0.00129162\n",
      "2019-01-29T02:24:30.233153: step 532, loss 0.200938, acc 0.9375, learning_rate 0.00128844\n",
      "2019-01-29T02:24:30.351492: step 533, loss 0.217225, acc 0.9375, learning_rate 0.00128528\n",
      "2019-01-29T02:24:30.483393: step 534, loss 0.25689, acc 0.921875, learning_rate 0.00128212\n",
      "2019-01-29T02:24:30.604161: step 535, loss 0.139879, acc 0.984375, learning_rate 0.00127897\n",
      "2019-01-29T02:24:30.727250: step 536, loss 0.169543, acc 0.984375, learning_rate 0.00127583\n",
      "2019-01-29T02:24:30.851365: step 537, loss 0.220937, acc 0.96875, learning_rate 0.0012727\n",
      "2019-01-29T02:24:30.955054: step 538, loss 0.280079, acc 0.9375, learning_rate 0.00126957\n",
      "2019-01-29T02:24:31.077741: step 539, loss 0.193925, acc 0.984375, learning_rate 0.00126646\n",
      "2019-01-29T02:24:31.208560: step 540, loss 0.191861, acc 0.953125, learning_rate 0.00126335\n",
      "2019-01-29T02:24:31.331334: step 541, loss 0.143127, acc 1, learning_rate 0.00126025\n",
      "2019-01-29T02:24:31.457443: step 542, loss 0.198905, acc 0.984375, learning_rate 0.00125716\n",
      "2019-01-29T02:24:31.583127: step 543, loss 0.179567, acc 0.96875, learning_rate 0.00125408\n",
      "2019-01-29T02:24:31.709489: step 544, loss 0.221588, acc 0.9375, learning_rate 0.001251\n",
      "2019-01-29T02:24:31.836489: step 545, loss 0.252104, acc 0.9375, learning_rate 0.00124793\n",
      "2019-01-29T02:24:31.959755: step 546, loss 0.180024, acc 0.953125, learning_rate 0.00124488\n",
      "2019-01-29T02:24:32.084234: step 547, loss 0.210555, acc 0.96875, learning_rate 0.00124183\n",
      "2019-01-29T02:24:32.211966: step 548, loss 0.173665, acc 0.984375, learning_rate 0.00123878\n",
      "2019-01-29T02:24:32.298138: step 549, loss 0.25441, acc 0.953125, learning_rate 0.00123575\n",
      "2019-01-29T02:24:32.424184: step 550, loss 0.148208, acc 1, learning_rate 0.00123272\n",
      "2019-01-29T02:24:32.541572: step 551, loss 0.163555, acc 0.984375, learning_rate 0.00122971\n",
      "2019-01-29T02:24:32.632110: step 552, loss 0.210233, acc 0.953125, learning_rate 0.0012267\n",
      "2019-01-29T02:24:32.753686: step 553, loss 0.142084, acc 1, learning_rate 0.00122369\n",
      "2019-01-29T02:24:32.875881: step 554, loss 0.214037, acc 0.9375, learning_rate 0.0012207\n",
      "2019-01-29T02:24:32.991963: step 555, loss 0.18309, acc 0.96875, learning_rate 0.00121771\n",
      "2019-01-29T02:24:33.113181: step 556, loss 0.231567, acc 0.921875, learning_rate 0.00121474\n",
      "2019-01-29T02:24:33.235470: step 557, loss 0.149391, acc 0.984375, learning_rate 0.00121177\n",
      "2019-01-29T02:24:33.359635: step 558, loss 0.230129, acc 0.984375, learning_rate 0.0012088\n",
      "2019-01-29T02:24:33.486510: step 559, loss 0.197675, acc 0.9375, learning_rate 0.00120585\n",
      "2019-01-29T02:24:33.617004: step 560, loss 0.159333, acc 0.984375, learning_rate 0.0012029\n",
      "2019-01-29T02:24:33.749109: step 561, loss 0.166893, acc 0.96875, learning_rate 0.00119997\n",
      "2019-01-29T02:24:33.833792: step 562, loss 0.194687, acc 0.953125, learning_rate 0.00119704\n",
      "2019-01-29T02:24:33.941645: step 563, loss 0.221942, acc 0.96875, learning_rate 0.00119411\n",
      "2019-01-29T02:24:34.068712: step 564, loss 0.203859, acc 0.9375, learning_rate 0.0011912\n",
      "2019-01-29T02:24:34.185916: step 565, loss 0.190557, acc 0.9375, learning_rate 0.00118829\n",
      "2019-01-29T02:24:34.302633: step 566, loss 0.161033, acc 1, learning_rate 0.00118539\n",
      "2019-01-29T02:24:34.389066: step 567, loss 0.20188, acc 0.953125, learning_rate 0.0011825\n",
      "2019-01-29T02:24:34.515400: step 568, loss 0.139747, acc 1, learning_rate 0.00117962\n",
      "2019-01-29T02:24:34.640297: step 569, loss 0.184208, acc 1, learning_rate 0.00117674\n",
      "2019-01-29T02:24:34.769722: step 570, loss 0.226595, acc 0.96875, learning_rate 0.00117387\n",
      "2019-01-29T02:24:34.891012: step 571, loss 0.188577, acc 0.96875, learning_rate 0.00117101\n",
      "2019-01-29T02:24:35.015221: step 572, loss 0.206346, acc 0.953125, learning_rate 0.00116816\n",
      "2019-01-29T02:24:35.136457: step 573, loss 0.178983, acc 0.96875, learning_rate 0.00116531\n",
      "2019-01-29T02:24:35.266148: step 574, loss 0.165541, acc 0.96875, learning_rate 0.00116247\n",
      "2019-01-29T02:24:35.387762: step 575, loss 0.14053, acc 0.984375, learning_rate 0.00115964\n",
      "2019-01-29T02:24:35.477063: step 576, loss 0.217787, acc 0.953125, learning_rate 0.00115682\n",
      "2019-01-29T02:24:35.605865: step 577, loss 0.233539, acc 0.953125, learning_rate 0.001154\n",
      "2019-01-29T02:24:35.728612: step 578, loss 0.18871, acc 0.96875, learning_rate 0.00115119\n",
      "2019-01-29T02:24:35.859286: step 579, loss 0.243991, acc 0.921875, learning_rate 0.00114839\n",
      "2019-01-29T02:24:35.985788: step 580, loss 0.214297, acc 0.96875, learning_rate 0.0011456\n",
      "2019-01-29T02:24:36.113340: step 581, loss 0.24367, acc 0.921875, learning_rate 0.00114282\n",
      "2019-01-29T02:24:36.238178: step 582, loss 0.222055, acc 0.9375, learning_rate 0.00114004\n",
      "2019-01-29T02:24:36.346499: step 583, loss 0.173487, acc 0.984375, learning_rate 0.00113727\n",
      "2019-01-29T02:24:36.468031: step 584, loss 0.190103, acc 0.953125, learning_rate 0.0011345\n",
      "2019-01-29T02:24:36.593419: step 585, loss 0.24314, acc 0.90625, learning_rate 0.00113175\n",
      "2019-01-29T02:24:36.680721: step 586, loss 0.159977, acc 0.96875, learning_rate 0.001129\n",
      "2019-01-29T02:24:36.806583: step 587, loss 0.168599, acc 0.96875, learning_rate 0.00112626\n",
      "2019-01-29T02:24:36.892432: step 588, loss 0.198631, acc 0.953125, learning_rate 0.00112352\n",
      "2019-01-29T02:24:37.024348: step 589, loss 0.184703, acc 0.953125, learning_rate 0.00112079\n",
      "2019-01-29T02:24:37.148238: step 590, loss 0.133848, acc 1, learning_rate 0.00111808\n",
      "2019-01-29T02:24:37.275286: step 591, loss 0.211141, acc 0.96875, learning_rate 0.00111536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:24:37.398071: step 592, loss 0.220469, acc 0.96875, learning_rate 0.00111266\n",
      "2019-01-29T02:24:37.525897: step 593, loss 0.22628, acc 0.9375, learning_rate 0.00110996\n",
      "2019-01-29T02:24:37.653185: step 594, loss 0.180622, acc 0.96875, learning_rate 0.00110727\n",
      "2019-01-29T02:24:37.784837: step 595, loss 0.256656, acc 0.90625, learning_rate 0.00110459\n",
      "2019-01-29T02:24:37.872245: step 596, loss 0.271038, acc 0.921875, learning_rate 0.00110191\n",
      "2019-01-29T02:24:37.996615: step 597, loss 0.131879, acc 1, learning_rate 0.00109924\n",
      "2019-01-29T02:24:38.121686: step 598, loss 0.229843, acc 0.9375, learning_rate 0.00109658\n",
      "2019-01-29T02:24:38.245866: step 599, loss 0.183129, acc 0.953125, learning_rate 0.00109392\n",
      "2019-01-29T02:24:38.330319: step 600, loss 0.192417, acc 0.966667, learning_rate 0.00109127\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:24:38.357508: step 600, loss 0.548573, acc 0.774859\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-600\n",
      "\n",
      "2019-01-29T02:24:38.676807: step 601, loss 0.13494, acc 0.984375, learning_rate 0.00108863\n",
      "2019-01-29T02:24:38.799515: step 602, loss 0.164071, acc 0.984375, learning_rate 0.001086\n",
      "2019-01-29T02:24:38.930584: step 603, loss 0.181947, acc 0.953125, learning_rate 0.00108337\n",
      "2019-01-29T02:24:39.056505: step 604, loss 0.189543, acc 0.96875, learning_rate 0.00108075\n",
      "2019-01-29T02:24:39.176173: step 605, loss 0.120692, acc 1, learning_rate 0.00107814\n",
      "2019-01-29T02:24:39.266660: step 606, loss 0.135789, acc 1, learning_rate 0.00107553\n",
      "2019-01-29T02:24:39.355671: step 607, loss 0.163435, acc 0.96875, learning_rate 0.00107293\n",
      "2019-01-29T02:24:39.462474: step 608, loss 0.136246, acc 1, learning_rate 0.00107034\n",
      "2019-01-29T02:24:39.549256: step 609, loss 0.173004, acc 0.96875, learning_rate 0.00106776\n",
      "2019-01-29T02:24:39.675305: step 610, loss 0.160609, acc 0.984375, learning_rate 0.00106518\n",
      "2019-01-29T02:24:39.802831: step 611, loss 0.129413, acc 1, learning_rate 0.00106261\n",
      "2019-01-29T02:24:39.923040: step 612, loss 0.124036, acc 1, learning_rate 0.00106004\n",
      "2019-01-29T02:24:40.052566: step 613, loss 0.162653, acc 0.96875, learning_rate 0.00105749\n",
      "2019-01-29T02:24:40.176591: step 614, loss 0.183569, acc 0.984375, learning_rate 0.00105493\n",
      "2019-01-29T02:24:40.266007: step 615, loss 0.134203, acc 1, learning_rate 0.00105239\n",
      "2019-01-29T02:24:40.353887: step 616, loss 0.149825, acc 0.984375, learning_rate 0.00104985\n",
      "2019-01-29T02:24:40.474661: step 617, loss 0.139986, acc 1, learning_rate 0.00104732\n",
      "2019-01-29T02:24:40.600975: step 618, loss 0.132614, acc 0.984375, learning_rate 0.0010448\n",
      "2019-01-29T02:24:40.689526: step 619, loss 0.143031, acc 0.984375, learning_rate 0.00104228\n",
      "2019-01-29T02:24:40.813176: step 620, loss 0.214944, acc 0.953125, learning_rate 0.00103977\n",
      "2019-01-29T02:24:40.941575: step 621, loss 0.145967, acc 0.984375, learning_rate 0.00103727\n",
      "2019-01-29T02:24:41.046936: step 622, loss 0.164733, acc 0.984375, learning_rate 0.00103477\n",
      "2019-01-29T02:24:41.136311: step 623, loss 0.155921, acc 0.96875, learning_rate 0.00103228\n",
      "2019-01-29T02:24:41.227820: step 624, loss 0.156334, acc 0.96875, learning_rate 0.0010298\n",
      "2019-01-29T02:24:41.314248: step 625, loss 0.250583, acc 0.921875, learning_rate 0.00102732\n",
      "2019-01-29T02:24:41.438180: step 626, loss 0.109516, acc 1, learning_rate 0.00102485\n",
      "2019-01-29T02:24:41.527864: step 627, loss 0.147022, acc 1, learning_rate 0.00102238\n",
      "2019-01-29T02:24:41.652561: step 628, loss 0.157894, acc 0.984375, learning_rate 0.00101993\n",
      "2019-01-29T02:24:41.782065: step 629, loss 0.166912, acc 0.984375, learning_rate 0.00101748\n",
      "2019-01-29T02:24:41.888213: step 630, loss 0.149773, acc 0.96875, learning_rate 0.00101503\n",
      "2019-01-29T02:24:42.010059: step 631, loss 0.139637, acc 1, learning_rate 0.00101259\n",
      "2019-01-29T02:24:42.136644: step 632, loss 0.139819, acc 0.984375, learning_rate 0.00101016\n",
      "2019-01-29T02:24:42.223022: step 633, loss 0.129851, acc 0.984375, learning_rate 0.00100774\n",
      "2019-01-29T02:24:42.350379: step 634, loss 0.166078, acc 0.984375, learning_rate 0.00100532\n",
      "2019-01-29T02:24:42.472376: step 635, loss 0.256158, acc 0.921875, learning_rate 0.00100291\n",
      "2019-01-29T02:24:42.600109: step 636, loss 0.129811, acc 0.984375, learning_rate 0.0010005\n",
      "2019-01-29T02:24:42.728285: step 637, loss 0.139501, acc 1, learning_rate 0.000998102\n",
      "2019-01-29T02:24:42.852381: step 638, loss 0.158641, acc 0.96875, learning_rate 0.000995709\n",
      "2019-01-29T02:24:42.980001: step 639, loss 0.139884, acc 0.984375, learning_rate 0.000993323\n",
      "2019-01-29T02:24:43.104018: step 640, loss 0.156203, acc 0.984375, learning_rate 0.000990943\n",
      "2019-01-29T02:24:43.228402: step 641, loss 0.156872, acc 0.96875, learning_rate 0.000988569\n",
      "2019-01-29T02:24:43.351917: step 642, loss 0.148203, acc 1, learning_rate 0.000986202\n",
      "2019-01-29T02:24:43.475346: step 643, loss 0.186104, acc 0.96875, learning_rate 0.000983841\n",
      "2019-01-29T02:24:43.604551: step 644, loss 0.170976, acc 0.96875, learning_rate 0.000981486\n",
      "2019-01-29T02:24:43.694612: step 645, loss 0.140406, acc 1, learning_rate 0.000979137\n",
      "2019-01-29T02:24:43.819486: step 646, loss 0.115791, acc 1, learning_rate 0.000976795\n",
      "2019-01-29T02:24:44.018489: step 647, loss 0.203091, acc 0.984375, learning_rate 0.000974459\n",
      "2019-01-29T02:24:44.102958: step 648, loss 0.151277, acc 0.953125, learning_rate 0.00097213\n",
      "2019-01-29T02:24:44.231705: step 649, loss 0.136436, acc 0.96875, learning_rate 0.000969806\n",
      "2019-01-29T02:24:44.358906: step 650, loss 0.162417, acc 0.984375, learning_rate 0.000967489\n",
      "2019-01-29T02:24:44.479588: step 651, loss 0.162766, acc 0.984375, learning_rate 0.000965177\n",
      "2019-01-29T02:24:44.603967: step 652, loss 0.196952, acc 0.953125, learning_rate 0.000962872\n",
      "2019-01-29T02:24:44.690067: step 653, loss 0.185824, acc 0.96875, learning_rate 0.000960574\n",
      "2019-01-29T02:24:44.817083: step 654, loss 0.125526, acc 1, learning_rate 0.000958281\n",
      "2019-01-29T02:24:44.905162: step 655, loss 0.127762, acc 0.984375, learning_rate 0.000955994\n",
      "2019-01-29T02:24:45.033389: step 656, loss 0.182267, acc 0.96875, learning_rate 0.000953714\n",
      "2019-01-29T02:24:45.163207: step 657, loss 0.159131, acc 0.984375, learning_rate 0.000951439\n",
      "2019-01-29T02:24:45.251274: step 658, loss 0.149825, acc 1, learning_rate 0.000949171\n",
      "2019-01-29T02:24:45.356503: step 659, loss 0.131395, acc 0.984375, learning_rate 0.000946908\n",
      "2019-01-29T02:24:45.482998: step 660, loss 0.257697, acc 0.9375, learning_rate 0.000944652\n",
      "2019-01-29T02:24:45.609123: step 661, loss 0.139971, acc 1, learning_rate 0.000942402\n",
      "2019-01-29T02:24:45.698280: step 662, loss 0.133982, acc 1, learning_rate 0.000940157\n",
      "2019-01-29T02:24:45.820520: step 663, loss 0.109845, acc 1, learning_rate 0.000937919\n",
      "2019-01-29T02:24:45.947591: step 664, loss 0.20218, acc 0.953125, learning_rate 0.000935686\n",
      "2019-01-29T02:24:46.075427: step 665, loss 0.136496, acc 0.984375, learning_rate 0.00093346\n",
      "2019-01-29T02:24:46.203902: step 666, loss 0.174542, acc 0.984375, learning_rate 0.000931239\n",
      "2019-01-29T02:24:46.292872: step 667, loss 0.148708, acc 0.96875, learning_rate 0.000929025\n",
      "2019-01-29T02:24:46.411685: step 668, loss 0.174872, acc 0.96875, learning_rate 0.000926816\n",
      "2019-01-29T02:24:46.541470: step 669, loss 0.145254, acc 0.984375, learning_rate 0.000924613\n",
      "2019-01-29T02:24:46.665715: step 670, loss 0.177462, acc 0.953125, learning_rate 0.000922416\n",
      "2019-01-29T02:24:46.789125: step 671, loss 0.14082, acc 0.984375, learning_rate 0.000920225\n",
      "2019-01-29T02:24:46.877493: step 672, loss 0.161357, acc 0.953125, learning_rate 0.00091804\n",
      "2019-01-29T02:24:47.008325: step 673, loss 0.161766, acc 0.984375, learning_rate 0.000915861\n",
      "2019-01-29T02:24:47.131517: step 674, loss 0.208173, acc 0.9375, learning_rate 0.000913687\n",
      "2019-01-29T02:24:47.257366: step 675, loss 0.151458, acc 0.96875, learning_rate 0.000911519\n",
      "2019-01-29T02:24:47.383325: step 676, loss 0.166331, acc 0.96875, learning_rate 0.000909357\n",
      "2019-01-29T02:24:47.510460: step 677, loss 0.137511, acc 0.984375, learning_rate 0.000907201\n",
      "2019-01-29T02:24:47.594930: step 678, loss 0.129138, acc 1, learning_rate 0.00090505\n",
      "2019-01-29T02:24:47.726810: step 679, loss 0.142701, acc 0.984375, learning_rate 0.000902905\n",
      "2019-01-29T02:24:47.848185: step 680, loss 0.190405, acc 0.953125, learning_rate 0.000900766\n",
      "2019-01-29T02:24:47.979834: step 681, loss 0.169828, acc 1, learning_rate 0.000898633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:24:48.111011: step 682, loss 0.164757, acc 0.984375, learning_rate 0.000896505\n",
      "2019-01-29T02:24:48.236468: step 683, loss 0.18333, acc 0.96875, learning_rate 0.000894383\n",
      "2019-01-29T02:24:48.361998: step 684, loss 0.160007, acc 0.984375, learning_rate 0.000892267\n",
      "2019-01-29T02:24:48.450511: step 685, loss 0.192924, acc 0.921875, learning_rate 0.000890156\n",
      "2019-01-29T02:24:48.566972: step 686, loss 0.15017, acc 0.984375, learning_rate 0.000888051\n",
      "2019-01-29T02:24:48.700714: step 687, loss 0.164901, acc 0.96875, learning_rate 0.000885951\n",
      "2019-01-29T02:24:48.825766: step 688, loss 0.127527, acc 0.984375, learning_rate 0.000883857\n",
      "2019-01-29T02:24:48.911672: step 689, loss 0.126078, acc 0.984375, learning_rate 0.000881769\n",
      "2019-01-29T02:24:48.998919: step 690, loss 0.137895, acc 0.984375, learning_rate 0.000879686\n",
      "2019-01-29T02:24:49.124886: step 691, loss 0.15204, acc 0.984375, learning_rate 0.000877609\n",
      "2019-01-29T02:24:49.255522: step 692, loss 0.17066, acc 0.984375, learning_rate 0.000875537\n",
      "2019-01-29T02:24:49.380538: step 693, loss 0.114169, acc 0.984375, learning_rate 0.000873471\n",
      "2019-01-29T02:24:49.500522: step 694, loss 0.133729, acc 0.984375, learning_rate 0.00087141\n",
      "2019-01-29T02:24:49.628954: step 695, loss 0.139863, acc 0.984375, learning_rate 0.000869355\n",
      "2019-01-29T02:24:49.715154: step 696, loss 0.132863, acc 0.984375, learning_rate 0.000867305\n",
      "2019-01-29T02:24:49.844651: step 697, loss 0.132907, acc 0.984375, learning_rate 0.000865261\n",
      "2019-01-29T02:24:49.972133: step 698, loss 0.127811, acc 0.984375, learning_rate 0.000863222\n",
      "2019-01-29T02:24:50.103101: step 699, loss 0.166074, acc 0.96875, learning_rate 0.000861189\n",
      "2019-01-29T02:24:50.229141: step 700, loss 0.166305, acc 0.953125, learning_rate 0.000859161\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:24:50.256021: step 700, loss 0.576711, acc 0.781426\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-700\n",
      "\n",
      "2019-01-29T02:24:50.590179: step 701, loss 0.123069, acc 0.984375, learning_rate 0.000857138\n",
      "2019-01-29T02:24:50.712097: step 702, loss 0.125558, acc 0.984375, learning_rate 0.000855121\n",
      "2019-01-29T02:24:50.839598: step 703, loss 0.174883, acc 0.96875, learning_rate 0.000853109\n",
      "2019-01-29T02:24:50.955326: step 704, loss 0.145159, acc 0.984375, learning_rate 0.000851103\n",
      "2019-01-29T02:24:51.043635: step 705, loss 0.14378, acc 0.96875, learning_rate 0.000849102\n",
      "2019-01-29T02:24:51.172918: step 706, loss 0.102742, acc 1, learning_rate 0.000847106\n",
      "2019-01-29T02:24:51.297510: step 707, loss 0.150826, acc 0.953125, learning_rate 0.000845115\n",
      "2019-01-29T02:24:51.423390: step 708, loss 0.167701, acc 0.984375, learning_rate 0.00084313\n",
      "2019-01-29T02:24:51.513079: step 709, loss 0.125227, acc 1, learning_rate 0.00084115\n",
      "2019-01-29T02:24:51.644412: step 710, loss 0.162249, acc 0.953125, learning_rate 0.000839176\n",
      "2019-01-29T02:24:51.772907: step 711, loss 0.139955, acc 1, learning_rate 0.000837206\n",
      "2019-01-29T02:24:51.901668: step 712, loss 0.134492, acc 0.984375, learning_rate 0.000835242\n",
      "2019-01-29T02:24:52.025738: step 713, loss 0.157649, acc 0.96875, learning_rate 0.000833283\n",
      "2019-01-29T02:24:52.153758: step 714, loss 0.162502, acc 0.984375, learning_rate 0.00083133\n",
      "2019-01-29T02:24:52.281158: step 715, loss 0.115144, acc 1, learning_rate 0.000829381\n",
      "2019-01-29T02:24:52.408660: step 716, loss 0.188126, acc 0.96875, learning_rate 0.000827438\n",
      "2019-01-29T02:24:52.536670: step 717, loss 0.19343, acc 0.953125, learning_rate 0.0008255\n",
      "2019-01-29T02:24:52.661977: step 718, loss 0.113737, acc 1, learning_rate 0.000823567\n",
      "2019-01-29T02:24:52.780722: step 719, loss 0.136876, acc 1, learning_rate 0.00082164\n",
      "2019-01-29T02:24:52.900161: step 720, loss 0.152337, acc 0.984375, learning_rate 0.000819717\n",
      "2019-01-29T02:24:53.023808: step 721, loss 0.188889, acc 0.953125, learning_rate 0.000817799\n",
      "2019-01-29T02:24:53.115142: step 722, loss 0.119356, acc 1, learning_rate 0.000815887\n",
      "2019-01-29T02:24:53.204681: step 723, loss 0.16104, acc 0.96875, learning_rate 0.00081398\n",
      "2019-01-29T02:24:53.328639: step 724, loss 0.141477, acc 0.984375, learning_rate 0.000812078\n",
      "2019-01-29T02:24:53.455119: step 725, loss 0.182561, acc 0.9375, learning_rate 0.00081018\n",
      "2019-01-29T02:24:53.576616: step 726, loss 0.193632, acc 0.96875, learning_rate 0.000808288\n",
      "2019-01-29T02:24:53.702147: step 727, loss 0.152159, acc 0.984375, learning_rate 0.000806401\n",
      "2019-01-29T02:24:53.821155: step 728, loss 0.127298, acc 1, learning_rate 0.000804519\n",
      "2019-01-29T02:24:53.943959: step 729, loss 0.22037, acc 0.921875, learning_rate 0.000802642\n",
      "2019-01-29T02:24:54.073824: step 730, loss 0.110358, acc 1, learning_rate 0.00080077\n",
      "2019-01-29T02:24:54.160829: step 731, loss 0.19433, acc 0.9375, learning_rate 0.000798903\n",
      "2019-01-29T02:24:54.287755: step 732, loss 0.145811, acc 0.984375, learning_rate 0.000797041\n",
      "2019-01-29T02:24:54.415570: step 733, loss 0.156375, acc 0.96875, learning_rate 0.000795184\n",
      "2019-01-29T02:24:54.539377: step 734, loss 0.141673, acc 0.96875, learning_rate 0.000793332\n",
      "2019-01-29T02:24:54.627452: step 735, loss 0.132416, acc 1, learning_rate 0.000791485\n",
      "2019-01-29T02:24:54.717887: step 736, loss 0.148854, acc 0.984375, learning_rate 0.000789643\n",
      "2019-01-29T02:24:54.845979: step 737, loss 0.108325, acc 1, learning_rate 0.000787805\n",
      "2019-01-29T02:24:54.967293: step 738, loss 0.163974, acc 0.984375, learning_rate 0.000785973\n",
      "2019-01-29T02:24:55.058475: step 739, loss 0.158153, acc 1, learning_rate 0.000784145\n",
      "2019-01-29T02:24:55.182359: step 740, loss 0.126443, acc 0.984375, learning_rate 0.000782322\n",
      "2019-01-29T02:24:55.303766: step 741, loss 0.117517, acc 1, learning_rate 0.000780505\n",
      "2019-01-29T02:24:55.432528: step 742, loss 0.131307, acc 0.984375, learning_rate 0.000778692\n",
      "2019-01-29T02:24:55.558987: step 743, loss 0.164319, acc 0.96875, learning_rate 0.000776883\n",
      "2019-01-29T02:24:55.685508: step 744, loss 0.0981079, acc 1, learning_rate 0.00077508\n",
      "2019-01-29T02:24:55.806715: step 745, loss 0.204913, acc 0.921875, learning_rate 0.000773281\n",
      "2019-01-29T02:24:55.935312: step 746, loss 0.160833, acc 0.96875, learning_rate 0.000771488\n",
      "2019-01-29T02:24:56.025156: step 747, loss 0.149105, acc 1, learning_rate 0.000769699\n",
      "2019-01-29T02:24:56.148513: step 748, loss 0.159457, acc 0.953125, learning_rate 0.000767914\n",
      "2019-01-29T02:24:56.268115: step 749, loss 0.118075, acc 1, learning_rate 0.000766135\n",
      "2019-01-29T02:24:56.357006: step 750, loss 0.15842, acc 0.983333, learning_rate 0.00076436\n",
      "2019-01-29T02:24:56.479167: step 751, loss 0.133713, acc 0.984375, learning_rate 0.00076259\n",
      "2019-01-29T02:24:56.568135: step 752, loss 0.107383, acc 0.984375, learning_rate 0.000760825\n",
      "2019-01-29T02:24:56.695427: step 753, loss 0.100267, acc 1, learning_rate 0.000759064\n",
      "2019-01-29T02:24:56.826452: step 754, loss 0.177312, acc 0.96875, learning_rate 0.000757309\n",
      "2019-01-29T02:24:56.951196: step 755, loss 0.120953, acc 0.984375, learning_rate 0.000755557\n",
      "2019-01-29T02:24:57.078453: step 756, loss 0.131494, acc 1, learning_rate 0.000753811\n",
      "2019-01-29T02:24:57.204530: step 757, loss 0.159301, acc 0.96875, learning_rate 0.000752069\n",
      "2019-01-29T02:24:57.335937: step 758, loss 0.126006, acc 0.984375, learning_rate 0.000750332\n",
      "2019-01-29T02:24:57.423647: step 759, loss 0.158976, acc 0.96875, learning_rate 0.000748599\n",
      "2019-01-29T02:24:57.555399: step 760, loss 0.110814, acc 1, learning_rate 0.000746871\n",
      "2019-01-29T02:24:57.657818: step 761, loss 0.129826, acc 1, learning_rate 0.000745148\n",
      "2019-01-29T02:24:57.746358: step 762, loss 0.121793, acc 0.984375, learning_rate 0.000743429\n",
      "2019-01-29T02:24:57.878169: step 763, loss 0.115543, acc 0.984375, learning_rate 0.000741714\n",
      "2019-01-29T02:24:58.008017: step 764, loss 0.15004, acc 0.953125, learning_rate 0.000740005\n",
      "2019-01-29T02:24:58.094525: step 765, loss 0.139826, acc 1, learning_rate 0.0007383\n",
      "2019-01-29T02:24:58.220210: step 766, loss 0.130753, acc 0.984375, learning_rate 0.000736599\n",
      "2019-01-29T02:24:58.351821: step 767, loss 0.129917, acc 0.984375, learning_rate 0.000734903\n",
      "2019-01-29T02:24:58.479906: step 768, loss 0.174092, acc 0.984375, learning_rate 0.000733212\n",
      "2019-01-29T02:24:58.607460: step 769, loss 0.125401, acc 1, learning_rate 0.000731525\n",
      "2019-01-29T02:24:58.730770: step 770, loss 0.134076, acc 1, learning_rate 0.000729842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:24:58.819450: step 771, loss 0.116356, acc 1, learning_rate 0.000728164\n",
      "2019-01-29T02:24:58.945488: step 772, loss 0.185426, acc 0.9375, learning_rate 0.00072649\n",
      "2019-01-29T02:24:59.070294: step 773, loss 0.139539, acc 0.953125, learning_rate 0.000724821\n",
      "2019-01-29T02:24:59.201153: step 774, loss 0.124493, acc 1, learning_rate 0.000723157\n",
      "2019-01-29T02:24:59.326872: step 775, loss 0.161348, acc 0.984375, learning_rate 0.000721496\n",
      "2019-01-29T02:24:59.454942: step 776, loss 0.130529, acc 0.984375, learning_rate 0.000719841\n",
      "2019-01-29T02:24:59.544188: step 777, loss 0.0861827, acc 1, learning_rate 0.000718189\n",
      "2019-01-29T02:24:59.673659: step 778, loss 0.0986229, acc 1, learning_rate 0.000716542\n",
      "2019-01-29T02:24:59.758131: step 779, loss 0.132724, acc 1, learning_rate 0.0007149\n",
      "2019-01-29T02:24:59.883633: step 780, loss 0.136974, acc 0.96875, learning_rate 0.000713261\n",
      "2019-01-29T02:25:00.007346: step 781, loss 0.111158, acc 0.984375, learning_rate 0.000711628\n",
      "2019-01-29T02:25:00.131490: step 782, loss 0.115136, acc 1, learning_rate 0.000709998\n",
      "2019-01-29T02:25:00.258132: step 783, loss 0.126719, acc 1, learning_rate 0.000708373\n",
      "2019-01-29T02:25:00.389649: step 784, loss 0.135478, acc 0.96875, learning_rate 0.000706752\n",
      "2019-01-29T02:25:00.513200: step 785, loss 0.156729, acc 0.96875, learning_rate 0.000705135\n",
      "2019-01-29T02:25:00.640960: step 786, loss 0.160797, acc 0.96875, learning_rate 0.000703523\n",
      "2019-01-29T02:25:00.765951: step 787, loss 0.125065, acc 1, learning_rate 0.000701915\n",
      "2019-01-29T02:25:00.888109: step 788, loss 0.100796, acc 1, learning_rate 0.000700312\n",
      "2019-01-29T02:25:00.971925: step 789, loss 0.109851, acc 1, learning_rate 0.000698712\n",
      "2019-01-29T02:25:01.097660: step 790, loss 0.127543, acc 1, learning_rate 0.000697117\n",
      "2019-01-29T02:25:01.226269: step 791, loss 0.156155, acc 0.96875, learning_rate 0.000695526\n",
      "2019-01-29T02:25:01.313676: step 792, loss 0.129843, acc 1, learning_rate 0.00069394\n",
      "2019-01-29T02:25:01.444555: step 793, loss 0.12444, acc 1, learning_rate 0.000692357\n",
      "2019-01-29T02:25:01.563373: step 794, loss 0.128493, acc 0.96875, learning_rate 0.000690779\n",
      "2019-01-29T02:25:01.690507: step 795, loss 0.0844766, acc 1, learning_rate 0.000689205\n",
      "2019-01-29T02:25:01.821594: step 796, loss 0.117062, acc 1, learning_rate 0.000687635\n",
      "2019-01-29T02:25:01.925009: step 797, loss 0.100609, acc 1, learning_rate 0.00068607\n",
      "2019-01-29T02:25:02.052607: step 798, loss 0.0987591, acc 1, learning_rate 0.000684508\n",
      "2019-01-29T02:25:02.138408: step 799, loss 0.159394, acc 0.96875, learning_rate 0.000682951\n",
      "2019-01-29T02:25:02.264002: step 800, loss 0.122338, acc 0.984375, learning_rate 0.000681398\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:25:02.291198: step 800, loss 0.589228, acc 0.770169\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-800\n",
      "\n",
      "2019-01-29T02:25:02.630791: step 801, loss 0.127981, acc 1, learning_rate 0.000679849\n",
      "2019-01-29T02:25:02.758599: step 802, loss 0.131563, acc 0.96875, learning_rate 0.000678304\n",
      "2019-01-29T02:25:02.892065: step 803, loss 0.141268, acc 0.984375, learning_rate 0.000676764\n",
      "2019-01-29T02:25:03.023077: step 804, loss 0.109426, acc 1, learning_rate 0.000675227\n",
      "2019-01-29T02:25:03.111514: step 805, loss 0.116771, acc 0.984375, learning_rate 0.000673694\n",
      "2019-01-29T02:25:03.201602: step 806, loss 0.108161, acc 1, learning_rate 0.000672166\n",
      "2019-01-29T02:25:03.328535: step 807, loss 0.108402, acc 1, learning_rate 0.000670642\n",
      "2019-01-29T02:25:03.452901: step 808, loss 0.124722, acc 0.96875, learning_rate 0.000669121\n",
      "2019-01-29T02:25:03.579966: step 809, loss 0.128465, acc 0.984375, learning_rate 0.000667605\n",
      "2019-01-29T02:25:03.708753: step 810, loss 0.120175, acc 1, learning_rate 0.000666093\n",
      "2019-01-29T02:25:03.836138: step 811, loss 0.114283, acc 1, learning_rate 0.000664585\n",
      "2019-01-29T02:25:03.962661: step 812, loss 0.146235, acc 0.984375, learning_rate 0.00066308\n",
      "2019-01-29T02:25:04.050096: step 813, loss 0.118554, acc 1, learning_rate 0.00066158\n",
      "2019-01-29T02:25:04.178866: step 814, loss 0.139544, acc 0.984375, learning_rate 0.000660084\n",
      "2019-01-29T02:25:04.264940: step 815, loss 0.137632, acc 0.984375, learning_rate 0.000658592\n",
      "2019-01-29T02:25:04.391915: step 816, loss 0.136685, acc 0.96875, learning_rate 0.000657104\n",
      "2019-01-29T02:25:04.518828: step 817, loss 0.104882, acc 1, learning_rate 0.000655619\n",
      "2019-01-29T02:25:04.629153: step 818, loss 0.116289, acc 1, learning_rate 0.000654139\n",
      "2019-01-29T02:25:04.756441: step 819, loss 0.165005, acc 0.96875, learning_rate 0.000652663\n",
      "2019-01-29T02:25:04.888909: step 820, loss 0.111774, acc 1, learning_rate 0.00065119\n",
      "2019-01-29T02:25:05.015822: step 821, loss 0.166696, acc 0.96875, learning_rate 0.000649722\n",
      "2019-01-29T02:25:05.139675: step 822, loss 0.128033, acc 0.984375, learning_rate 0.000648257\n",
      "2019-01-29T02:25:05.268182: step 823, loss 0.207226, acc 0.9375, learning_rate 0.000646797\n",
      "2019-01-29T02:25:05.397697: step 824, loss 0.119222, acc 0.984375, learning_rate 0.00064534\n",
      "2019-01-29T02:25:05.521195: step 825, loss 0.175064, acc 0.953125, learning_rate 0.000643887\n",
      "2019-01-29T02:25:05.648269: step 826, loss 0.10649, acc 1, learning_rate 0.000642438\n",
      "2019-01-29T02:25:05.774722: step 827, loss 0.144246, acc 0.984375, learning_rate 0.000640993\n",
      "2019-01-29T02:25:05.903098: step 828, loss 0.165634, acc 0.96875, learning_rate 0.000639551\n",
      "2019-01-29T02:25:06.034148: step 829, loss 0.101822, acc 0.984375, learning_rate 0.000638114\n",
      "2019-01-29T02:25:06.154304: step 830, loss 0.134246, acc 0.96875, learning_rate 0.00063668\n",
      "2019-01-29T02:25:06.283363: step 831, loss 0.103414, acc 1, learning_rate 0.00063525\n",
      "2019-01-29T02:25:06.408373: step 832, loss 0.129901, acc 1, learning_rate 0.000633824\n",
      "2019-01-29T02:25:06.530559: step 833, loss 0.0941206, acc 1, learning_rate 0.000632402\n",
      "2019-01-29T02:25:06.653348: step 834, loss 0.11789, acc 0.984375, learning_rate 0.000630984\n",
      "2019-01-29T02:25:06.772111: step 835, loss 0.129896, acc 1, learning_rate 0.000629569\n",
      "2019-01-29T02:25:06.894374: step 836, loss 0.107738, acc 1, learning_rate 0.000628158\n",
      "2019-01-29T02:25:07.027302: step 837, loss 0.136121, acc 0.96875, learning_rate 0.000626751\n",
      "2019-01-29T02:25:07.152664: step 838, loss 0.172557, acc 0.96875, learning_rate 0.000625348\n",
      "2019-01-29T02:25:07.273777: step 839, loss 0.130217, acc 0.984375, learning_rate 0.000623948\n",
      "2019-01-29T02:25:07.403872: step 840, loss 0.119388, acc 1, learning_rate 0.000622552\n",
      "2019-01-29T02:25:07.503624: step 841, loss 0.183615, acc 0.96875, learning_rate 0.00062116\n",
      "2019-01-29T02:25:07.633384: step 842, loss 0.098914, acc 1, learning_rate 0.000619771\n",
      "2019-01-29T02:25:07.722471: step 843, loss 0.148403, acc 0.984375, learning_rate 0.000618387\n",
      "2019-01-29T02:25:07.809774: step 844, loss 0.143886, acc 0.96875, learning_rate 0.000617005\n",
      "2019-01-29T02:25:07.937805: step 845, loss 0.127319, acc 1, learning_rate 0.000615628\n",
      "2019-01-29T02:25:08.018239: step 846, loss 0.148741, acc 0.96875, learning_rate 0.000614254\n",
      "2019-01-29T02:25:08.146697: step 847, loss 0.170807, acc 0.96875, learning_rate 0.000612884\n",
      "2019-01-29T02:25:08.277389: step 848, loss 0.137667, acc 0.96875, learning_rate 0.000611518\n",
      "2019-01-29T02:25:08.400239: step 849, loss 0.13294, acc 0.984375, learning_rate 0.000610155\n",
      "2019-01-29T02:25:08.526928: step 850, loss 0.13473, acc 0.96875, learning_rate 0.000608796\n",
      "2019-01-29T02:25:08.658977: step 851, loss 0.148119, acc 1, learning_rate 0.00060744\n",
      "2019-01-29T02:25:08.746237: step 852, loss 0.111999, acc 0.984375, learning_rate 0.000606088\n",
      "2019-01-29T02:25:08.868511: step 853, loss 0.12581, acc 0.984375, learning_rate 0.00060474\n",
      "2019-01-29T02:25:08.989713: step 854, loss 0.118912, acc 0.984375, learning_rate 0.000603395\n",
      "2019-01-29T02:25:09.121546: step 855, loss 0.105335, acc 1, learning_rate 0.000602054\n",
      "2019-01-29T02:25:09.243175: step 856, loss 0.130033, acc 0.96875, learning_rate 0.000600717\n",
      "2019-01-29T02:25:09.367252: step 857, loss 0.106685, acc 1, learning_rate 0.000599382\n",
      "2019-01-29T02:25:09.515105: step 858, loss 0.147149, acc 0.984375, learning_rate 0.000598052\n",
      "2019-01-29T02:25:09.641212: step 859, loss 0.114001, acc 1, learning_rate 0.000596725\n",
      "2019-01-29T02:25:09.730608: step 860, loss 0.149135, acc 0.96875, learning_rate 0.000595402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:25:09.854713: step 861, loss 0.139705, acc 0.96875, learning_rate 0.000594082\n",
      "2019-01-29T02:25:09.942363: step 862, loss 0.120403, acc 1, learning_rate 0.000592766\n",
      "2019-01-29T02:25:10.025479: step 863, loss 0.142733, acc 0.984375, learning_rate 0.000591453\n",
      "2019-01-29T02:25:10.149762: step 864, loss 0.132629, acc 0.984375, learning_rate 0.000590143\n",
      "2019-01-29T02:25:10.278105: step 865, loss 0.121983, acc 1, learning_rate 0.000588837\n",
      "2019-01-29T02:25:10.405544: step 866, loss 0.0944863, acc 1, learning_rate 0.000587535\n",
      "2019-01-29T02:25:10.527925: step 867, loss 0.136486, acc 0.984375, learning_rate 0.000586236\n",
      "2019-01-29T02:25:10.657884: step 868, loss 0.128238, acc 1, learning_rate 0.000584941\n",
      "2019-01-29T02:25:10.785813: step 869, loss 0.138945, acc 0.96875, learning_rate 0.000583649\n",
      "2019-01-29T02:25:10.916610: step 870, loss 0.132623, acc 1, learning_rate 0.00058236\n",
      "2019-01-29T02:25:11.037833: step 871, loss 0.108645, acc 1, learning_rate 0.000581075\n",
      "2019-01-29T02:25:11.165429: step 872, loss 0.10721, acc 1, learning_rate 0.000579793\n",
      "2019-01-29T02:25:11.293374: step 873, loss 0.191349, acc 0.953125, learning_rate 0.000578515\n",
      "2019-01-29T02:25:11.384676: step 874, loss 0.0897155, acc 1, learning_rate 0.00057724\n",
      "2019-01-29T02:25:11.472856: step 875, loss 0.12046, acc 0.984375, learning_rate 0.000575969\n",
      "2019-01-29T02:25:11.598954: step 876, loss 0.140689, acc 1, learning_rate 0.000574701\n",
      "2019-01-29T02:25:11.722838: step 877, loss 0.11838, acc 1, learning_rate 0.000573436\n",
      "2019-01-29T02:25:11.851401: step 878, loss 0.135208, acc 0.984375, learning_rate 0.000572175\n",
      "2019-01-29T02:25:11.973916: step 879, loss 0.135303, acc 1, learning_rate 0.000570917\n",
      "2019-01-29T02:25:12.057455: step 880, loss 0.138535, acc 0.984375, learning_rate 0.000569662\n",
      "2019-01-29T02:25:12.184700: step 881, loss 0.147327, acc 0.96875, learning_rate 0.000568411\n",
      "2019-01-29T02:25:12.274247: step 882, loss 0.098582, acc 1, learning_rate 0.000567163\n",
      "2019-01-29T02:25:12.402887: step 883, loss 0.131618, acc 0.96875, learning_rate 0.000565918\n",
      "2019-01-29T02:25:12.521814: step 884, loss 0.208899, acc 0.96875, learning_rate 0.000564677\n",
      "2019-01-29T02:25:12.606565: step 885, loss 0.112675, acc 1, learning_rate 0.000563439\n",
      "2019-01-29T02:25:12.694357: step 886, loss 0.108661, acc 1, learning_rate 0.000562204\n",
      "2019-01-29T02:25:12.783116: step 887, loss 0.113024, acc 1, learning_rate 0.000560973\n",
      "2019-01-29T02:25:12.902593: step 888, loss 0.137372, acc 0.984375, learning_rate 0.000559745\n",
      "2019-01-29T02:25:13.028885: step 889, loss 0.146099, acc 0.984375, learning_rate 0.00055852\n",
      "2019-01-29T02:25:13.151585: step 890, loss 0.171062, acc 0.9375, learning_rate 0.000557298\n",
      "2019-01-29T02:25:13.273950: step 891, loss 0.106158, acc 1, learning_rate 0.00055608\n",
      "2019-01-29T02:25:13.397739: step 892, loss 0.109252, acc 1, learning_rate 0.000554865\n",
      "2019-01-29T02:25:13.488674: step 893, loss 0.093366, acc 1, learning_rate 0.000553653\n",
      "2019-01-29T02:25:13.616680: step 894, loss 0.148402, acc 0.984375, learning_rate 0.000552444\n",
      "2019-01-29T02:25:13.743414: step 895, loss 0.214446, acc 0.984375, learning_rate 0.000551239\n",
      "2019-01-29T02:25:13.871576: step 896, loss 0.163407, acc 0.96875, learning_rate 0.000550037\n",
      "2019-01-29T02:25:14.003700: step 897, loss 0.0986331, acc 1, learning_rate 0.000548838\n",
      "2019-01-29T02:25:14.121712: step 898, loss 0.187984, acc 0.96875, learning_rate 0.000547642\n",
      "2019-01-29T02:25:14.246485: step 899, loss 0.123923, acc 0.984375, learning_rate 0.000546449\n",
      "2019-01-29T02:25:14.369505: step 900, loss 0.150287, acc 0.983333, learning_rate 0.00054526\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:25:14.476012: step 900, loss 0.585833, acc 0.773921\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-900\n",
      "\n",
      "2019-01-29T02:25:14.818555: step 901, loss 0.121279, acc 0.984375, learning_rate 0.000544073\n",
      "2019-01-29T02:25:14.941038: step 902, loss 0.0933623, acc 1, learning_rate 0.00054289\n",
      "2019-01-29T02:25:15.067526: step 903, loss 0.124874, acc 1, learning_rate 0.00054171\n",
      "2019-01-29T02:25:15.188609: step 904, loss 0.0948003, acc 1, learning_rate 0.000540534\n",
      "2019-01-29T02:25:15.314203: step 905, loss 0.0995126, acc 1, learning_rate 0.00053936\n",
      "2019-01-29T02:25:15.405671: step 906, loss 0.1112, acc 1, learning_rate 0.000538189\n",
      "2019-01-29T02:25:15.529959: step 907, loss 0.0991784, acc 1, learning_rate 0.000537022\n",
      "2019-01-29T02:25:15.656309: step 908, loss 0.11335, acc 0.984375, learning_rate 0.000535858\n",
      "2019-01-29T02:25:15.784179: step 909, loss 0.0968964, acc 1, learning_rate 0.000534696\n",
      "2019-01-29T02:25:15.905809: step 910, loss 0.107086, acc 1, learning_rate 0.000533538\n",
      "2019-01-29T02:25:15.992360: step 911, loss 0.0993864, acc 0.984375, learning_rate 0.000532383\n",
      "2019-01-29T02:25:16.117011: step 912, loss 0.108547, acc 0.984375, learning_rate 0.000531231\n",
      "2019-01-29T02:25:16.205513: step 913, loss 0.0945896, acc 1, learning_rate 0.000530082\n",
      "2019-01-29T02:25:16.332874: step 914, loss 0.0946239, acc 1, learning_rate 0.000528937\n",
      "2019-01-29T02:25:16.456380: step 915, loss 0.137534, acc 0.96875, learning_rate 0.000527794\n",
      "2019-01-29T02:25:16.576049: step 916, loss 0.141415, acc 0.96875, learning_rate 0.000526654\n",
      "2019-01-29T02:25:16.706036: step 917, loss 0.105591, acc 1, learning_rate 0.000525517\n",
      "2019-01-29T02:25:16.796397: step 918, loss 0.12428, acc 1, learning_rate 0.000524384\n",
      "2019-01-29T02:25:16.924818: step 919, loss 0.117797, acc 0.984375, learning_rate 0.000523253\n",
      "2019-01-29T02:25:17.012304: step 920, loss 0.148591, acc 0.953125, learning_rate 0.000522125\n",
      "2019-01-29T02:25:17.098203: step 921, loss 0.113522, acc 1, learning_rate 0.000521001\n",
      "2019-01-29T02:25:17.223180: step 922, loss 0.0917765, acc 1, learning_rate 0.000519879\n",
      "2019-01-29T02:25:17.348697: step 923, loss 0.098936, acc 1, learning_rate 0.00051876\n",
      "2019-01-29T02:25:17.475294: step 924, loss 0.15676, acc 0.96875, learning_rate 0.000517645\n",
      "2019-01-29T02:25:17.561175: step 925, loss 0.106418, acc 1, learning_rate 0.000516532\n",
      "2019-01-29T02:25:17.650976: step 926, loss 0.107844, acc 1, learning_rate 0.000515422\n",
      "2019-01-29T02:25:17.777222: step 927, loss 0.122345, acc 0.96875, learning_rate 0.000514316\n",
      "2019-01-29T02:25:17.902220: step 928, loss 0.126447, acc 0.96875, learning_rate 0.000513212\n",
      "2019-01-29T02:25:18.021887: step 929, loss 0.117327, acc 1, learning_rate 0.000512111\n",
      "2019-01-29T02:25:18.148737: step 930, loss 0.131028, acc 0.984375, learning_rate 0.000511013\n",
      "2019-01-29T02:25:18.264973: step 931, loss 0.194198, acc 0.984375, learning_rate 0.000509918\n",
      "2019-01-29T02:25:18.390449: step 932, loss 0.136822, acc 0.96875, learning_rate 0.000508826\n",
      "2019-01-29T02:25:18.517090: step 933, loss 0.147053, acc 0.96875, learning_rate 0.000507737\n",
      "2019-01-29T02:25:18.641452: step 934, loss 0.137098, acc 0.953125, learning_rate 0.00050665\n",
      "2019-01-29T02:25:18.769179: step 935, loss 0.132367, acc 0.984375, learning_rate 0.000505567\n",
      "2019-01-29T02:25:18.896935: step 936, loss 0.132006, acc 1, learning_rate 0.000504486\n",
      "2019-01-29T02:25:19.024804: step 937, loss 0.115081, acc 0.984375, learning_rate 0.000503409\n",
      "2019-01-29T02:25:19.146094: step 938, loss 0.101353, acc 1, learning_rate 0.000502334\n",
      "2019-01-29T02:25:19.237150: step 939, loss 0.0928704, acc 1, learning_rate 0.000501262\n",
      "2019-01-29T02:25:19.365079: step 940, loss 0.132147, acc 0.984375, learning_rate 0.000500193\n",
      "2019-01-29T02:25:19.490950: step 941, loss 0.102143, acc 1, learning_rate 0.000499127\n",
      "2019-01-29T02:25:19.666338: step 942, loss 0.155974, acc 0.96875, learning_rate 0.000498063\n",
      "2019-01-29T02:25:19.787729: step 943, loss 0.104828, acc 1, learning_rate 0.000497003\n",
      "2019-01-29T02:25:19.911541: step 944, loss 0.149255, acc 0.96875, learning_rate 0.000495945\n",
      "2019-01-29T02:25:20.039050: step 945, loss 0.104729, acc 1, learning_rate 0.00049489\n",
      "2019-01-29T02:25:20.130319: step 946, loss 0.124278, acc 1, learning_rate 0.000493838\n",
      "2019-01-29T02:25:20.258418: step 947, loss 0.150701, acc 0.96875, learning_rate 0.000492789\n",
      "2019-01-29T02:25:20.362941: step 948, loss 0.127798, acc 0.984375, learning_rate 0.000491742\n",
      "2019-01-29T02:25:20.482867: step 949, loss 0.107354, acc 1, learning_rate 0.000490699\n",
      "2019-01-29T02:25:20.614109: step 950, loss 0.118679, acc 0.984375, learning_rate 0.000489658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:25:20.737780: step 951, loss 0.14995, acc 0.984375, learning_rate 0.00048862\n",
      "2019-01-29T02:25:20.867784: step 952, loss 0.133196, acc 1, learning_rate 0.000487584\n",
      "2019-01-29T02:25:20.959319: step 953, loss 0.118203, acc 0.984375, learning_rate 0.000486552\n",
      "2019-01-29T02:25:21.086084: step 954, loss 0.13915, acc 0.984375, learning_rate 0.000485522\n",
      "2019-01-29T02:25:21.214432: step 955, loss 0.116963, acc 1, learning_rate 0.000484495\n",
      "2019-01-29T02:25:21.335982: step 956, loss 0.120149, acc 0.984375, learning_rate 0.00048347\n",
      "2019-01-29T02:25:21.455848: step 957, loss 0.10612, acc 1, learning_rate 0.000482449\n",
      "2019-01-29T02:25:21.573553: step 958, loss 0.112008, acc 1, learning_rate 0.00048143\n",
      "2019-01-29T02:25:21.700500: step 959, loss 0.128463, acc 0.984375, learning_rate 0.000480414\n",
      "2019-01-29T02:25:21.827503: step 960, loss 0.151908, acc 0.984375, learning_rate 0.0004794\n",
      "2019-01-29T02:25:21.918098: step 961, loss 0.111982, acc 0.984375, learning_rate 0.000478389\n",
      "2019-01-29T02:25:22.040982: step 962, loss 0.122598, acc 0.96875, learning_rate 0.000477381\n",
      "2019-01-29T02:25:22.169166: step 963, loss 0.10569, acc 1, learning_rate 0.000476376\n",
      "2019-01-29T02:25:22.295917: step 964, loss 0.0944893, acc 1, learning_rate 0.000475373\n",
      "2019-01-29T02:25:22.422495: step 965, loss 0.11348, acc 0.96875, learning_rate 0.000474373\n",
      "2019-01-29T02:25:22.552316: step 966, loss 0.0925134, acc 1, learning_rate 0.000473375\n",
      "2019-01-29T02:25:22.680259: step 967, loss 0.127173, acc 0.984375, learning_rate 0.000472381\n",
      "2019-01-29T02:25:22.810993: step 968, loss 0.11531, acc 0.984375, learning_rate 0.000471389\n",
      "2019-01-29T02:25:22.924250: step 969, loss 0.138361, acc 0.984375, learning_rate 0.000470399\n",
      "2019-01-29T02:25:23.011548: step 970, loss 0.103401, acc 1, learning_rate 0.000469412\n",
      "2019-01-29T02:25:23.138099: step 971, loss 0.105981, acc 0.984375, learning_rate 0.000468428\n",
      "2019-01-29T02:25:23.265086: step 972, loss 0.132239, acc 0.984375, learning_rate 0.000467447\n",
      "2019-01-29T02:25:23.389776: step 973, loss 0.108395, acc 0.984375, learning_rate 0.000466468\n",
      "2019-01-29T02:25:23.514328: step 974, loss 0.129928, acc 0.984375, learning_rate 0.000465491\n",
      "2019-01-29T02:25:23.644386: step 975, loss 0.0952258, acc 1, learning_rate 0.000464517\n",
      "2019-01-29T02:25:23.773905: step 976, loss 0.153163, acc 0.96875, learning_rate 0.000463546\n",
      "2019-01-29T02:25:23.901057: step 977, loss 0.132583, acc 0.984375, learning_rate 0.000462578\n",
      "2019-01-29T02:25:24.032789: step 978, loss 0.151724, acc 0.984375, learning_rate 0.000461612\n",
      "2019-01-29T02:25:24.122515: step 979, loss 0.11593, acc 1, learning_rate 0.000460648\n",
      "2019-01-29T02:25:24.251830: step 980, loss 0.111718, acc 1, learning_rate 0.000459687\n",
      "2019-01-29T02:25:24.379091: step 981, loss 0.159555, acc 0.96875, learning_rate 0.000458729\n",
      "2019-01-29T02:25:24.499279: step 982, loss 0.126858, acc 1, learning_rate 0.000457773\n",
      "2019-01-29T02:25:24.584040: step 983, loss 0.104594, acc 1, learning_rate 0.00045682\n",
      "2019-01-29T02:25:24.707571: step 984, loss 0.0927326, acc 1, learning_rate 0.00045587\n",
      "2019-01-29T02:25:24.837793: step 985, loss 0.125603, acc 0.984375, learning_rate 0.000454922\n",
      "2019-01-29T02:25:24.926498: step 986, loss 0.13749, acc 0.96875, learning_rate 0.000453976\n",
      "2019-01-29T02:25:25.050636: step 987, loss 0.0836492, acc 1, learning_rate 0.000453033\n",
      "2019-01-29T02:25:25.141122: step 988, loss 0.144874, acc 0.984375, learning_rate 0.000452092\n",
      "2019-01-29T02:25:25.267941: step 989, loss 0.0972772, acc 0.984375, learning_rate 0.000451154\n",
      "2019-01-29T02:25:25.393605: step 990, loss 0.128867, acc 0.984375, learning_rate 0.000450219\n",
      "2019-01-29T02:25:25.481819: step 991, loss 0.108794, acc 1, learning_rate 0.000449286\n",
      "2019-01-29T02:25:25.610144: step 992, loss 0.0841934, acc 1, learning_rate 0.000448355\n",
      "2019-01-29T02:25:25.737120: step 993, loss 0.132066, acc 0.984375, learning_rate 0.000447427\n",
      "2019-01-29T02:25:25.862170: step 994, loss 0.110709, acc 0.984375, learning_rate 0.000446501\n",
      "2019-01-29T02:25:25.983720: step 995, loss 0.105144, acc 1, learning_rate 0.000445578\n",
      "2019-01-29T02:25:26.116086: step 996, loss 0.116686, acc 0.984375, learning_rate 0.000444657\n",
      "2019-01-29T02:25:26.240550: step 997, loss 0.0897262, acc 1, learning_rate 0.000443739\n",
      "2019-01-29T02:25:26.367055: step 998, loss 0.117008, acc 0.984375, learning_rate 0.000442823\n",
      "2019-01-29T02:25:26.448941: step 999, loss 0.120579, acc 1, learning_rate 0.00044191\n",
      "2019-01-29T02:25:26.574251: step 1000, loss 0.124147, acc 0.984375, learning_rate 0.000440999\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:25:26.601292: step 1000, loss 0.602568, acc 0.766416\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1000\n",
      "\n",
      "2019-01-29T02:25:26.937023: step 1001, loss 0.144741, acc 0.984375, learning_rate 0.000440091\n",
      "2019-01-29T02:25:27.062039: step 1002, loss 0.113511, acc 1, learning_rate 0.000439185\n",
      "2019-01-29T02:25:27.183999: step 1003, loss 0.122496, acc 1, learning_rate 0.000438281\n",
      "2019-01-29T02:25:27.311655: step 1004, loss 0.0949052, acc 1, learning_rate 0.00043738\n",
      "2019-01-29T02:25:27.434623: step 1005, loss 0.166234, acc 0.96875, learning_rate 0.000436481\n",
      "2019-01-29T02:25:27.560777: step 1006, loss 0.133573, acc 1, learning_rate 0.000435584\n",
      "2019-01-29T02:25:27.650306: step 1007, loss 0.146595, acc 0.96875, learning_rate 0.00043469\n",
      "2019-01-29T02:25:27.779724: step 1008, loss 0.110432, acc 1, learning_rate 0.000433799\n",
      "2019-01-29T02:25:27.900352: step 1009, loss 0.134003, acc 0.984375, learning_rate 0.000432909\n",
      "2019-01-29T02:25:28.023090: step 1010, loss 0.108722, acc 0.984375, learning_rate 0.000432022\n",
      "2019-01-29T02:25:28.153699: step 1011, loss 0.107381, acc 1, learning_rate 0.000431138\n",
      "2019-01-29T02:25:28.281222: step 1012, loss 0.0996247, acc 1, learning_rate 0.000430256\n",
      "2019-01-29T02:25:28.371001: step 1013, loss 0.115747, acc 0.984375, learning_rate 0.000429376\n",
      "2019-01-29T02:25:28.492920: step 1014, loss 0.0940748, acc 1, learning_rate 0.000428498\n",
      "2019-01-29T02:25:28.624819: step 1015, loss 0.125941, acc 1, learning_rate 0.000427623\n",
      "2019-01-29T02:25:28.746889: step 1016, loss 0.132628, acc 0.984375, learning_rate 0.00042675\n",
      "2019-01-29T02:25:28.877385: step 1017, loss 0.108066, acc 1, learning_rate 0.00042588\n",
      "2019-01-29T02:25:29.002065: step 1018, loss 0.134336, acc 0.96875, learning_rate 0.000425011\n",
      "2019-01-29T02:25:29.132314: step 1019, loss 0.0884738, acc 1, learning_rate 0.000424145\n",
      "2019-01-29T02:25:29.254352: step 1020, loss 0.094515, acc 1, learning_rate 0.000423282\n",
      "2019-01-29T02:25:29.382140: step 1021, loss 0.100131, acc 1, learning_rate 0.000422421\n",
      "2019-01-29T02:25:29.509432: step 1022, loss 0.0913218, acc 1, learning_rate 0.000421562\n",
      "2019-01-29T02:25:29.680867: step 1023, loss 0.108226, acc 1, learning_rate 0.000420705\n",
      "2019-01-29T02:25:29.805799: step 1024, loss 0.103891, acc 1, learning_rate 0.00041985\n",
      "2019-01-29T02:25:29.932467: step 1025, loss 0.0929369, acc 1, learning_rate 0.000418998\n",
      "2019-01-29T02:25:30.062484: step 1026, loss 0.117104, acc 0.984375, learning_rate 0.000418148\n",
      "2019-01-29T02:25:30.189055: step 1027, loss 0.125987, acc 0.96875, learning_rate 0.000417301\n",
      "2019-01-29T02:25:30.276561: step 1028, loss 0.116599, acc 0.984375, learning_rate 0.000416455\n",
      "2019-01-29T02:25:30.401821: step 1029, loss 0.115214, acc 1, learning_rate 0.000415612\n",
      "2019-01-29T02:25:30.524907: step 1030, loss 0.135399, acc 1, learning_rate 0.000414771\n",
      "2019-01-29T02:25:30.652332: step 1031, loss 0.092977, acc 1, learning_rate 0.000413933\n",
      "2019-01-29T02:25:30.741254: step 1032, loss 0.0956107, acc 1, learning_rate 0.000413096\n",
      "2019-01-29T02:25:30.831222: step 1033, loss 0.0910958, acc 1, learning_rate 0.000412262\n",
      "2019-01-29T02:25:30.954573: step 1034, loss 0.0947902, acc 1, learning_rate 0.00041143\n",
      "2019-01-29T02:25:31.062804: step 1035, loss 0.107518, acc 1, learning_rate 0.000410601\n",
      "2019-01-29T02:25:31.192337: step 1036, loss 0.110997, acc 1, learning_rate 0.000409773\n",
      "2019-01-29T02:25:31.322503: step 1037, loss 0.11956, acc 0.984375, learning_rate 0.000408948\n",
      "2019-01-29T02:25:31.413270: step 1038, loss 0.162441, acc 0.953125, learning_rate 0.000408125\n",
      "2019-01-29T02:25:31.539194: step 1039, loss 0.083787, acc 1, learning_rate 0.000407304\n",
      "2019-01-29T02:25:31.627093: step 1040, loss 0.14905, acc 0.984375, learning_rate 0.000406485\n",
      "2019-01-29T02:25:31.755191: step 1041, loss 0.113858, acc 0.984375, learning_rate 0.000405668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:25:31.882834: step 1042, loss 0.120715, acc 1, learning_rate 0.000404854\n",
      "2019-01-29T02:25:32.011173: step 1043, loss 0.131517, acc 1, learning_rate 0.000404042\n",
      "2019-01-29T02:25:32.098826: step 1044, loss 0.115194, acc 0.984375, learning_rate 0.000403232\n",
      "2019-01-29T02:25:32.221453: step 1045, loss 0.0938163, acc 1, learning_rate 0.000402424\n",
      "2019-01-29T02:25:32.351409: step 1046, loss 0.131937, acc 0.984375, learning_rate 0.000401618\n",
      "2019-01-29T02:25:32.478044: step 1047, loss 0.100341, acc 1, learning_rate 0.000400815\n",
      "2019-01-29T02:25:32.604573: step 1048, loss 0.104978, acc 1, learning_rate 0.000400013\n",
      "2019-01-29T02:25:32.693414: step 1049, loss 0.10724, acc 1, learning_rate 0.000399214\n",
      "2019-01-29T02:25:32.815646: step 1050, loss 0.0987492, acc 1, learning_rate 0.000398417\n",
      "2019-01-29T02:25:32.941286: step 1051, loss 0.123082, acc 0.984375, learning_rate 0.000397622\n",
      "2019-01-29T02:25:33.026487: step 1052, loss 0.09958, acc 1, learning_rate 0.000396829\n",
      "2019-01-29T02:25:33.150007: step 1053, loss 0.106883, acc 1, learning_rate 0.000396038\n",
      "2019-01-29T02:25:33.237248: step 1054, loss 0.0815726, acc 1, learning_rate 0.000395249\n",
      "2019-01-29T02:25:33.358151: step 1055, loss 0.104579, acc 1, learning_rate 0.000394463\n",
      "2019-01-29T02:25:33.488017: step 1056, loss 0.0847144, acc 1, learning_rate 0.000393678\n",
      "2019-01-29T02:25:33.621692: step 1057, loss 0.0949092, acc 1, learning_rate 0.000392896\n",
      "2019-01-29T02:25:33.748679: step 1058, loss 0.107774, acc 1, learning_rate 0.000392115\n",
      "2019-01-29T02:25:33.876732: step 1059, loss 0.120392, acc 1, learning_rate 0.000391337\n",
      "2019-01-29T02:25:34.000404: step 1060, loss 0.0938857, acc 0.984375, learning_rate 0.000390561\n",
      "2019-01-29T02:25:34.090711: step 1061, loss 0.10248, acc 1, learning_rate 0.000389787\n",
      "2019-01-29T02:25:34.217378: step 1062, loss 0.122177, acc 0.984375, learning_rate 0.000389015\n",
      "2019-01-29T02:25:34.340647: step 1063, loss 0.107044, acc 1, learning_rate 0.000388245\n",
      "2019-01-29T02:25:34.467476: step 1064, loss 0.133884, acc 0.96875, learning_rate 0.000387477\n",
      "2019-01-29T02:25:34.599978: step 1065, loss 0.111611, acc 0.984375, learning_rate 0.000386711\n",
      "2019-01-29T02:25:34.763769: step 1066, loss 0.159496, acc 0.96875, learning_rate 0.000385947\n",
      "2019-01-29T02:25:34.886670: step 1067, loss 0.101001, acc 1, learning_rate 0.000385185\n",
      "2019-01-29T02:25:35.012009: step 1068, loss 0.0874853, acc 1, learning_rate 0.000384425\n",
      "2019-01-29T02:25:35.143036: step 1069, loss 0.128398, acc 0.984375, learning_rate 0.000383668\n",
      "2019-01-29T02:25:35.227176: step 1070, loss 0.102198, acc 1, learning_rate 0.000382912\n",
      "2019-01-29T02:25:35.354494: step 1071, loss 0.0939936, acc 0.984375, learning_rate 0.000382158\n",
      "2019-01-29T02:25:35.439666: step 1072, loss 0.0964591, acc 1, learning_rate 0.000381406\n",
      "2019-01-29T02:25:35.566527: step 1073, loss 0.125808, acc 0.984375, learning_rate 0.000380657\n",
      "2019-01-29T02:25:35.686580: step 1074, loss 0.0880178, acc 1, learning_rate 0.000379909\n",
      "2019-01-29T02:25:35.818208: step 1075, loss 0.11151, acc 1, learning_rate 0.000379163\n",
      "2019-01-29T02:25:35.944669: step 1076, loss 0.0942811, acc 1, learning_rate 0.000378419\n",
      "2019-01-29T02:25:36.051454: step 1077, loss 0.0935128, acc 1, learning_rate 0.000377678\n",
      "2019-01-29T02:25:36.176565: step 1078, loss 0.110676, acc 1, learning_rate 0.000376938\n",
      "2019-01-29T02:25:36.305444: step 1079, loss 0.0936693, acc 1, learning_rate 0.0003762\n",
      "2019-01-29T02:25:36.436374: step 1080, loss 0.0919221, acc 1, learning_rate 0.000375464\n",
      "2019-01-29T02:25:36.566007: step 1081, loss 0.101317, acc 1, learning_rate 0.00037473\n",
      "2019-01-29T02:25:36.690395: step 1082, loss 0.129027, acc 0.984375, learning_rate 0.000373998\n",
      "2019-01-29T02:25:36.816842: step 1083, loss 0.100326, acc 1, learning_rate 0.000373268\n",
      "2019-01-29T02:25:36.941249: step 1084, loss 0.116535, acc 1, learning_rate 0.00037254\n",
      "2019-01-29T02:25:37.030961: step 1085, loss 0.100355, acc 1, learning_rate 0.000371814\n",
      "2019-01-29T02:25:37.154249: step 1086, loss 0.134078, acc 0.984375, learning_rate 0.00037109\n",
      "2019-01-29T02:25:37.283312: step 1087, loss 0.104391, acc 1, learning_rate 0.000370368\n",
      "2019-01-29T02:25:37.401352: step 1088, loss 0.107564, acc 0.984375, learning_rate 0.000369647\n",
      "2019-01-29T02:25:37.532735: step 1089, loss 0.0976148, acc 1, learning_rate 0.000368929\n",
      "2019-01-29T02:25:37.656914: step 1090, loss 0.0944179, acc 1, learning_rate 0.000368213\n",
      "2019-01-29T02:25:37.780046: step 1091, loss 0.10305, acc 1, learning_rate 0.000367498\n",
      "2019-01-29T02:25:37.903006: step 1092, loss 0.11161, acc 1, learning_rate 0.000366785\n",
      "2019-01-29T02:25:38.034008: step 1093, loss 0.0946598, acc 1, learning_rate 0.000366075\n",
      "2019-01-29T02:25:38.162123: step 1094, loss 0.167856, acc 0.953125, learning_rate 0.000365366\n",
      "2019-01-29T02:25:38.286312: step 1095, loss 0.135538, acc 0.96875, learning_rate 0.000364659\n",
      "2019-01-29T02:25:38.414756: step 1096, loss 0.139329, acc 0.984375, learning_rate 0.000363954\n",
      "2019-01-29T02:25:38.548623: step 1097, loss 0.0838283, acc 1, learning_rate 0.00036325\n",
      "2019-01-29T02:25:38.680915: step 1098, loss 0.0945057, acc 1, learning_rate 0.000362549\n",
      "2019-01-29T02:25:38.771542: step 1099, loss 0.104432, acc 1, learning_rate 0.00036185\n",
      "2019-01-29T02:25:38.896162: step 1100, loss 0.0972478, acc 1, learning_rate 0.000361152\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:25:38.923618: step 1100, loss 0.610044, acc 0.761726\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1100\n",
      "\n",
      "2019-01-29T02:25:39.271227: step 1101, loss 0.183305, acc 0.96875, learning_rate 0.000360456\n",
      "2019-01-29T02:25:39.402958: step 1102, loss 0.0951645, acc 1, learning_rate 0.000359762\n",
      "2019-01-29T02:25:39.526911: step 1103, loss 0.10715, acc 1, learning_rate 0.00035907\n",
      "2019-01-29T02:25:39.651842: step 1104, loss 0.101879, acc 0.984375, learning_rate 0.00035838\n",
      "2019-01-29T02:25:39.778855: step 1105, loss 0.112004, acc 1, learning_rate 0.000357692\n",
      "2019-01-29T02:25:39.907012: step 1106, loss 0.0878003, acc 1, learning_rate 0.000357005\n",
      "2019-01-29T02:25:40.034247: step 1107, loss 0.0923276, acc 1, learning_rate 0.00035632\n",
      "2019-01-29T02:25:40.161149: step 1108, loss 0.0979235, acc 1, learning_rate 0.000355637\n",
      "2019-01-29T02:25:40.286196: step 1109, loss 0.100898, acc 0.984375, learning_rate 0.000354956\n",
      "2019-01-29T02:25:40.375748: step 1110, loss 0.0814355, acc 1, learning_rate 0.000354277\n",
      "2019-01-29T02:25:40.474167: step 1111, loss 0.107258, acc 0.984375, learning_rate 0.0003536\n",
      "2019-01-29T02:25:40.596322: step 1112, loss 0.116268, acc 1, learning_rate 0.000352924\n",
      "2019-01-29T02:25:40.692532: step 1113, loss 0.110481, acc 0.984375, learning_rate 0.00035225\n",
      "2019-01-29T02:25:40.813603: step 1114, loss 0.0993986, acc 1, learning_rate 0.000351578\n",
      "2019-01-29T02:25:40.940351: step 1115, loss 0.190079, acc 0.9375, learning_rate 0.000350908\n",
      "2019-01-29T02:25:41.028862: step 1116, loss 0.129736, acc 0.984375, learning_rate 0.000350239\n",
      "2019-01-29T02:25:41.116864: step 1117, loss 0.117782, acc 0.984375, learning_rate 0.000349573\n",
      "2019-01-29T02:25:41.242657: step 1118, loss 0.0925191, acc 1, learning_rate 0.000348908\n",
      "2019-01-29T02:25:41.366976: step 1119, loss 0.0908479, acc 1, learning_rate 0.000348245\n",
      "2019-01-29T02:25:41.491088: step 1120, loss 0.133841, acc 0.953125, learning_rate 0.000347583\n",
      "2019-01-29T02:25:41.581501: step 1121, loss 0.100986, acc 1, learning_rate 0.000346924\n",
      "2019-01-29T02:25:41.667847: step 1122, loss 0.106205, acc 0.984375, learning_rate 0.000346266\n",
      "2019-01-29T02:25:41.796841: step 1123, loss 0.126757, acc 0.984375, learning_rate 0.00034561\n",
      "2019-01-29T02:25:41.919930: step 1124, loss 0.134903, acc 0.96875, learning_rate 0.000344955\n",
      "2019-01-29T02:25:42.047770: step 1125, loss 0.126089, acc 0.984375, learning_rate 0.000344303\n",
      "2019-01-29T02:25:42.170243: step 1126, loss 0.13774, acc 0.984375, learning_rate 0.000343652\n",
      "2019-01-29T02:25:42.297467: step 1127, loss 0.142703, acc 0.953125, learning_rate 0.000343003\n",
      "2019-01-29T02:25:42.424626: step 1128, loss 0.115692, acc 0.984375, learning_rate 0.000342355\n",
      "2019-01-29T02:25:42.509019: step 1129, loss 0.120493, acc 1, learning_rate 0.00034171\n",
      "2019-01-29T02:25:42.638583: step 1130, loss 0.08597, acc 1, learning_rate 0.000341066\n",
      "2019-01-29T02:25:42.726363: step 1131, loss 0.0808667, acc 1, learning_rate 0.000340423\n",
      "2019-01-29T02:25:42.859131: step 1132, loss 0.0978349, acc 1, learning_rate 0.000339783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:25:42.986355: step 1133, loss 0.0935176, acc 1, learning_rate 0.000339144\n",
      "2019-01-29T02:25:43.075992: step 1134, loss 0.108588, acc 1, learning_rate 0.000338507\n",
      "2019-01-29T02:25:43.203023: step 1135, loss 0.120119, acc 0.984375, learning_rate 0.000337871\n",
      "2019-01-29T02:25:43.292409: step 1136, loss 0.0803164, acc 1, learning_rate 0.000337238\n",
      "2019-01-29T02:25:43.378468: step 1137, loss 0.119634, acc 0.984375, learning_rate 0.000336606\n",
      "2019-01-29T02:25:43.504745: step 1138, loss 0.125768, acc 0.984375, learning_rate 0.000335975\n",
      "2019-01-29T02:25:43.627289: step 1139, loss 0.111227, acc 0.984375, learning_rate 0.000335346\n",
      "2019-01-29T02:25:43.757225: step 1140, loss 0.10181, acc 1, learning_rate 0.000334719\n",
      "2019-01-29T02:25:43.887521: step 1141, loss 0.100189, acc 1, learning_rate 0.000334094\n",
      "2019-01-29T02:25:44.013926: step 1142, loss 0.0842151, acc 1, learning_rate 0.00033347\n",
      "2019-01-29T02:25:44.102096: step 1143, loss 0.129958, acc 0.984375, learning_rate 0.000332848\n",
      "2019-01-29T02:25:44.224935: step 1144, loss 0.09704, acc 1, learning_rate 0.000332228\n",
      "2019-01-29T02:25:44.355352: step 1145, loss 0.093489, acc 0.984375, learning_rate 0.000331609\n",
      "2019-01-29T02:25:44.445299: step 1146, loss 0.0926525, acc 1, learning_rate 0.000330992\n",
      "2019-01-29T02:25:44.570592: step 1147, loss 0.146832, acc 0.984375, learning_rate 0.000330377\n",
      "2019-01-29T02:25:44.689418: step 1148, loss 0.0930899, acc 1, learning_rate 0.000329763\n",
      "2019-01-29T02:25:44.814451: step 1149, loss 0.088972, acc 1, learning_rate 0.000329151\n",
      "2019-01-29T02:25:44.934744: step 1150, loss 0.111092, acc 0.953125, learning_rate 0.00032854\n",
      "2019-01-29T02:25:45.060289: step 1151, loss 0.098046, acc 1, learning_rate 0.000327932\n",
      "2019-01-29T02:25:45.188062: step 1152, loss 0.121449, acc 0.984375, learning_rate 0.000327324\n",
      "2019-01-29T02:25:45.316147: step 1153, loss 0.105425, acc 1, learning_rate 0.000326719\n",
      "2019-01-29T02:25:45.438632: step 1154, loss 0.109565, acc 0.984375, learning_rate 0.000326115\n",
      "2019-01-29T02:25:45.564828: step 1155, loss 0.134639, acc 0.96875, learning_rate 0.000325512\n",
      "2019-01-29T02:25:45.689806: step 1156, loss 0.100202, acc 1, learning_rate 0.000324911\n",
      "2019-01-29T02:25:45.812461: step 1157, loss 0.116008, acc 0.984375, learning_rate 0.000324312\n",
      "2019-01-29T02:25:45.942799: step 1158, loss 0.12346, acc 0.984375, learning_rate 0.000323715\n",
      "2019-01-29T02:25:46.065531: step 1159, loss 0.122984, acc 1, learning_rate 0.000323119\n",
      "2019-01-29T02:25:46.188108: step 1160, loss 0.124583, acc 0.96875, learning_rate 0.000322524\n",
      "2019-01-29T02:25:46.309747: step 1161, loss 0.10112, acc 0.984375, learning_rate 0.000321931\n",
      "2019-01-29T02:25:46.431261: step 1162, loss 0.0821973, acc 1, learning_rate 0.00032134\n",
      "2019-01-29T02:25:46.560785: step 1163, loss 0.101856, acc 1, learning_rate 0.00032075\n",
      "2019-01-29T02:25:46.648665: step 1164, loss 0.109315, acc 1, learning_rate 0.000320162\n",
      "2019-01-29T02:25:46.772335: step 1165, loss 0.1524, acc 0.984375, learning_rate 0.000319576\n",
      "2019-01-29T02:25:46.896282: step 1166, loss 0.105364, acc 1, learning_rate 0.000318991\n",
      "2019-01-29T02:25:46.983036: step 1167, loss 0.125614, acc 0.984375, learning_rate 0.000318407\n",
      "2019-01-29T02:25:47.067568: step 1168, loss 0.10784, acc 1, learning_rate 0.000317825\n",
      "2019-01-29T02:25:47.192003: step 1169, loss 0.0904606, acc 1, learning_rate 0.000317245\n",
      "2019-01-29T02:25:47.320005: step 1170, loss 0.110254, acc 0.96875, learning_rate 0.000316666\n",
      "2019-01-29T02:25:47.444222: step 1171, loss 0.086026, acc 1, learning_rate 0.000316089\n",
      "2019-01-29T02:25:47.567457: step 1172, loss 0.153829, acc 0.953125, learning_rate 0.000315513\n",
      "2019-01-29T02:25:47.656816: step 1173, loss 0.11633, acc 1, learning_rate 0.000314939\n",
      "2019-01-29T02:25:47.778041: step 1174, loss 0.0977365, acc 0.984375, learning_rate 0.000314366\n",
      "2019-01-29T02:25:47.897783: step 1175, loss 0.0945363, acc 0.984375, learning_rate 0.000313795\n",
      "2019-01-29T02:25:47.988811: step 1176, loss 0.114419, acc 0.984375, learning_rate 0.000313226\n",
      "2019-01-29T02:25:48.111378: step 1177, loss 0.0995577, acc 1, learning_rate 0.000312658\n",
      "2019-01-29T02:25:48.234282: step 1178, loss 0.137853, acc 1, learning_rate 0.000312091\n",
      "2019-01-29T02:25:48.353458: step 1179, loss 0.115745, acc 0.984375, learning_rate 0.000311526\n",
      "2019-01-29T02:25:48.483862: step 1180, loss 0.12622, acc 0.984375, learning_rate 0.000310962\n",
      "2019-01-29T02:25:48.612989: step 1181, loss 0.107223, acc 0.984375, learning_rate 0.0003104\n",
      "2019-01-29T02:25:48.729949: step 1182, loss 0.0831954, acc 1, learning_rate 0.00030984\n",
      "2019-01-29T02:25:48.856001: step 1183, loss 0.0932152, acc 1, learning_rate 0.000309281\n",
      "2019-01-29T02:25:48.975577: step 1184, loss 0.12438, acc 0.984375, learning_rate 0.000308723\n",
      "2019-01-29T02:25:49.101215: step 1185, loss 0.119003, acc 0.984375, learning_rate 0.000308167\n",
      "2019-01-29T02:25:49.227321: step 1186, loss 0.0831245, acc 1, learning_rate 0.000307612\n",
      "2019-01-29T02:25:49.352009: step 1187, loss 0.0923478, acc 1, learning_rate 0.000307059\n",
      "2019-01-29T02:25:49.472883: step 1188, loss 0.127869, acc 0.984375, learning_rate 0.000306508\n",
      "2019-01-29T02:25:49.599821: step 1189, loss 0.117693, acc 0.984375, learning_rate 0.000305958\n",
      "2019-01-29T02:25:49.729405: step 1190, loss 0.121917, acc 0.96875, learning_rate 0.000305409\n",
      "2019-01-29T02:25:49.853196: step 1191, loss 0.104744, acc 1, learning_rate 0.000304862\n",
      "2019-01-29T02:25:49.940621: step 1192, loss 0.0963889, acc 0.984375, learning_rate 0.000304316\n",
      "2019-01-29T02:25:50.031569: step 1193, loss 0.11397, acc 0.984375, learning_rate 0.000303771\n",
      "2019-01-29T02:25:50.157514: step 1194, loss 0.0843806, acc 1, learning_rate 0.000303229\n",
      "2019-01-29T02:25:50.288351: step 1195, loss 0.0938289, acc 1, learning_rate 0.000302687\n",
      "2019-01-29T02:25:50.411289: step 1196, loss 0.102024, acc 0.984375, learning_rate 0.000302147\n",
      "2019-01-29T02:25:50.540133: step 1197, loss 0.121284, acc 0.984375, learning_rate 0.000301609\n",
      "2019-01-29T02:25:50.628287: step 1198, loss 0.11575, acc 1, learning_rate 0.000301071\n",
      "2019-01-29T02:25:50.753525: step 1199, loss 0.129472, acc 0.984375, learning_rate 0.000300536\n",
      "2019-01-29T02:25:50.881412: step 1200, loss 0.0948609, acc 0.983333, learning_rate 0.000300001\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:25:50.908321: step 1200, loss 0.608037, acc 0.760788\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1200\n",
      "\n",
      "2019-01-29T02:25:51.223155: step 1201, loss 0.0985262, acc 1, learning_rate 0.000299469\n",
      "2019-01-29T02:25:51.348128: step 1202, loss 0.0813637, acc 1, learning_rate 0.000298937\n",
      "2019-01-29T02:25:51.434092: step 1203, loss 0.149838, acc 0.96875, learning_rate 0.000298407\n",
      "2019-01-29T02:25:51.557742: step 1204, loss 0.0925304, acc 0.984375, learning_rate 0.000297879\n",
      "2019-01-29T02:25:51.690125: step 1205, loss 0.0898067, acc 1, learning_rate 0.000297351\n",
      "2019-01-29T02:25:51.818194: step 1206, loss 0.153737, acc 0.96875, learning_rate 0.000296826\n",
      "2019-01-29T02:25:51.909523: step 1207, loss 0.114867, acc 0.984375, learning_rate 0.000296301\n",
      "2019-01-29T02:25:52.032420: step 1208, loss 0.0902708, acc 0.984375, learning_rate 0.000295778\n",
      "2019-01-29T02:25:52.159740: step 1209, loss 0.104221, acc 1, learning_rate 0.000295257\n",
      "2019-01-29T02:25:52.284656: step 1210, loss 0.0879328, acc 1, learning_rate 0.000294736\n",
      "2019-01-29T02:25:52.410386: step 1211, loss 0.11854, acc 1, learning_rate 0.000294218\n",
      "2019-01-29T02:25:52.537369: step 1212, loss 0.100416, acc 0.984375, learning_rate 0.0002937\n",
      "2019-01-29T02:25:52.664536: step 1213, loss 0.0871016, acc 1, learning_rate 0.000293184\n",
      "2019-01-29T02:25:52.780852: step 1214, loss 0.0878607, acc 1, learning_rate 0.000292669\n",
      "2019-01-29T02:25:52.904609: step 1215, loss 0.0887725, acc 1, learning_rate 0.000292156\n",
      "2019-01-29T02:25:53.031916: step 1216, loss 0.0974822, acc 1, learning_rate 0.000291644\n",
      "2019-01-29T02:25:53.157526: step 1217, loss 0.105979, acc 0.984375, learning_rate 0.000291134\n",
      "2019-01-29T02:25:53.284344: step 1218, loss 0.143585, acc 0.984375, learning_rate 0.000290624\n",
      "2019-01-29T02:25:53.409492: step 1219, loss 0.124314, acc 1, learning_rate 0.000290116\n",
      "2019-01-29T02:25:53.535333: step 1220, loss 0.0985405, acc 1, learning_rate 0.00028961\n",
      "2019-01-29T02:25:53.663932: step 1221, loss 0.0926585, acc 1, learning_rate 0.000289105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:25:53.789283: step 1222, loss 0.0695456, acc 1, learning_rate 0.000288601\n",
      "2019-01-29T02:25:53.879887: step 1223, loss 0.129174, acc 0.984375, learning_rate 0.000288098\n",
      "2019-01-29T02:25:54.007164: step 1224, loss 0.0741789, acc 1, learning_rate 0.000287597\n",
      "2019-01-29T02:25:54.127086: step 1225, loss 0.0946592, acc 1, learning_rate 0.000287098\n",
      "2019-01-29T02:25:54.254388: step 1226, loss 0.094707, acc 1, learning_rate 0.000286599\n",
      "2019-01-29T02:25:54.376447: step 1227, loss 0.0682561, acc 1, learning_rate 0.000286102\n",
      "2019-01-29T02:25:54.463927: step 1228, loss 0.0992643, acc 1, learning_rate 0.000285606\n",
      "2019-01-29T02:25:54.587842: step 1229, loss 0.103704, acc 0.984375, learning_rate 0.000285112\n",
      "2019-01-29T02:25:54.716956: step 1230, loss 0.0987542, acc 1, learning_rate 0.000284618\n",
      "2019-01-29T02:25:54.807042: step 1231, loss 0.0866365, acc 1, learning_rate 0.000284127\n",
      "2019-01-29T02:25:54.938369: step 1232, loss 0.172359, acc 0.953125, learning_rate 0.000283636\n",
      "2019-01-29T02:25:55.064877: step 1233, loss 0.110589, acc 0.984375, learning_rate 0.000283147\n",
      "2019-01-29T02:25:55.194218: step 1234, loss 0.0908984, acc 1, learning_rate 0.000282659\n",
      "2019-01-29T02:25:55.318306: step 1235, loss 0.120585, acc 0.984375, learning_rate 0.000282172\n",
      "2019-01-29T02:25:55.442905: step 1236, loss 0.121943, acc 0.984375, learning_rate 0.000281687\n",
      "2019-01-29T02:25:55.569629: step 1237, loss 0.107264, acc 0.984375, learning_rate 0.000281203\n",
      "2019-01-29T02:25:55.694058: step 1238, loss 0.0978237, acc 1, learning_rate 0.00028072\n",
      "2019-01-29T02:25:55.783757: step 1239, loss 0.0912275, acc 1, learning_rate 0.000280239\n",
      "2019-01-29T02:25:55.908771: step 1240, loss 0.118995, acc 0.984375, learning_rate 0.000279758\n",
      "2019-01-29T02:25:56.039037: step 1241, loss 0.150422, acc 0.96875, learning_rate 0.000279279\n",
      "2019-01-29T02:25:56.163386: step 1242, loss 0.0858473, acc 1, learning_rate 0.000278802\n",
      "2019-01-29T02:25:56.295426: step 1243, loss 0.124715, acc 0.984375, learning_rate 0.000278325\n",
      "2019-01-29T02:25:56.420564: step 1244, loss 0.103852, acc 0.984375, learning_rate 0.00027785\n",
      "2019-01-29T02:25:56.541068: step 1245, loss 0.12028, acc 0.984375, learning_rate 0.000277376\n",
      "2019-01-29T02:25:56.673282: step 1246, loss 0.10385, acc 0.984375, learning_rate 0.000276904\n",
      "2019-01-29T02:25:56.799370: step 1247, loss 0.0803163, acc 1, learning_rate 0.000276433\n",
      "2019-01-29T02:25:56.923142: step 1248, loss 0.150387, acc 0.953125, learning_rate 0.000275963\n",
      "2019-01-29T02:25:57.043784: step 1249, loss 0.0778558, acc 1, learning_rate 0.000275494\n",
      "2019-01-29T02:25:57.169738: step 1250, loss 0.104223, acc 0.984375, learning_rate 0.000275026\n",
      "2019-01-29T02:25:57.302458: step 1251, loss 0.124029, acc 0.984375, learning_rate 0.00027456\n",
      "2019-01-29T02:25:57.425237: step 1252, loss 0.0989662, acc 1, learning_rate 0.000274095\n",
      "2019-01-29T02:25:57.549804: step 1253, loss 0.0862902, acc 1, learning_rate 0.000273631\n",
      "2019-01-29T02:25:57.680514: step 1254, loss 0.102901, acc 1, learning_rate 0.000273168\n",
      "2019-01-29T02:25:57.804067: step 1255, loss 0.098058, acc 1, learning_rate 0.000272707\n",
      "2019-01-29T02:25:57.929268: step 1256, loss 0.084513, acc 1, learning_rate 0.000272247\n",
      "2019-01-29T02:25:58.052611: step 1257, loss 0.101934, acc 1, learning_rate 0.000271788\n",
      "2019-01-29T02:25:58.175916: step 1258, loss 0.090809, acc 1, learning_rate 0.00027133\n",
      "2019-01-29T02:25:58.299808: step 1259, loss 0.091124, acc 1, learning_rate 0.000270874\n",
      "2019-01-29T02:25:58.386056: step 1260, loss 0.104404, acc 0.984375, learning_rate 0.000270419\n",
      "2019-01-29T02:25:58.511875: step 1261, loss 0.112226, acc 1, learning_rate 0.000269965\n",
      "2019-01-29T02:25:58.634095: step 1262, loss 0.0905462, acc 0.984375, learning_rate 0.000269512\n",
      "2019-01-29T02:25:58.759412: step 1263, loss 0.102988, acc 0.984375, learning_rate 0.00026906\n",
      "2019-01-29T02:25:58.885611: step 1264, loss 0.110227, acc 0.984375, learning_rate 0.00026861\n",
      "2019-01-29T02:25:59.008094: step 1265, loss 0.117288, acc 0.984375, learning_rate 0.00026816\n",
      "2019-01-29T02:25:59.138674: step 1266, loss 0.102039, acc 1, learning_rate 0.000267712\n",
      "2019-01-29T02:25:59.266335: step 1267, loss 0.0965768, acc 1, learning_rate 0.000267266\n",
      "2019-01-29T02:25:59.393390: step 1268, loss 0.116191, acc 0.984375, learning_rate 0.00026682\n",
      "2019-01-29T02:25:59.481297: step 1269, loss 0.101544, acc 0.984375, learning_rate 0.000266376\n",
      "2019-01-29T02:25:59.567988: step 1270, loss 0.114253, acc 0.96875, learning_rate 0.000265932\n",
      "2019-01-29T02:25:59.693012: step 1271, loss 0.0803595, acc 1, learning_rate 0.00026549\n",
      "2019-01-29T02:25:59.816151: step 1272, loss 0.114344, acc 0.984375, learning_rate 0.000265049\n",
      "2019-01-29T02:25:59.948817: step 1273, loss 0.120604, acc 0.984375, learning_rate 0.00026461\n",
      "2019-01-29T02:26:00.071504: step 1274, loss 0.148564, acc 0.96875, learning_rate 0.000264171\n",
      "2019-01-29T02:26:00.203549: step 1275, loss 0.0966007, acc 1, learning_rate 0.000263734\n",
      "2019-01-29T02:26:00.287421: step 1276, loss 0.105954, acc 1, learning_rate 0.000263297\n",
      "2019-01-29T02:26:00.417490: step 1277, loss 0.103179, acc 0.984375, learning_rate 0.000262862\n",
      "2019-01-29T02:26:00.542891: step 1278, loss 0.121207, acc 0.984375, learning_rate 0.000262428\n",
      "2019-01-29T02:26:00.663527: step 1279, loss 0.0791561, acc 1, learning_rate 0.000261996\n",
      "2019-01-29T02:26:00.786820: step 1280, loss 0.093039, acc 1, learning_rate 0.000261564\n",
      "2019-01-29T02:26:00.916979: step 1281, loss 0.0940762, acc 1, learning_rate 0.000261134\n",
      "2019-01-29T02:26:01.047034: step 1282, loss 0.119117, acc 0.96875, learning_rate 0.000260704\n",
      "2019-01-29T02:26:01.172252: step 1283, loss 0.135636, acc 0.984375, learning_rate 0.000260276\n",
      "2019-01-29T02:26:01.299469: step 1284, loss 0.0929878, acc 0.984375, learning_rate 0.000259849\n",
      "2019-01-29T02:26:01.390009: step 1285, loss 0.086308, acc 1, learning_rate 0.000259423\n",
      "2019-01-29T02:26:01.478484: step 1286, loss 0.115651, acc 1, learning_rate 0.000258999\n",
      "2019-01-29T02:26:01.608562: step 1287, loss 0.0973745, acc 1, learning_rate 0.000258575\n",
      "2019-01-29T02:26:01.693779: step 1288, loss 0.119857, acc 0.984375, learning_rate 0.000258153\n",
      "2019-01-29T02:26:01.821146: step 1289, loss 0.0957643, acc 1, learning_rate 0.000257731\n",
      "2019-01-29T02:26:01.947376: step 1290, loss 0.118055, acc 0.984375, learning_rate 0.000257311\n",
      "2019-01-29T02:26:02.076954: step 1291, loss 0.109599, acc 0.984375, learning_rate 0.000256892\n",
      "2019-01-29T02:26:02.200221: step 1292, loss 0.151476, acc 0.953125, learning_rate 0.000256474\n",
      "2019-01-29T02:26:02.317177: step 1293, loss 0.133547, acc 0.984375, learning_rate 0.000256057\n",
      "2019-01-29T02:26:02.448216: step 1294, loss 0.0854476, acc 1, learning_rate 0.000255641\n",
      "2019-01-29T02:26:02.573133: step 1295, loss 0.0972502, acc 1, learning_rate 0.000255227\n",
      "2019-01-29T02:26:02.660550: step 1296, loss 0.0806897, acc 1, learning_rate 0.000254813\n",
      "2019-01-29T02:26:02.778201: step 1297, loss 0.10979, acc 0.984375, learning_rate 0.000254401\n",
      "2019-01-29T02:26:02.862761: step 1298, loss 0.145341, acc 1, learning_rate 0.000253989\n",
      "2019-01-29T02:26:02.988377: step 1299, loss 0.0989205, acc 1, learning_rate 0.000253579\n",
      "2019-01-29T02:26:03.114706: step 1300, loss 0.118962, acc 0.96875, learning_rate 0.00025317\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:26:03.142506: step 1300, loss 0.615104, acc 0.769231\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1300\n",
      "\n",
      "2019-01-29T02:26:03.487775: step 1301, loss 0.109193, acc 1, learning_rate 0.000252762\n",
      "2019-01-29T02:26:03.612817: step 1302, loss 0.116486, acc 0.984375, learning_rate 0.000252355\n",
      "2019-01-29T02:26:03.736614: step 1303, loss 0.110201, acc 1, learning_rate 0.000251949\n",
      "2019-01-29T02:26:03.866189: step 1304, loss 0.0915975, acc 1, learning_rate 0.000251544\n",
      "2019-01-29T02:26:03.991928: step 1305, loss 0.108087, acc 0.984375, learning_rate 0.00025114\n",
      "2019-01-29T02:26:04.081315: step 1306, loss 0.114445, acc 0.984375, learning_rate 0.000250737\n",
      "2019-01-29T02:26:04.210854: step 1307, loss 0.0876265, acc 1, learning_rate 0.000250336\n",
      "2019-01-29T02:26:04.318468: step 1308, loss 0.10124, acc 1, learning_rate 0.000249935\n",
      "2019-01-29T02:26:04.407205: step 1309, loss 0.149098, acc 0.953125, learning_rate 0.000249536\n",
      "2019-01-29T02:26:04.536365: step 1310, loss 0.0976951, acc 0.984375, learning_rate 0.000249138\n",
      "2019-01-29T02:26:04.664339: step 1311, loss 0.0871453, acc 1, learning_rate 0.00024874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:26:04.754960: step 1312, loss 0.103299, acc 1, learning_rate 0.000248344\n",
      "2019-01-29T02:26:04.850532: step 1313, loss 0.0783894, acc 1, learning_rate 0.000247949\n",
      "2019-01-29T02:26:04.937892: step 1314, loss 0.106409, acc 0.984375, learning_rate 0.000247555\n",
      "2019-01-29T02:26:05.063866: step 1315, loss 0.109382, acc 0.984375, learning_rate 0.000247161\n",
      "2019-01-29T02:26:05.194122: step 1316, loss 0.0927511, acc 1, learning_rate 0.000246769\n",
      "2019-01-29T02:26:05.312864: step 1317, loss 0.0964888, acc 1, learning_rate 0.000246378\n",
      "2019-01-29T02:26:05.441131: step 1318, loss 0.107945, acc 0.984375, learning_rate 0.000245988\n",
      "2019-01-29T02:26:05.527891: step 1319, loss 0.0871295, acc 1, learning_rate 0.000245599\n",
      "2019-01-29T02:26:05.615459: step 1320, loss 0.107266, acc 1, learning_rate 0.000245211\n",
      "2019-01-29T02:26:05.717913: step 1321, loss 0.118191, acc 0.96875, learning_rate 0.000244825\n",
      "2019-01-29T02:26:05.806647: step 1322, loss 0.0967406, acc 1, learning_rate 0.000244439\n",
      "2019-01-29T02:26:05.926168: step 1323, loss 0.115102, acc 0.984375, learning_rate 0.000244054\n",
      "2019-01-29T02:26:06.049874: step 1324, loss 0.0947202, acc 1, learning_rate 0.00024367\n",
      "2019-01-29T02:26:06.177114: step 1325, loss 0.110503, acc 1, learning_rate 0.000243287\n",
      "2019-01-29T02:26:06.304003: step 1326, loss 0.101026, acc 1, learning_rate 0.000242906\n",
      "2019-01-29T02:26:06.428601: step 1327, loss 0.126573, acc 0.984375, learning_rate 0.000242525\n",
      "2019-01-29T02:26:06.552078: step 1328, loss 0.0716104, acc 1, learning_rate 0.000242145\n",
      "2019-01-29T02:26:06.636600: step 1329, loss 0.114661, acc 1, learning_rate 0.000241766\n",
      "2019-01-29T02:26:06.722869: step 1330, loss 0.089506, acc 1, learning_rate 0.000241389\n",
      "2019-01-29T02:26:06.844930: step 1331, loss 0.0893393, acc 1, learning_rate 0.000241012\n",
      "2019-01-29T02:26:06.952011: step 1332, loss 0.113615, acc 0.984375, learning_rate 0.000240636\n",
      "2019-01-29T02:26:07.077059: step 1333, loss 0.13752, acc 0.96875, learning_rate 0.000240262\n",
      "2019-01-29T02:26:07.203168: step 1334, loss 0.107806, acc 0.984375, learning_rate 0.000239888\n",
      "2019-01-29T02:26:07.332459: step 1335, loss 0.0846329, acc 1, learning_rate 0.000239515\n",
      "2019-01-29T02:26:07.459026: step 1336, loss 0.11702, acc 0.984375, learning_rate 0.000239144\n",
      "2019-01-29T02:26:07.589979: step 1337, loss 0.07908, acc 1, learning_rate 0.000238773\n",
      "2019-01-29T02:26:07.717952: step 1338, loss 0.103131, acc 0.984375, learning_rate 0.000238403\n",
      "2019-01-29T02:26:07.842639: step 1339, loss 0.0938206, acc 0.984375, learning_rate 0.000238034\n",
      "2019-01-29T02:26:07.969674: step 1340, loss 0.0964897, acc 0.984375, learning_rate 0.000237667\n",
      "2019-01-29T02:26:08.100940: step 1341, loss 0.117149, acc 1, learning_rate 0.0002373\n",
      "2019-01-29T02:26:08.228151: step 1342, loss 0.0760733, acc 1, learning_rate 0.000236934\n",
      "2019-01-29T02:26:08.354724: step 1343, loss 0.0767657, acc 1, learning_rate 0.000236569\n",
      "2019-01-29T02:26:08.440604: step 1344, loss 0.134956, acc 0.984375, learning_rate 0.000236205\n",
      "2019-01-29T02:26:08.568237: step 1345, loss 0.142404, acc 0.984375, learning_rate 0.000235843\n",
      "2019-01-29T02:26:08.694676: step 1346, loss 0.0862801, acc 1, learning_rate 0.000235481\n",
      "2019-01-29T02:26:08.824569: step 1347, loss 0.10349, acc 1, learning_rate 0.00023512\n",
      "2019-01-29T02:26:08.949484: step 1348, loss 0.0963592, acc 1, learning_rate 0.00023476\n",
      "2019-01-29T02:26:09.036055: step 1349, loss 0.103126, acc 1, learning_rate 0.000234401\n",
      "2019-01-29T02:26:09.162747: step 1350, loss 0.0904834, acc 1, learning_rate 0.000234043\n",
      "2019-01-29T02:26:09.288487: step 1351, loss 0.124556, acc 1, learning_rate 0.000233685\n",
      "2019-01-29T02:26:09.414827: step 1352, loss 0.116715, acc 0.984375, learning_rate 0.000233329\n",
      "2019-01-29T02:26:09.540981: step 1353, loss 0.109087, acc 0.984375, learning_rate 0.000232974\n",
      "2019-01-29T02:26:09.666692: step 1354, loss 0.0789232, acc 1, learning_rate 0.00023262\n",
      "2019-01-29T02:26:09.795312: step 1355, loss 0.0812813, acc 1, learning_rate 0.000232266\n",
      "2019-01-29T02:26:09.926166: step 1356, loss 0.105403, acc 0.984375, learning_rate 0.000231914\n",
      "2019-01-29T02:26:10.051967: step 1357, loss 0.121744, acc 1, learning_rate 0.000231563\n",
      "2019-01-29T02:26:10.179680: step 1358, loss 0.0783751, acc 1, learning_rate 0.000231212\n",
      "2019-01-29T02:26:10.267712: step 1359, loss 0.0764736, acc 1, learning_rate 0.000230863\n",
      "2019-01-29T02:26:10.397884: step 1360, loss 0.0646933, acc 1, learning_rate 0.000230514\n",
      "2019-01-29T02:26:10.486913: step 1361, loss 0.10107, acc 1, learning_rate 0.000230166\n",
      "2019-01-29T02:26:10.612191: step 1362, loss 0.116643, acc 1, learning_rate 0.000229819\n",
      "2019-01-29T02:26:10.709387: step 1363, loss 0.0993754, acc 1, learning_rate 0.000229474\n",
      "2019-01-29T02:26:10.835123: step 1364, loss 0.131366, acc 0.984375, learning_rate 0.000229129\n",
      "2019-01-29T02:26:10.958625: step 1365, loss 0.11377, acc 0.984375, learning_rate 0.000228785\n",
      "2019-01-29T02:26:11.077879: step 1366, loss 0.0981861, acc 1, learning_rate 0.000228441\n",
      "2019-01-29T02:26:11.206600: step 1367, loss 0.119707, acc 0.984375, learning_rate 0.000228099\n",
      "2019-01-29T02:26:11.294723: step 1368, loss 0.0960345, acc 0.984375, learning_rate 0.000227758\n",
      "2019-01-29T02:26:11.405286: step 1369, loss 0.120323, acc 0.984375, learning_rate 0.000227418\n",
      "2019-01-29T02:26:11.524505: step 1370, loss 0.0797655, acc 1, learning_rate 0.000227078\n",
      "2019-01-29T02:26:11.611861: step 1371, loss 0.0745765, acc 1, learning_rate 0.00022674\n",
      "2019-01-29T02:26:11.709420: step 1372, loss 0.103844, acc 1, learning_rate 0.000226402\n",
      "2019-01-29T02:26:11.832921: step 1373, loss 0.0929076, acc 1, learning_rate 0.000226065\n",
      "2019-01-29T02:26:11.926910: step 1374, loss 0.0780801, acc 1, learning_rate 0.000225729\n",
      "2019-01-29T02:26:12.016275: step 1375, loss 0.0817297, acc 1, learning_rate 0.000225394\n",
      "2019-01-29T02:26:12.149013: step 1376, loss 0.103367, acc 1, learning_rate 0.00022506\n",
      "2019-01-29T02:26:12.275659: step 1377, loss 0.0789974, acc 1, learning_rate 0.000224727\n",
      "2019-01-29T02:26:12.399558: step 1378, loss 0.0944022, acc 1, learning_rate 0.000224395\n",
      "2019-01-29T02:26:12.530035: step 1379, loss 0.12089, acc 0.984375, learning_rate 0.000224063\n",
      "2019-01-29T02:26:12.649620: step 1380, loss 0.0926623, acc 1, learning_rate 0.000223733\n",
      "2019-01-29T02:26:12.736627: step 1381, loss 0.0923587, acc 1, learning_rate 0.000223403\n",
      "2019-01-29T02:26:12.861589: step 1382, loss 0.115168, acc 1, learning_rate 0.000223074\n",
      "2019-01-29T02:26:12.990458: step 1383, loss 0.0972622, acc 1, learning_rate 0.000222746\n",
      "2019-01-29T02:26:13.106158: step 1384, loss 0.134928, acc 0.96875, learning_rate 0.000222419\n",
      "2019-01-29T02:26:13.240448: step 1385, loss 0.11314, acc 0.984375, learning_rate 0.000222093\n",
      "2019-01-29T02:26:13.372734: step 1386, loss 0.0835877, acc 1, learning_rate 0.000221768\n",
      "2019-01-29T02:26:13.499758: step 1387, loss 0.0968723, acc 1, learning_rate 0.000221444\n",
      "2019-01-29T02:26:13.621546: step 1388, loss 0.0878893, acc 1, learning_rate 0.00022112\n",
      "2019-01-29T02:26:13.714672: step 1389, loss 0.0840266, acc 1, learning_rate 0.000220797\n",
      "2019-01-29T02:26:13.846069: step 1390, loss 0.0989158, acc 0.984375, learning_rate 0.000220476\n",
      "2019-01-29T02:26:13.973595: step 1391, loss 0.117158, acc 1, learning_rate 0.000220155\n",
      "2019-01-29T02:26:14.096441: step 1392, loss 0.105965, acc 1, learning_rate 0.000219834\n",
      "2019-01-29T02:26:14.223797: step 1393, loss 0.0720661, acc 1, learning_rate 0.000219515\n",
      "2019-01-29T02:26:14.351635: step 1394, loss 0.0899963, acc 1, learning_rate 0.000219197\n",
      "2019-01-29T02:26:14.481318: step 1395, loss 0.126742, acc 0.984375, learning_rate 0.000218879\n",
      "2019-01-29T02:26:14.568112: step 1396, loss 0.107257, acc 1, learning_rate 0.000218562\n",
      "2019-01-29T02:26:14.668080: step 1397, loss 0.10571, acc 1, learning_rate 0.000218247\n",
      "2019-01-29T02:26:14.752434: step 1398, loss 0.0735059, acc 1, learning_rate 0.000217932\n",
      "2019-01-29T02:26:14.876683: step 1399, loss 0.0888965, acc 0.984375, learning_rate 0.000217617\n",
      "2019-01-29T02:26:15.002668: step 1400, loss 0.0798697, acc 1, learning_rate 0.000217304\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:26:15.030811: step 1400, loss 0.611268, acc 0.761726\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1400\n",
      "\n",
      "2019-01-29T02:26:15.364316: step 1401, loss 0.107435, acc 0.984375, learning_rate 0.000216991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:26:15.486060: step 1402, loss 0.107193, acc 0.984375, learning_rate 0.00021668\n",
      "2019-01-29T02:26:15.608433: step 1403, loss 0.0865449, acc 1, learning_rate 0.000216369\n",
      "2019-01-29T02:26:15.724751: step 1404, loss 0.0987163, acc 1, learning_rate 0.000216059\n",
      "2019-01-29T02:26:15.851682: step 1405, loss 0.16977, acc 0.984375, learning_rate 0.00021575\n",
      "2019-01-29T02:26:15.957411: step 1406, loss 0.0923863, acc 1, learning_rate 0.000215441\n",
      "2019-01-29T02:26:16.078983: step 1407, loss 0.0969282, acc 1, learning_rate 0.000215134\n",
      "2019-01-29T02:26:16.210139: step 1408, loss 0.0933746, acc 0.984375, learning_rate 0.000214827\n",
      "2019-01-29T02:26:16.333749: step 1409, loss 0.121432, acc 0.953125, learning_rate 0.000214521\n",
      "2019-01-29T02:26:16.430804: step 1410, loss 0.0962746, acc 0.984375, learning_rate 0.000214216\n",
      "2019-01-29T02:26:16.559969: step 1411, loss 0.137691, acc 0.984375, learning_rate 0.000213912\n",
      "2019-01-29T02:26:16.691211: step 1412, loss 0.14477, acc 0.953125, learning_rate 0.000213608\n",
      "2019-01-29T02:26:16.775559: step 1413, loss 0.0791212, acc 1, learning_rate 0.000213305\n",
      "2019-01-29T02:26:16.901728: step 1414, loss 0.103701, acc 1, learning_rate 0.000213004\n",
      "2019-01-29T02:26:16.989438: step 1415, loss 0.118797, acc 1, learning_rate 0.000212703\n",
      "2019-01-29T02:26:17.121668: step 1416, loss 0.0933158, acc 1, learning_rate 0.000212402\n",
      "2019-01-29T02:26:17.246465: step 1417, loss 0.0929487, acc 1, learning_rate 0.000212103\n",
      "2019-01-29T02:26:17.375666: step 1418, loss 0.104476, acc 1, learning_rate 0.000211804\n",
      "2019-01-29T02:26:17.465485: step 1419, loss 0.095697, acc 0.984375, learning_rate 0.000211506\n",
      "2019-01-29T02:26:17.589156: step 1420, loss 0.125609, acc 0.984375, learning_rate 0.000211209\n",
      "2019-01-29T02:26:17.715806: step 1421, loss 0.0917488, acc 1, learning_rate 0.000210913\n",
      "2019-01-29T02:26:17.841728: step 1422, loss 0.0857894, acc 1, learning_rate 0.000210617\n",
      "2019-01-29T02:26:17.968495: step 1423, loss 0.0958656, acc 1, learning_rate 0.000210323\n",
      "2019-01-29T02:26:18.056955: step 1424, loss 0.107743, acc 0.984375, learning_rate 0.000210029\n",
      "2019-01-29T02:26:18.177573: step 1425, loss 0.0845621, acc 0.984375, learning_rate 0.000209736\n",
      "2019-01-29T02:26:18.300514: step 1426, loss 0.101835, acc 1, learning_rate 0.000209443\n",
      "2019-01-29T02:26:18.429837: step 1427, loss 0.0895295, acc 1, learning_rate 0.000209152\n",
      "2019-01-29T02:26:18.552597: step 1428, loss 0.0772947, acc 1, learning_rate 0.000208861\n",
      "2019-01-29T02:26:18.685998: step 1429, loss 0.145264, acc 0.96875, learning_rate 0.000208571\n",
      "2019-01-29T02:26:18.813122: step 1430, loss 0.0975207, acc 1, learning_rate 0.000208282\n",
      "2019-01-29T02:26:18.937307: step 1431, loss 0.0974954, acc 1, learning_rate 0.000207993\n",
      "2019-01-29T02:26:19.056977: step 1432, loss 0.131824, acc 0.96875, learning_rate 0.000207705\n",
      "2019-01-29T02:26:19.178702: step 1433, loss 0.0983896, acc 1, learning_rate 0.000207418\n",
      "2019-01-29T02:26:19.301708: step 1434, loss 0.0965845, acc 0.984375, learning_rate 0.000207132\n",
      "2019-01-29T02:26:19.426809: step 1435, loss 0.115719, acc 0.984375, learning_rate 0.000206847\n",
      "2019-01-29T02:26:19.554033: step 1436, loss 0.11439, acc 1, learning_rate 0.000206562\n",
      "2019-01-29T02:26:19.685650: step 1437, loss 0.0788944, acc 1, learning_rate 0.000206278\n",
      "2019-01-29T02:26:19.776626: step 1438, loss 0.0788335, acc 1, learning_rate 0.000205995\n",
      "2019-01-29T02:26:19.869539: step 1439, loss 0.0784085, acc 1, learning_rate 0.000205713\n",
      "2019-01-29T02:26:19.993740: step 1440, loss 0.0887177, acc 1, learning_rate 0.000205431\n",
      "2019-01-29T02:26:20.122644: step 1441, loss 0.12071, acc 0.984375, learning_rate 0.00020515\n",
      "2019-01-29T02:26:20.206897: step 1442, loss 0.0806773, acc 1, learning_rate 0.00020487\n",
      "2019-01-29T02:26:20.334726: step 1443, loss 0.0839328, acc 1, learning_rate 0.000204591\n",
      "2019-01-29T02:26:20.458006: step 1444, loss 0.129551, acc 0.984375, learning_rate 0.000204312\n",
      "2019-01-29T02:26:20.583272: step 1445, loss 0.109118, acc 1, learning_rate 0.000204034\n",
      "2019-01-29T02:26:20.711234: step 1446, loss 0.0932718, acc 1, learning_rate 0.000203757\n",
      "2019-01-29T02:26:20.838967: step 1447, loss 0.123037, acc 0.984375, learning_rate 0.00020348\n",
      "2019-01-29T02:26:20.926513: step 1448, loss 0.0954018, acc 1, learning_rate 0.000203205\n",
      "2019-01-29T02:26:21.056521: step 1449, loss 0.105279, acc 0.984375, learning_rate 0.00020293\n",
      "2019-01-29T02:26:21.144364: step 1450, loss 0.106285, acc 1, learning_rate 0.000202656\n",
      "2019-01-29T02:26:21.270791: step 1451, loss 0.116187, acc 1, learning_rate 0.000202382\n",
      "2019-01-29T02:26:21.356930: step 1452, loss 0.109195, acc 1, learning_rate 0.000202109\n",
      "2019-01-29T02:26:21.480988: step 1453, loss 0.115069, acc 0.984375, learning_rate 0.000201837\n",
      "2019-01-29T02:26:21.602982: step 1454, loss 0.113544, acc 0.984375, learning_rate 0.000201566\n",
      "2019-01-29T02:26:21.726304: step 1455, loss 0.12126, acc 0.96875, learning_rate 0.000201295\n",
      "2019-01-29T02:26:21.851358: step 1456, loss 0.0916367, acc 0.984375, learning_rate 0.000201026\n",
      "2019-01-29T02:26:21.974109: step 1457, loss 0.0864581, acc 1, learning_rate 0.000200756\n",
      "2019-01-29T02:26:22.096478: step 1458, loss 0.0903103, acc 1, learning_rate 0.000200488\n",
      "2019-01-29T02:26:22.229174: step 1459, loss 0.0952944, acc 0.984375, learning_rate 0.00020022\n",
      "2019-01-29T02:26:22.354219: step 1460, loss 0.072039, acc 1, learning_rate 0.000199953\n",
      "2019-01-29T02:26:22.479794: step 1461, loss 0.0850154, acc 1, learning_rate 0.000199687\n",
      "2019-01-29T02:26:22.612788: step 1462, loss 0.0977405, acc 1, learning_rate 0.000199421\n",
      "2019-01-29T02:26:22.742032: step 1463, loss 0.08216, acc 1, learning_rate 0.000199156\n",
      "2019-01-29T02:26:22.869119: step 1464, loss 0.0977229, acc 0.984375, learning_rate 0.000198892\n",
      "2019-01-29T02:26:22.993813: step 1465, loss 0.0830639, acc 1, learning_rate 0.000198629\n",
      "2019-01-29T02:26:23.115132: step 1466, loss 0.12207, acc 0.984375, learning_rate 0.000198366\n",
      "2019-01-29T02:26:23.233371: step 1467, loss 0.076211, acc 1, learning_rate 0.000198104\n",
      "2019-01-29T02:26:23.354904: step 1468, loss 0.102675, acc 1, learning_rate 0.000197843\n",
      "2019-01-29T02:26:23.479689: step 1469, loss 0.0742713, acc 1, learning_rate 0.000197582\n",
      "2019-01-29T02:26:23.612633: step 1470, loss 0.12282, acc 0.96875, learning_rate 0.000197322\n",
      "2019-01-29T02:26:23.701546: step 1471, loss 0.0879334, acc 1, learning_rate 0.000197063\n",
      "2019-01-29T02:26:23.828540: step 1472, loss 0.0884591, acc 1, learning_rate 0.000196804\n",
      "2019-01-29T02:26:23.957176: step 1473, loss 0.116207, acc 0.96875, learning_rate 0.000196546\n",
      "2019-01-29T02:26:24.088623: step 1474, loss 0.104286, acc 1, learning_rate 0.000196289\n",
      "2019-01-29T02:26:24.178739: step 1475, loss 0.0794569, acc 1, learning_rate 0.000196032\n",
      "2019-01-29T02:26:24.278605: step 1476, loss 0.08033, acc 0.984375, learning_rate 0.000195777\n",
      "2019-01-29T02:26:24.400115: step 1477, loss 0.0823337, acc 1, learning_rate 0.000195521\n",
      "2019-01-29T02:26:24.523213: step 1478, loss 0.124845, acc 0.96875, learning_rate 0.000195267\n",
      "2019-01-29T02:26:24.647629: step 1479, loss 0.102316, acc 0.984375, learning_rate 0.000195013\n",
      "2019-01-29T02:26:24.777922: step 1480, loss 0.112145, acc 0.984375, learning_rate 0.00019476\n",
      "2019-01-29T02:26:24.906491: step 1481, loss 0.146208, acc 0.96875, learning_rate 0.000194507\n",
      "2019-01-29T02:26:25.035971: step 1482, loss 0.0775706, acc 1, learning_rate 0.000194256\n",
      "2019-01-29T02:26:25.164166: step 1483, loss 0.128647, acc 0.984375, learning_rate 0.000194005\n",
      "2019-01-29T02:26:25.254740: step 1484, loss 0.0974269, acc 1, learning_rate 0.000193754\n",
      "2019-01-29T02:26:25.379719: step 1485, loss 0.0984859, acc 1, learning_rate 0.000193504\n",
      "2019-01-29T02:26:25.505823: step 1486, loss 0.0821808, acc 1, learning_rate 0.000193255\n",
      "2019-01-29T02:26:25.633869: step 1487, loss 0.0879737, acc 1, learning_rate 0.000193007\n",
      "2019-01-29T02:26:25.757406: step 1488, loss 0.0804771, acc 1, learning_rate 0.000192759\n",
      "2019-01-29T02:26:25.843443: step 1489, loss 0.115081, acc 0.96875, learning_rate 0.000192512\n",
      "2019-01-29T02:26:25.968179: step 1490, loss 0.0943733, acc 0.984375, learning_rate 0.000192265\n",
      "2019-01-29T02:26:26.097518: step 1491, loss 0.129046, acc 0.984375, learning_rate 0.00019202\n",
      "2019-01-29T02:26:26.213145: step 1492, loss 0.121958, acc 0.96875, learning_rate 0.000191774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:26:26.342048: step 1493, loss 0.0906154, acc 1, learning_rate 0.00019153\n",
      "2019-01-29T02:26:26.467853: step 1494, loss 0.0882611, acc 1, learning_rate 0.000191286\n",
      "2019-01-29T02:26:26.592858: step 1495, loss 0.0873735, acc 1, learning_rate 0.000191043\n",
      "2019-01-29T02:26:26.719870: step 1496, loss 0.122703, acc 0.984375, learning_rate 0.0001908\n",
      "2019-01-29T02:26:26.804041: step 1497, loss 0.0926667, acc 1, learning_rate 0.000190558\n",
      "2019-01-29T02:26:26.925747: step 1498, loss 0.115253, acc 1, learning_rate 0.000190317\n",
      "2019-01-29T02:26:27.053762: step 1499, loss 0.117625, acc 0.96875, learning_rate 0.000190076\n",
      "2019-01-29T02:26:27.183275: step 1500, loss 0.0848755, acc 1, learning_rate 0.000189836\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:26:27.210394: step 1500, loss 0.614801, acc 0.761726\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1500\n",
      "\n",
      "2019-01-29T02:26:27.507295: step 1501, loss 0.101095, acc 1, learning_rate 0.000189597\n",
      "2019-01-29T02:26:27.632918: step 1502, loss 0.0827941, acc 1, learning_rate 0.000189358\n",
      "2019-01-29T02:26:27.721057: step 1503, loss 0.0659811, acc 1, learning_rate 0.00018912\n",
      "2019-01-29T02:26:27.809025: step 1504, loss 0.125597, acc 0.96875, learning_rate 0.000188883\n",
      "2019-01-29T02:26:27.900645: step 1505, loss 0.0769104, acc 1, learning_rate 0.000188646\n",
      "2019-01-29T02:26:28.031674: step 1506, loss 0.10291, acc 1, learning_rate 0.00018841\n",
      "2019-01-29T02:26:28.161939: step 1507, loss 0.0873883, acc 1, learning_rate 0.000188174\n",
      "2019-01-29T02:26:28.290248: step 1508, loss 0.0962505, acc 0.984375, learning_rate 0.000187939\n",
      "2019-01-29T02:26:28.377191: step 1509, loss 0.109065, acc 1, learning_rate 0.000187705\n",
      "2019-01-29T02:26:28.503553: step 1510, loss 0.0792115, acc 1, learning_rate 0.000187472\n",
      "2019-01-29T02:26:28.627569: step 1511, loss 0.103637, acc 0.984375, learning_rate 0.000187238\n",
      "2019-01-29T02:26:28.718201: step 1512, loss 0.100569, acc 0.984375, learning_rate 0.000187006\n",
      "2019-01-29T02:26:28.848825: step 1513, loss 0.102021, acc 0.984375, learning_rate 0.000186774\n",
      "2019-01-29T02:26:28.980256: step 1514, loss 0.0864735, acc 1, learning_rate 0.000186543\n",
      "2019-01-29T02:26:29.104098: step 1515, loss 0.0973918, acc 1, learning_rate 0.000186312\n",
      "2019-01-29T02:26:29.230612: step 1516, loss 0.0896319, acc 1, learning_rate 0.000186083\n",
      "2019-01-29T02:26:29.355032: step 1517, loss 0.0965709, acc 0.984375, learning_rate 0.000185853\n",
      "2019-01-29T02:26:29.482395: step 1518, loss 0.130953, acc 0.984375, learning_rate 0.000185624\n",
      "2019-01-29T02:26:29.609162: step 1519, loss 0.0892789, acc 1, learning_rate 0.000185396\n",
      "2019-01-29T02:26:29.697970: step 1520, loss 0.108956, acc 0.984375, learning_rate 0.000185169\n",
      "2019-01-29T02:26:29.822162: step 1521, loss 0.102539, acc 0.984375, learning_rate 0.000184942\n",
      "2019-01-29T02:26:29.943659: step 1522, loss 0.0850153, acc 1, learning_rate 0.000184716\n",
      "2019-01-29T02:26:30.023475: step 1523, loss 0.122474, acc 0.96875, learning_rate 0.00018449\n",
      "2019-01-29T02:26:30.144243: step 1524, loss 0.10177, acc 0.984375, learning_rate 0.000184265\n",
      "2019-01-29T02:26:30.272314: step 1525, loss 0.134727, acc 0.96875, learning_rate 0.00018404\n",
      "2019-01-29T02:26:30.396925: step 1526, loss 0.0962488, acc 1, learning_rate 0.000183816\n",
      "2019-01-29T02:26:30.523585: step 1527, loss 0.0888471, acc 0.984375, learning_rate 0.000183593\n",
      "2019-01-29T02:26:30.648514: step 1528, loss 0.0956403, acc 1, learning_rate 0.00018337\n",
      "2019-01-29T02:26:30.779698: step 1529, loss 0.138578, acc 0.96875, learning_rate 0.000183148\n",
      "2019-01-29T02:26:30.906508: step 1530, loss 0.0833499, acc 1, learning_rate 0.000182927\n",
      "2019-01-29T02:26:31.033224: step 1531, loss 0.128189, acc 0.984375, learning_rate 0.000182706\n",
      "2019-01-29T02:26:31.159546: step 1532, loss 0.108073, acc 0.984375, learning_rate 0.000182485\n",
      "2019-01-29T02:26:31.284765: step 1533, loss 0.0950771, acc 1, learning_rate 0.000182266\n",
      "2019-01-29T02:26:31.407860: step 1534, loss 0.145457, acc 0.984375, learning_rate 0.000182047\n",
      "2019-01-29T02:26:31.522100: step 1535, loss 0.108452, acc 0.96875, learning_rate 0.000181828\n",
      "2019-01-29T02:26:31.646844: step 1536, loss 0.0780734, acc 1, learning_rate 0.00018161\n",
      "2019-01-29T02:26:31.770053: step 1537, loss 0.0729587, acc 1, learning_rate 0.000181392\n",
      "2019-01-29T02:26:31.902735: step 1538, loss 0.115098, acc 1, learning_rate 0.000181176\n",
      "2019-01-29T02:26:32.028569: step 1539, loss 0.0942965, acc 1, learning_rate 0.000180959\n",
      "2019-01-29T02:26:32.151467: step 1540, loss 0.113909, acc 1, learning_rate 0.000180744\n",
      "2019-01-29T02:26:32.279829: step 1541, loss 0.0807095, acc 1, learning_rate 0.000180529\n",
      "2019-01-29T02:26:32.407029: step 1542, loss 0.0971148, acc 0.984375, learning_rate 0.000180314\n",
      "2019-01-29T02:26:32.572191: step 1543, loss 0.0891282, acc 0.984375, learning_rate 0.0001801\n",
      "2019-01-29T02:26:32.696624: step 1544, loss 0.0903517, acc 0.984375, learning_rate 0.000179887\n",
      "2019-01-29T02:26:32.784250: step 1545, loss 0.0814612, acc 1, learning_rate 0.000179674\n",
      "2019-01-29T02:26:32.882667: step 1546, loss 0.118316, acc 0.984375, learning_rate 0.000179462\n",
      "2019-01-29T02:26:32.972021: step 1547, loss 0.0953265, acc 0.984375, learning_rate 0.00017925\n",
      "2019-01-29T02:26:33.062243: step 1548, loss 0.0778434, acc 1, learning_rate 0.000179039\n",
      "2019-01-29T02:26:33.153222: step 1549, loss 0.0720081, acc 1, learning_rate 0.000178828\n",
      "2019-01-29T02:26:33.273957: step 1550, loss 0.0874303, acc 1, learning_rate 0.000178618\n",
      "2019-01-29T02:26:33.395836: step 1551, loss 0.0978031, acc 1, learning_rate 0.000178409\n",
      "2019-01-29T02:26:33.480050: step 1552, loss 0.0765299, acc 1, learning_rate 0.0001782\n",
      "2019-01-29T02:26:33.604798: step 1553, loss 0.157376, acc 0.953125, learning_rate 0.000177991\n",
      "2019-01-29T02:26:33.729152: step 1554, loss 0.0918253, acc 1, learning_rate 0.000177784\n",
      "2019-01-29T02:26:34.033919: step 1555, loss 0.0883845, acc 1, learning_rate 0.000177576\n",
      "2019-01-29T02:26:34.161826: step 1556, loss 0.0720306, acc 1, learning_rate 0.00017737\n",
      "2019-01-29T02:26:34.292376: step 1557, loss 0.0913242, acc 1, learning_rate 0.000177164\n",
      "2019-01-29T02:26:34.419399: step 1558, loss 0.100895, acc 1, learning_rate 0.000176958\n",
      "2019-01-29T02:26:34.544802: step 1559, loss 0.0767644, acc 1, learning_rate 0.000176753\n",
      "2019-01-29T02:26:34.676307: step 1560, loss 0.109551, acc 0.984375, learning_rate 0.000176548\n",
      "2019-01-29T02:26:34.803173: step 1561, loss 0.0954688, acc 1, learning_rate 0.000176345\n",
      "2019-01-29T02:26:34.922029: step 1562, loss 0.0944719, acc 1, learning_rate 0.000176141\n",
      "2019-01-29T02:26:35.044883: step 1563, loss 0.0941716, acc 1, learning_rate 0.000175938\n",
      "2019-01-29T02:26:35.172009: step 1564, loss 0.095173, acc 1, learning_rate 0.000175736\n",
      "2019-01-29T02:26:35.299242: step 1565, loss 0.0909233, acc 1, learning_rate 0.000175534\n",
      "2019-01-29T02:26:35.431498: step 1566, loss 0.108914, acc 0.984375, learning_rate 0.000175333\n",
      "2019-01-29T02:26:35.519836: step 1567, loss 0.0996623, acc 1, learning_rate 0.000175132\n",
      "2019-01-29T02:26:35.606951: step 1568, loss 0.0917145, acc 1, learning_rate 0.000174932\n",
      "2019-01-29T02:26:35.737960: step 1569, loss 0.0994462, acc 1, learning_rate 0.000174732\n",
      "2019-01-29T02:26:35.861119: step 1570, loss 0.0987381, acc 0.984375, learning_rate 0.000174533\n",
      "2019-01-29T02:26:35.948628: step 1571, loss 0.0716374, acc 1, learning_rate 0.000174335\n",
      "2019-01-29T02:26:36.068902: step 1572, loss 0.0903398, acc 0.984375, learning_rate 0.000174137\n",
      "2019-01-29T02:26:36.192048: step 1573, loss 0.0891679, acc 1, learning_rate 0.000173939\n",
      "2019-01-29T02:26:36.324247: step 1574, loss 0.0742526, acc 1, learning_rate 0.000173742\n",
      "2019-01-29T02:26:36.451093: step 1575, loss 0.0972494, acc 1, learning_rate 0.000173546\n",
      "2019-01-29T02:26:36.573614: step 1576, loss 0.113711, acc 0.984375, learning_rate 0.00017335\n",
      "2019-01-29T02:26:36.697341: step 1577, loss 0.154948, acc 0.96875, learning_rate 0.000173154\n",
      "2019-01-29T02:26:36.817195: step 1578, loss 0.0936411, acc 1, learning_rate 0.000172959\n",
      "2019-01-29T02:26:36.904172: step 1579, loss 0.0824676, acc 1, learning_rate 0.000172765\n",
      "2019-01-29T02:26:37.026152: step 1580, loss 0.0965385, acc 1, learning_rate 0.000172571\n",
      "2019-01-29T02:26:37.115377: step 1581, loss 0.0971244, acc 1, learning_rate 0.000172378\n",
      "2019-01-29T02:26:37.239614: step 1582, loss 0.0805982, acc 1, learning_rate 0.000172185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:26:37.363059: step 1583, loss 0.106386, acc 0.984375, learning_rate 0.000171993\n",
      "2019-01-29T02:26:37.484289: step 1584, loss 0.0846193, acc 1, learning_rate 0.000171801\n",
      "2019-01-29T02:26:37.570509: step 1585, loss 0.0926538, acc 1, learning_rate 0.00017161\n",
      "2019-01-29T02:26:37.697797: step 1586, loss 0.0708336, acc 1, learning_rate 0.000171419\n",
      "2019-01-29T02:26:37.783549: step 1587, loss 0.122059, acc 0.984375, learning_rate 0.000171229\n",
      "2019-01-29T02:26:37.911548: step 1588, loss 0.104512, acc 0.984375, learning_rate 0.000171039\n",
      "2019-01-29T02:26:38.038726: step 1589, loss 0.0890099, acc 1, learning_rate 0.00017085\n",
      "2019-01-29T02:26:38.129582: step 1590, loss 0.0687877, acc 1, learning_rate 0.000170661\n",
      "2019-01-29T02:26:38.216385: step 1591, loss 0.0959315, acc 1, learning_rate 0.000170473\n",
      "2019-01-29T02:26:38.306810: step 1592, loss 0.0769414, acc 1, learning_rate 0.000170285\n",
      "2019-01-29T02:26:38.428220: step 1593, loss 0.0760015, acc 1, learning_rate 0.000170098\n",
      "2019-01-29T02:26:38.546956: step 1594, loss 0.0941291, acc 1, learning_rate 0.000169911\n",
      "2019-01-29T02:26:38.673096: step 1595, loss 0.084353, acc 1, learning_rate 0.000169725\n",
      "2019-01-29T02:26:38.800550: step 1596, loss 0.0755499, acc 1, learning_rate 0.000169539\n",
      "2019-01-29T02:26:38.931880: step 1597, loss 0.116862, acc 0.984375, learning_rate 0.000169353\n",
      "2019-01-29T02:26:39.056500: step 1598, loss 0.0800004, acc 1, learning_rate 0.000169169\n",
      "2019-01-29T02:26:39.156864: step 1599, loss 0.0991305, acc 0.984375, learning_rate 0.000168984\n",
      "2019-01-29T02:26:39.283640: step 1600, loss 0.101767, acc 0.984375, learning_rate 0.000168801\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:26:39.310880: step 1600, loss 0.617914, acc 0.75985\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1600\n",
      "\n",
      "2019-01-29T02:26:39.656868: step 1601, loss 0.0875747, acc 1, learning_rate 0.000168617\n",
      "2019-01-29T02:26:39.752718: step 1602, loss 0.0872095, acc 1, learning_rate 0.000168435\n",
      "2019-01-29T02:26:39.871003: step 1603, loss 0.10851, acc 0.984375, learning_rate 0.000168252\n",
      "2019-01-29T02:26:39.999305: step 1604, loss 0.0965055, acc 1, learning_rate 0.00016807\n",
      "2019-01-29T02:26:40.123061: step 1605, loss 0.0959708, acc 0.984375, learning_rate 0.000167889\n",
      "2019-01-29T02:26:40.239218: step 1606, loss 0.119191, acc 1, learning_rate 0.000167708\n",
      "2019-01-29T02:26:40.362913: step 1607, loss 0.0765611, acc 1, learning_rate 0.000167528\n",
      "2019-01-29T02:26:40.491706: step 1608, loss 0.0857088, acc 1, learning_rate 0.000167348\n",
      "2019-01-29T02:26:40.616787: step 1609, loss 0.0806828, acc 1, learning_rate 0.000167168\n",
      "2019-01-29T02:26:40.708541: step 1610, loss 0.0883491, acc 1, learning_rate 0.000166989\n",
      "2019-01-29T02:26:40.800581: step 1611, loss 0.127888, acc 0.96875, learning_rate 0.000166811\n",
      "2019-01-29T02:26:40.923843: step 1612, loss 0.112329, acc 0.96875, learning_rate 0.000166633\n",
      "2019-01-29T02:26:41.049931: step 1613, loss 0.115447, acc 0.984375, learning_rate 0.000166455\n",
      "2019-01-29T02:26:41.175735: step 1614, loss 0.0822374, acc 1, learning_rate 0.000166278\n",
      "2019-01-29T02:26:41.307576: step 1615, loss 0.0800734, acc 1, learning_rate 0.000166102\n",
      "2019-01-29T02:26:41.437269: step 1616, loss 0.101665, acc 0.984375, learning_rate 0.000165926\n",
      "2019-01-29T02:26:41.561486: step 1617, loss 0.101446, acc 1, learning_rate 0.00016575\n",
      "2019-01-29T02:26:41.683222: step 1618, loss 0.135094, acc 0.96875, learning_rate 0.000165575\n",
      "2019-01-29T02:26:41.812238: step 1619, loss 0.0954423, acc 1, learning_rate 0.0001654\n",
      "2019-01-29T02:26:41.896703: step 1620, loss 0.104018, acc 0.984375, learning_rate 0.000165226\n",
      "2019-01-29T02:26:42.026071: step 1621, loss 0.0776799, acc 1, learning_rate 0.000165052\n",
      "2019-01-29T02:26:42.149216: step 1622, loss 0.0842888, acc 1, learning_rate 0.000164879\n",
      "2019-01-29T02:26:42.270608: step 1623, loss 0.104037, acc 1, learning_rate 0.000164706\n",
      "2019-01-29T02:26:42.398511: step 1624, loss 0.100556, acc 1, learning_rate 0.000164534\n",
      "2019-01-29T02:26:42.519492: step 1625, loss 0.0928676, acc 1, learning_rate 0.000164362\n",
      "2019-01-29T02:26:42.647961: step 1626, loss 0.0795278, acc 1, learning_rate 0.00016419\n",
      "2019-01-29T02:26:42.775322: step 1627, loss 0.0989422, acc 1, learning_rate 0.000164019\n",
      "2019-01-29T02:26:42.899266: step 1628, loss 0.0934817, acc 0.984375, learning_rate 0.000163849\n",
      "2019-01-29T02:26:43.022312: step 1629, loss 0.116239, acc 0.96875, learning_rate 0.000163679\n",
      "2019-01-29T02:26:43.147644: step 1630, loss 0.0831913, acc 1, learning_rate 0.000163509\n",
      "2019-01-29T02:26:43.279467: step 1631, loss 0.0795675, acc 1, learning_rate 0.00016334\n",
      "2019-01-29T02:26:43.367064: step 1632, loss 0.110465, acc 0.96875, learning_rate 0.000163171\n",
      "2019-01-29T02:26:43.487944: step 1633, loss 0.0909063, acc 1, learning_rate 0.000163003\n",
      "2019-01-29T02:26:43.612077: step 1634, loss 0.11435, acc 0.984375, learning_rate 0.000162835\n",
      "2019-01-29T02:26:43.741682: step 1635, loss 0.0914335, acc 1, learning_rate 0.000162667\n",
      "2019-01-29T02:26:43.863298: step 1636, loss 0.0970786, acc 1, learning_rate 0.0001625\n",
      "2019-01-29T02:26:43.996197: step 1637, loss 0.0805705, acc 1, learning_rate 0.000162334\n",
      "2019-01-29T02:26:44.088837: step 1638, loss 0.0827052, acc 0.984375, learning_rate 0.000162168\n",
      "2019-01-29T02:26:44.217816: step 1639, loss 0.098768, acc 0.984375, learning_rate 0.000162002\n",
      "2019-01-29T02:26:44.341049: step 1640, loss 0.0872129, acc 1, learning_rate 0.000161837\n",
      "2019-01-29T02:26:44.463406: step 1641, loss 0.0961749, acc 1, learning_rate 0.000161672\n",
      "2019-01-29T02:26:44.588863: step 1642, loss 0.0861433, acc 1, learning_rate 0.000161508\n",
      "2019-01-29T02:26:44.715474: step 1643, loss 0.0803124, acc 1, learning_rate 0.000161344\n",
      "2019-01-29T02:26:44.845655: step 1644, loss 0.0739284, acc 1, learning_rate 0.000161181\n",
      "2019-01-29T02:26:44.970020: step 1645, loss 0.0825586, acc 1, learning_rate 0.000161018\n",
      "2019-01-29T02:26:45.098755: step 1646, loss 0.13686, acc 0.96875, learning_rate 0.000160855\n",
      "2019-01-29T02:26:45.221910: step 1647, loss 0.0948244, acc 0.984375, learning_rate 0.000160693\n",
      "2019-01-29T02:26:45.347277: step 1648, loss 0.11376, acc 0.984375, learning_rate 0.000160531\n",
      "2019-01-29T02:26:45.475644: step 1649, loss 0.0758282, acc 1, learning_rate 0.00016037\n",
      "2019-01-29T02:26:45.599044: step 1650, loss 0.077968, acc 1, learning_rate 0.000160209\n",
      "2019-01-29T02:26:45.725432: step 1651, loss 0.0945202, acc 1, learning_rate 0.000160049\n",
      "2019-01-29T02:26:45.851391: step 1652, loss 0.0816129, acc 1, learning_rate 0.000159889\n",
      "2019-01-29T02:26:45.980108: step 1653, loss 0.108227, acc 1, learning_rate 0.000159729\n",
      "2019-01-29T02:26:46.098870: step 1654, loss 0.118439, acc 0.984375, learning_rate 0.00015957\n",
      "2019-01-29T02:26:46.229829: step 1655, loss 0.110702, acc 0.984375, learning_rate 0.000159411\n",
      "2019-01-29T02:26:46.355845: step 1656, loss 0.0878181, acc 1, learning_rate 0.000159253\n",
      "2019-01-29T02:26:46.485704: step 1657, loss 0.0825701, acc 1, learning_rate 0.000159095\n",
      "2019-01-29T02:26:46.606144: step 1658, loss 0.0901522, acc 0.984375, learning_rate 0.000158938\n",
      "2019-01-29T02:26:46.700065: step 1659, loss 0.0850014, acc 1, learning_rate 0.000158781\n",
      "2019-01-29T02:26:46.824630: step 1660, loss 0.0984209, acc 0.984375, learning_rate 0.000158624\n",
      "2019-01-29T02:26:46.943485: step 1661, loss 0.11478, acc 0.984375, learning_rate 0.000158468\n",
      "2019-01-29T02:26:47.032504: step 1662, loss 0.0965862, acc 1, learning_rate 0.000158312\n",
      "2019-01-29T02:26:47.159744: step 1663, loss 0.0792573, acc 1, learning_rate 0.000158157\n",
      "2019-01-29T02:26:47.278933: step 1664, loss 0.125436, acc 0.96875, learning_rate 0.000158002\n",
      "2019-01-29T02:26:47.405981: step 1665, loss 0.0903873, acc 1, learning_rate 0.000157847\n",
      "2019-01-29T02:26:47.529878: step 1666, loss 0.120717, acc 0.96875, learning_rate 0.000157693\n",
      "2019-01-29T02:26:47.653566: step 1667, loss 0.083096, acc 0.984375, learning_rate 0.00015754\n",
      "2019-01-29T02:26:47.785662: step 1668, loss 0.122954, acc 0.984375, learning_rate 0.000157386\n",
      "2019-01-29T02:26:47.911732: step 1669, loss 0.0862691, acc 1, learning_rate 0.000157233\n",
      "2019-01-29T02:26:48.035171: step 1670, loss 0.0899791, acc 0.984375, learning_rate 0.000157081\n",
      "2019-01-29T02:26:48.125544: step 1671, loss 0.0788845, acc 1, learning_rate 0.000156929\n",
      "2019-01-29T02:26:48.261307: step 1672, loss 0.0877868, acc 1, learning_rate 0.000156777\n",
      "2019-01-29T02:26:48.350038: step 1673, loss 0.0765223, acc 1, learning_rate 0.000156626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:26:48.474934: step 1674, loss 0.0856302, acc 0.984375, learning_rate 0.000156475\n",
      "2019-01-29T02:26:48.604428: step 1675, loss 0.0861132, acc 1, learning_rate 0.000156325\n",
      "2019-01-29T02:26:48.733623: step 1676, loss 0.0947758, acc 1, learning_rate 0.000156174\n",
      "2019-01-29T02:26:48.853806: step 1677, loss 0.078215, acc 1, learning_rate 0.000156025\n",
      "2019-01-29T02:26:48.950655: step 1678, loss 0.109425, acc 0.984375, learning_rate 0.000155876\n",
      "2019-01-29T02:26:49.075074: step 1679, loss 0.0795548, acc 1, learning_rate 0.000155727\n",
      "2019-01-29T02:26:49.172929: step 1680, loss 0.110026, acc 0.984375, learning_rate 0.000155578\n",
      "2019-01-29T02:26:49.300044: step 1681, loss 0.116607, acc 0.984375, learning_rate 0.00015543\n",
      "2019-01-29T02:26:49.425749: step 1682, loss 0.111054, acc 0.96875, learning_rate 0.000155282\n",
      "2019-01-29T02:26:49.552702: step 1683, loss 0.0749916, acc 1, learning_rate 0.000155135\n",
      "2019-01-29T02:26:49.680908: step 1684, loss 0.0955271, acc 1, learning_rate 0.000154988\n",
      "2019-01-29T02:26:49.806998: step 1685, loss 0.0872815, acc 1, learning_rate 0.000154842\n",
      "2019-01-29T02:26:49.933085: step 1686, loss 0.0759406, acc 1, learning_rate 0.000154696\n",
      "2019-01-29T02:26:50.061072: step 1687, loss 0.122742, acc 0.984375, learning_rate 0.00015455\n",
      "2019-01-29T02:26:50.182783: step 1688, loss 0.0921493, acc 1, learning_rate 0.000154405\n",
      "2019-01-29T02:26:50.307765: step 1689, loss 0.0862351, acc 1, learning_rate 0.00015426\n",
      "2019-01-29T02:26:50.395695: step 1690, loss 0.103531, acc 0.984375, learning_rate 0.000154115\n",
      "2019-01-29T02:26:50.483225: step 1691, loss 0.101185, acc 0.984375, learning_rate 0.000153971\n",
      "2019-01-29T02:26:50.611752: step 1692, loss 0.0806935, acc 1, learning_rate 0.000153827\n",
      "2019-01-29T02:26:50.704604: step 1693, loss 0.0937516, acc 0.984375, learning_rate 0.000153684\n",
      "2019-01-29T02:26:50.827828: step 1694, loss 0.105665, acc 0.984375, learning_rate 0.000153541\n",
      "2019-01-29T02:26:50.918051: step 1695, loss 0.0865455, acc 1, learning_rate 0.000153398\n",
      "2019-01-29T02:26:51.051598: step 1696, loss 0.0896014, acc 1, learning_rate 0.000153256\n",
      "2019-01-29T02:26:51.177207: step 1697, loss 0.0836681, acc 1, learning_rate 0.000153114\n",
      "2019-01-29T02:26:51.301301: step 1698, loss 0.0996954, acc 0.984375, learning_rate 0.000152972\n",
      "2019-01-29T02:26:51.429246: step 1699, loss 0.0808357, acc 1, learning_rate 0.000152831\n",
      "2019-01-29T02:26:51.517305: step 1700, loss 0.0795625, acc 1, learning_rate 0.000152691\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:26:51.545561: step 1700, loss 0.622751, acc 0.760788\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1700\n",
      "\n",
      "2019-01-29T02:26:51.823415: step 1701, loss 0.116527, acc 0.984375, learning_rate 0.00015255\n",
      "2019-01-29T02:26:51.945930: step 1702, loss 0.104786, acc 1, learning_rate 0.00015241\n",
      "2019-01-29T02:26:52.037205: step 1703, loss 0.0847268, acc 1, learning_rate 0.00015227\n",
      "2019-01-29T02:26:52.162372: step 1704, loss 0.0710355, acc 1, learning_rate 0.000152131\n",
      "2019-01-29T02:26:52.288737: step 1705, loss 0.102614, acc 0.984375, learning_rate 0.000151992\n",
      "2019-01-29T02:26:52.409753: step 1706, loss 0.0891612, acc 1, learning_rate 0.000151854\n",
      "2019-01-29T02:26:52.498302: step 1707, loss 0.0735897, acc 1, learning_rate 0.000151716\n",
      "2019-01-29T02:26:52.619268: step 1708, loss 0.111583, acc 0.984375, learning_rate 0.000151578\n",
      "2019-01-29T02:26:52.749054: step 1709, loss 0.119801, acc 0.984375, learning_rate 0.00015144\n",
      "2019-01-29T02:26:52.838060: step 1710, loss 0.0784011, acc 1, learning_rate 0.000151303\n",
      "2019-01-29T02:26:52.965814: step 1711, loss 0.0649649, acc 1, learning_rate 0.000151167\n",
      "2019-01-29T02:26:53.088809: step 1712, loss 0.0737336, acc 1, learning_rate 0.00015103\n",
      "2019-01-29T02:26:53.191110: step 1713, loss 0.0996941, acc 1, learning_rate 0.000150894\n",
      "2019-01-29T02:26:53.312947: step 1714, loss 0.124332, acc 0.96875, learning_rate 0.000150759\n",
      "2019-01-29T02:26:53.430915: step 1715, loss 0.0938447, acc 1, learning_rate 0.000150624\n",
      "2019-01-29T02:26:53.557067: step 1716, loss 0.110279, acc 0.984375, learning_rate 0.000150489\n",
      "2019-01-29T02:26:53.683335: step 1717, loss 0.106598, acc 1, learning_rate 0.000150354\n",
      "2019-01-29T02:26:53.807964: step 1718, loss 0.0853909, acc 1, learning_rate 0.00015022\n",
      "2019-01-29T02:26:53.933930: step 1719, loss 0.103521, acc 0.984375, learning_rate 0.000150086\n",
      "2019-01-29T02:26:54.064803: step 1720, loss 0.0867357, acc 1, learning_rate 0.000149953\n",
      "2019-01-29T02:26:54.187374: step 1721, loss 0.0787069, acc 1, learning_rate 0.00014982\n",
      "2019-01-29T02:26:54.311162: step 1722, loss 0.0827658, acc 1, learning_rate 0.000149687\n",
      "2019-01-29T02:26:54.432434: step 1723, loss 0.084732, acc 1, learning_rate 0.000149555\n",
      "2019-01-29T02:26:54.546846: step 1724, loss 0.100318, acc 0.984375, learning_rate 0.000149423\n",
      "2019-01-29T02:26:54.669982: step 1725, loss 0.0990495, acc 1, learning_rate 0.000149291\n",
      "2019-01-29T02:26:54.794917: step 1726, loss 0.121822, acc 0.984375, learning_rate 0.00014916\n",
      "2019-01-29T02:26:54.921839: step 1727, loss 0.0906193, acc 1, learning_rate 0.000149029\n",
      "2019-01-29T02:26:55.046895: step 1728, loss 0.086054, acc 1, learning_rate 0.000148898\n",
      "2019-01-29T02:26:55.177908: step 1729, loss 0.111583, acc 0.96875, learning_rate 0.000148768\n",
      "2019-01-29T02:26:55.260360: step 1730, loss 0.0754459, acc 1, learning_rate 0.000148638\n",
      "2019-01-29T02:26:55.393209: step 1731, loss 0.139009, acc 0.96875, learning_rate 0.000148508\n",
      "2019-01-29T02:26:55.525053: step 1732, loss 0.0803136, acc 1, learning_rate 0.000148379\n",
      "2019-01-29T02:26:55.651575: step 1733, loss 0.0814828, acc 0.984375, learning_rate 0.00014825\n",
      "2019-01-29T02:26:55.776333: step 1734, loss 0.0958137, acc 1, learning_rate 0.000148122\n",
      "2019-01-29T02:26:55.902990: step 1735, loss 0.0845381, acc 1, learning_rate 0.000147993\n",
      "2019-01-29T02:26:56.024717: step 1736, loss 0.119779, acc 0.96875, learning_rate 0.000147866\n",
      "2019-01-29T02:26:56.108626: step 1737, loss 0.113142, acc 1, learning_rate 0.000147738\n",
      "2019-01-29T02:26:56.238001: step 1738, loss 0.0803575, acc 1, learning_rate 0.000147611\n",
      "2019-01-29T02:26:56.365436: step 1739, loss 0.108652, acc 0.984375, learning_rate 0.000147484\n",
      "2019-01-29T02:26:56.489990: step 1740, loss 0.0847544, acc 1, learning_rate 0.000147357\n",
      "2019-01-29T02:26:56.613772: step 1741, loss 0.0812818, acc 1, learning_rate 0.000147231\n",
      "2019-01-29T02:26:56.737836: step 1742, loss 0.108365, acc 1, learning_rate 0.000147105\n",
      "2019-01-29T02:26:56.862560: step 1743, loss 0.0728558, acc 1, learning_rate 0.00014698\n",
      "2019-01-29T02:26:56.983426: step 1744, loss 0.0862758, acc 0.984375, learning_rate 0.000146855\n",
      "2019-01-29T02:26:57.103823: step 1745, loss 0.0765158, acc 1, learning_rate 0.00014673\n",
      "2019-01-29T02:26:57.236900: step 1746, loss 0.0815221, acc 1, learning_rate 0.000146605\n",
      "2019-01-29T02:26:57.366185: step 1747, loss 0.102188, acc 0.984375, learning_rate 0.000146481\n",
      "2019-01-29T02:26:57.497462: step 1748, loss 0.0990545, acc 1, learning_rate 0.000146357\n",
      "2019-01-29T02:26:57.615986: step 1749, loss 0.106039, acc 0.984375, learning_rate 0.000146234\n",
      "2019-01-29T02:26:57.739029: step 1750, loss 0.0955709, acc 0.984375, learning_rate 0.000146111\n",
      "2019-01-29T02:26:57.828301: step 1751, loss 0.129907, acc 0.984375, learning_rate 0.000145988\n",
      "2019-01-29T02:26:57.958388: step 1752, loss 0.0778794, acc 1, learning_rate 0.000145865\n",
      "2019-01-29T02:26:58.081196: step 1753, loss 0.0906897, acc 0.984375, learning_rate 0.000145743\n",
      "2019-01-29T02:26:58.207022: step 1754, loss 0.102152, acc 0.984375, learning_rate 0.000145621\n",
      "2019-01-29T02:26:58.328910: step 1755, loss 0.079182, acc 1, learning_rate 0.0001455\n",
      "2019-01-29T02:26:58.418603: step 1756, loss 0.118834, acc 0.96875, learning_rate 0.000145379\n",
      "2019-01-29T02:26:58.542752: step 1757, loss 0.0898319, acc 1, learning_rate 0.000145258\n",
      "2019-01-29T02:26:58.668841: step 1758, loss 0.0921254, acc 0.984375, learning_rate 0.000145137\n",
      "2019-01-29T02:26:58.799260: step 1759, loss 0.101773, acc 1, learning_rate 0.000145017\n",
      "2019-01-29T02:26:58.917371: step 1760, loss 0.105711, acc 0.984375, learning_rate 0.000144897\n",
      "2019-01-29T02:26:59.034990: step 1761, loss 0.0846397, acc 0.984375, learning_rate 0.000144777\n",
      "2019-01-29T02:26:59.164301: step 1762, loss 0.0940741, acc 1, learning_rate 0.000144658\n",
      "2019-01-29T02:26:59.251305: step 1763, loss 0.108015, acc 1, learning_rate 0.000144539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:26:59.369079: step 1764, loss 0.11241, acc 1, learning_rate 0.00014442\n",
      "2019-01-29T02:26:59.499649: step 1765, loss 0.106003, acc 0.984375, learning_rate 0.000144302\n",
      "2019-01-29T02:26:59.630383: step 1766, loss 0.0827649, acc 1, learning_rate 0.000144184\n",
      "2019-01-29T02:26:59.723401: step 1767, loss 0.0873147, acc 1, learning_rate 0.000144066\n",
      "2019-01-29T02:26:59.850924: step 1768, loss 0.0711571, acc 1, learning_rate 0.000143949\n",
      "2019-01-29T02:26:59.974488: step 1769, loss 0.109128, acc 0.984375, learning_rate 0.000143832\n",
      "2019-01-29T02:27:00.098773: step 1770, loss 0.107517, acc 0.984375, learning_rate 0.000143715\n",
      "2019-01-29T02:27:00.221316: step 1771, loss 0.0720756, acc 1, learning_rate 0.000143599\n",
      "2019-01-29T02:27:00.309638: step 1772, loss 0.106837, acc 1, learning_rate 0.000143482\n",
      "2019-01-29T02:27:00.422961: step 1773, loss 0.09099, acc 1, learning_rate 0.000143367\n",
      "2019-01-29T02:27:00.544074: step 1774, loss 0.0900846, acc 0.984375, learning_rate 0.000143251\n",
      "2019-01-29T02:27:00.672354: step 1775, loss 0.110955, acc 0.984375, learning_rate 0.000143136\n",
      "2019-01-29T02:27:00.760213: step 1776, loss 0.113849, acc 0.984375, learning_rate 0.000143021\n",
      "2019-01-29T02:27:00.890490: step 1777, loss 0.113558, acc 1, learning_rate 0.000142906\n",
      "2019-01-29T02:27:01.021429: step 1778, loss 0.0676695, acc 1, learning_rate 0.000142792\n",
      "2019-01-29T02:27:01.146960: step 1779, loss 0.0978822, acc 1, learning_rate 0.000142678\n",
      "2019-01-29T02:27:01.269841: step 1780, loss 0.115811, acc 0.984375, learning_rate 0.000142564\n",
      "2019-01-29T02:27:01.393510: step 1781, loss 0.0749847, acc 1, learning_rate 0.000142451\n",
      "2019-01-29T02:27:01.520125: step 1782, loss 0.0856866, acc 1, learning_rate 0.000142338\n",
      "2019-01-29T02:27:01.647066: step 1783, loss 0.10092, acc 0.984375, learning_rate 0.000142225\n",
      "2019-01-29T02:27:01.778259: step 1784, loss 0.097561, acc 1, learning_rate 0.000142112\n",
      "2019-01-29T02:27:01.869234: step 1785, loss 0.120269, acc 0.96875, learning_rate 0.000142\n",
      "2019-01-29T02:27:01.956765: step 1786, loss 0.0928814, acc 1, learning_rate 0.000141888\n",
      "2019-01-29T02:27:02.081624: step 1787, loss 0.0987564, acc 1, learning_rate 0.000141777\n",
      "2019-01-29T02:27:02.168253: step 1788, loss 0.0808483, acc 1, learning_rate 0.000141665\n",
      "2019-01-29T02:27:02.290231: step 1789, loss 0.0874957, acc 1, learning_rate 0.000141554\n",
      "2019-01-29T02:27:02.420287: step 1790, loss 0.107716, acc 0.96875, learning_rate 0.000141444\n",
      "2019-01-29T02:27:02.541018: step 1791, loss 0.0742594, acc 1, learning_rate 0.000141333\n",
      "2019-01-29T02:27:02.663773: step 1792, loss 0.0980137, acc 0.984375, learning_rate 0.000141223\n",
      "2019-01-29T02:27:02.782575: step 1793, loss 0.0965463, acc 1, learning_rate 0.000141113\n",
      "2019-01-29T02:27:02.902130: step 1794, loss 0.0689965, acc 1, learning_rate 0.000141004\n",
      "2019-01-29T02:27:03.028201: step 1795, loss 0.0786928, acc 1, learning_rate 0.000140895\n",
      "2019-01-29T02:27:03.110758: step 1796, loss 0.0953864, acc 0.984375, learning_rate 0.000140786\n",
      "2019-01-29T02:27:03.240469: step 1797, loss 0.0855221, acc 1, learning_rate 0.000140677\n",
      "2019-01-29T02:27:03.367743: step 1798, loss 0.0760854, acc 1, learning_rate 0.000140569\n",
      "2019-01-29T02:27:03.498934: step 1799, loss 0.10304, acc 1, learning_rate 0.00014046\n",
      "2019-01-29T02:27:03.622348: step 1800, loss 0.094111, acc 1, learning_rate 0.000140353\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:27:03.649501: step 1800, loss 0.626952, acc 0.757974\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1800\n",
      "\n",
      "2019-01-29T02:27:03.984777: step 1801, loss 0.0734062, acc 1, learning_rate 0.000140245\n",
      "2019-01-29T02:27:04.095835: step 1802, loss 0.0932666, acc 0.984375, learning_rate 0.000140138\n",
      "2019-01-29T02:27:04.216621: step 1803, loss 0.0972411, acc 0.984375, learning_rate 0.000140031\n",
      "2019-01-29T02:27:04.348124: step 1804, loss 0.111493, acc 0.984375, learning_rate 0.000139924\n",
      "2019-01-29T02:27:04.475628: step 1805, loss 0.0873214, acc 1, learning_rate 0.000139818\n",
      "2019-01-29T02:27:04.603442: step 1806, loss 0.117283, acc 0.984375, learning_rate 0.000139712\n",
      "2019-01-29T02:27:04.690992: step 1807, loss 0.0716461, acc 1, learning_rate 0.000139606\n",
      "2019-01-29T02:27:04.817221: step 1808, loss 0.0895574, acc 1, learning_rate 0.000139501\n",
      "2019-01-29T02:27:04.944526: step 1809, loss 0.0903269, acc 1, learning_rate 0.000139395\n",
      "2019-01-29T02:27:05.064683: step 1810, loss 0.0949369, acc 0.984375, learning_rate 0.00013929\n",
      "2019-01-29T02:27:05.195614: step 1811, loss 0.0922425, acc 1, learning_rate 0.000139186\n",
      "2019-01-29T02:27:05.316225: step 1812, loss 0.100318, acc 1, learning_rate 0.000139081\n",
      "2019-01-29T02:27:05.440688: step 1813, loss 0.0964663, acc 1, learning_rate 0.000138977\n",
      "2019-01-29T02:27:05.524768: step 1814, loss 0.103989, acc 1, learning_rate 0.000138873\n",
      "2019-01-29T02:27:05.657065: step 1815, loss 0.0944416, acc 0.984375, learning_rate 0.00013877\n",
      "2019-01-29T02:27:05.780632: step 1816, loss 0.087255, acc 1, learning_rate 0.000138666\n",
      "2019-01-29T02:27:05.871459: step 1817, loss 0.0988583, acc 1, learning_rate 0.000138563\n",
      "2019-01-29T02:27:05.997050: step 1818, loss 0.0756588, acc 1, learning_rate 0.000138461\n",
      "2019-01-29T02:27:06.119907: step 1819, loss 0.0755308, acc 1, learning_rate 0.000138358\n",
      "2019-01-29T02:27:06.250606: step 1820, loss 0.110204, acc 1, learning_rate 0.000138256\n",
      "2019-01-29T02:27:06.374323: step 1821, loss 0.0869795, acc 1, learning_rate 0.000138154\n",
      "2019-01-29T02:27:06.505821: step 1822, loss 0.0976623, acc 1, learning_rate 0.000138052\n",
      "2019-01-29T02:27:06.631456: step 1823, loss 0.0782708, acc 1, learning_rate 0.000137951\n",
      "2019-01-29T02:27:06.752983: step 1824, loss 0.0866637, acc 1, learning_rate 0.00013785\n",
      "2019-01-29T02:27:06.879099: step 1825, loss 0.12955, acc 0.984375, learning_rate 0.000137749\n",
      "2019-01-29T02:27:06.999993: step 1826, loss 0.0989707, acc 0.984375, learning_rate 0.000137649\n",
      "2019-01-29T02:27:07.132636: step 1827, loss 0.0879597, acc 1, learning_rate 0.000137548\n",
      "2019-01-29T02:27:07.257072: step 1828, loss 0.0972223, acc 1, learning_rate 0.000137448\n",
      "2019-01-29T02:27:07.379346: step 1829, loss 0.0888223, acc 1, learning_rate 0.000137348\n",
      "2019-01-29T02:27:07.495698: step 1830, loss 0.125634, acc 0.984375, learning_rate 0.000137249\n",
      "2019-01-29T02:27:07.623282: step 1831, loss 0.0820197, acc 1, learning_rate 0.00013715\n",
      "2019-01-29T02:27:07.719463: step 1832, loss 0.0852461, acc 1, learning_rate 0.000137051\n",
      "2019-01-29T02:27:07.848116: step 1833, loss 0.0821225, acc 1, learning_rate 0.000136952\n",
      "2019-01-29T02:27:07.937642: step 1834, loss 0.0809811, acc 0.984375, learning_rate 0.000136854\n",
      "2019-01-29T02:27:08.030011: step 1835, loss 0.0823712, acc 0.984375, learning_rate 0.000136755\n",
      "2019-01-29T02:27:08.153074: step 1836, loss 0.0889502, acc 1, learning_rate 0.000136657\n",
      "2019-01-29T02:27:08.279617: step 1837, loss 0.120045, acc 0.984375, learning_rate 0.00013656\n",
      "2019-01-29T02:27:08.406763: step 1838, loss 0.0866618, acc 1, learning_rate 0.000136462\n",
      "2019-01-29T02:27:08.490581: step 1839, loss 0.0990036, acc 1, learning_rate 0.000136365\n",
      "2019-01-29T02:27:08.580598: step 1840, loss 0.0791648, acc 1, learning_rate 0.000136268\n",
      "2019-01-29T02:27:08.708823: step 1841, loss 0.0748847, acc 1, learning_rate 0.000136172\n",
      "2019-01-29T02:27:08.843428: step 1842, loss 0.0910932, acc 1, learning_rate 0.000136075\n",
      "2019-01-29T02:27:08.974687: step 1843, loss 0.112853, acc 0.984375, learning_rate 0.000135979\n",
      "2019-01-29T02:27:09.101337: step 1844, loss 0.0910703, acc 0.984375, learning_rate 0.000135883\n",
      "2019-01-29T02:27:09.228600: step 1845, loss 0.0634068, acc 1, learning_rate 0.000135788\n",
      "2019-01-29T02:27:09.347272: step 1846, loss 0.0830088, acc 1, learning_rate 0.000135692\n",
      "2019-01-29T02:27:09.469874: step 1847, loss 0.102739, acc 0.984375, learning_rate 0.000135597\n",
      "2019-01-29T02:27:09.592993: step 1848, loss 0.117093, acc 0.984375, learning_rate 0.000135503\n",
      "2019-01-29T02:27:09.714135: step 1849, loss 0.0737161, acc 1, learning_rate 0.000135408\n",
      "2019-01-29T02:27:09.801042: step 1850, loss 0.0760201, acc 1, learning_rate 0.000135314\n",
      "2019-01-29T02:27:09.924524: step 1851, loss 0.106659, acc 0.984375, learning_rate 0.00013522\n",
      "2019-01-29T02:27:10.047900: step 1852, loss 0.0803466, acc 1, learning_rate 0.000135126\n",
      "2019-01-29T02:27:10.172167: step 1853, loss 0.101235, acc 0.984375, learning_rate 0.000135032\n",
      "2019-01-29T02:27:10.261462: step 1854, loss 0.0796907, acc 1, learning_rate 0.000134939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:27:10.386077: step 1855, loss 0.075526, acc 1, learning_rate 0.000134846\n",
      "2019-01-29T02:27:10.512911: step 1856, loss 0.0771559, acc 1, learning_rate 0.000134753\n",
      "2019-01-29T02:27:10.639149: step 1857, loss 0.0771583, acc 1, learning_rate 0.00013466\n",
      "2019-01-29T02:27:10.729205: step 1858, loss 0.0841067, acc 1, learning_rate 0.000134568\n",
      "2019-01-29T02:27:10.849084: step 1859, loss 0.080728, acc 0.984375, learning_rate 0.000134476\n",
      "2019-01-29T02:27:10.968437: step 1860, loss 0.0839655, acc 0.984375, learning_rate 0.000134384\n",
      "2019-01-29T02:27:11.095531: step 1861, loss 0.0849651, acc 1, learning_rate 0.000134292\n",
      "2019-01-29T02:27:11.226149: step 1862, loss 0.0825479, acc 1, learning_rate 0.000134201\n",
      "2019-01-29T02:27:11.350091: step 1863, loss 0.0802948, acc 1, learning_rate 0.00013411\n",
      "2019-01-29T02:27:11.472057: step 1864, loss 0.091626, acc 0.984375, learning_rate 0.000134019\n",
      "2019-01-29T02:27:11.560979: step 1865, loss 0.104675, acc 0.984375, learning_rate 0.000133928\n",
      "2019-01-29T02:27:11.650465: step 1866, loss 0.0784685, acc 1, learning_rate 0.000133838\n",
      "2019-01-29T02:27:11.774777: step 1867, loss 0.0816648, acc 1, learning_rate 0.000133748\n",
      "2019-01-29T02:27:11.898802: step 1868, loss 0.0945148, acc 1, learning_rate 0.000133658\n",
      "2019-01-29T02:27:12.029343: step 1869, loss 0.0904308, acc 1, learning_rate 0.000133568\n",
      "2019-01-29T02:27:12.153735: step 1870, loss 0.104423, acc 1, learning_rate 0.000133479\n",
      "2019-01-29T02:27:12.260905: step 1871, loss 0.0932868, acc 1, learning_rate 0.00013339\n",
      "2019-01-29T02:27:12.389245: step 1872, loss 0.0924129, acc 0.984375, learning_rate 0.000133301\n",
      "2019-01-29T02:27:12.520639: step 1873, loss 0.0880457, acc 1, learning_rate 0.000133212\n",
      "2019-01-29T02:27:12.648245: step 1874, loss 0.0868487, acc 1, learning_rate 0.000133123\n",
      "2019-01-29T02:27:12.778790: step 1875, loss 0.0827511, acc 1, learning_rate 0.000133035\n",
      "2019-01-29T02:27:12.907150: step 1876, loss 0.0702432, acc 1, learning_rate 0.000132947\n",
      "2019-01-29T02:27:12.996843: step 1877, loss 0.102205, acc 0.984375, learning_rate 0.000132859\n",
      "2019-01-29T02:27:13.123961: step 1878, loss 0.121612, acc 0.984375, learning_rate 0.000132772\n",
      "2019-01-29T02:27:13.246791: step 1879, loss 0.086728, acc 0.984375, learning_rate 0.000132685\n",
      "2019-01-29T02:27:13.373624: step 1880, loss 0.0843167, acc 1, learning_rate 0.000132597\n",
      "2019-01-29T02:27:13.492610: step 1881, loss 0.111437, acc 0.984375, learning_rate 0.000132511\n",
      "2019-01-29T02:27:13.583553: step 1882, loss 0.0907332, acc 1, learning_rate 0.000132424\n",
      "2019-01-29T02:27:13.714941: step 1883, loss 0.0959743, acc 1, learning_rate 0.000132338\n",
      "2019-01-29T02:27:13.841658: step 1884, loss 0.0964691, acc 0.984375, learning_rate 0.000132251\n",
      "2019-01-29T02:27:13.967891: step 1885, loss 0.123904, acc 0.984375, learning_rate 0.000132166\n",
      "2019-01-29T02:27:14.089230: step 1886, loss 0.0867402, acc 1, learning_rate 0.00013208\n",
      "2019-01-29T02:27:14.218323: step 1887, loss 0.0878714, acc 1, learning_rate 0.000131994\n",
      "2019-01-29T02:27:14.343630: step 1888, loss 0.0853986, acc 1, learning_rate 0.000131909\n",
      "2019-01-29T02:27:14.464945: step 1889, loss 0.10266, acc 1, learning_rate 0.000131824\n",
      "2019-01-29T02:27:14.588427: step 1890, loss 0.0742477, acc 1, learning_rate 0.000131739\n",
      "2019-01-29T02:27:14.707140: step 1891, loss 0.0828605, acc 1, learning_rate 0.000131655\n",
      "2019-01-29T02:27:14.832000: step 1892, loss 0.09385, acc 1, learning_rate 0.00013157\n",
      "2019-01-29T02:27:14.953820: step 1893, loss 0.0725819, acc 1, learning_rate 0.000131486\n",
      "2019-01-29T02:27:15.077376: step 1894, loss 0.0784828, acc 1, learning_rate 0.000131402\n",
      "2019-01-29T02:27:15.199797: step 1895, loss 0.0889292, acc 1, learning_rate 0.000131319\n",
      "2019-01-29T02:27:15.331576: step 1896, loss 0.0679781, acc 1, learning_rate 0.000131235\n",
      "2019-01-29T02:27:15.459390: step 1897, loss 0.0907286, acc 1, learning_rate 0.000131152\n",
      "2019-01-29T02:27:15.584731: step 1898, loss 0.103999, acc 0.984375, learning_rate 0.000131069\n",
      "2019-01-29T02:27:15.706423: step 1899, loss 0.0727727, acc 1, learning_rate 0.000130986\n",
      "2019-01-29T02:27:15.833117: step 1900, loss 0.108892, acc 0.984375, learning_rate 0.000130904\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:27:15.860515: step 1900, loss 0.626486, acc 0.75985\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-1900\n",
      "\n",
      "2019-01-29T02:27:16.190404: step 1901, loss 0.0925776, acc 1, learning_rate 0.000130821\n",
      "2019-01-29T02:27:16.310560: step 1902, loss 0.0943287, acc 1, learning_rate 0.000130739\n",
      "2019-01-29T02:27:16.433984: step 1903, loss 0.0901739, acc 1, learning_rate 0.000130657\n",
      "2019-01-29T02:27:16.559842: step 1904, loss 0.102395, acc 1, learning_rate 0.000130576\n",
      "2019-01-29T02:27:16.691438: step 1905, loss 0.0815152, acc 1, learning_rate 0.000130494\n",
      "2019-01-29T02:27:16.821452: step 1906, loss 0.117177, acc 0.984375, learning_rate 0.000130413\n",
      "2019-01-29T02:27:16.950361: step 1907, loss 0.0940523, acc 1, learning_rate 0.000130332\n",
      "2019-01-29T02:27:17.083165: step 1908, loss 0.0838414, acc 1, learning_rate 0.000130251\n",
      "2019-01-29T02:27:17.215753: step 1909, loss 0.0748014, acc 1, learning_rate 0.000130171\n",
      "2019-01-29T02:27:17.337994: step 1910, loss 0.103071, acc 1, learning_rate 0.00013009\n",
      "2019-01-29T02:27:17.427554: step 1911, loss 0.109823, acc 1, learning_rate 0.00013001\n",
      "2019-01-29T02:27:17.549428: step 1912, loss 0.106079, acc 1, learning_rate 0.00012993\n",
      "2019-01-29T02:27:17.640266: step 1913, loss 0.0908444, acc 1, learning_rate 0.00012985\n",
      "2019-01-29T02:27:17.772083: step 1914, loss 0.0761971, acc 1, learning_rate 0.000129771\n",
      "2019-01-29T02:27:17.896167: step 1915, loss 0.0883252, acc 1, learning_rate 0.000129692\n",
      "2019-01-29T02:27:18.020817: step 1916, loss 0.0961857, acc 0.984375, learning_rate 0.000129612\n",
      "2019-01-29T02:27:18.148240: step 1917, loss 0.122422, acc 0.984375, learning_rate 0.000129534\n",
      "2019-01-29T02:27:18.270591: step 1918, loss 0.0775346, acc 1, learning_rate 0.000129455\n",
      "2019-01-29T02:27:18.397206: step 1919, loss 0.0990686, acc 0.984375, learning_rate 0.000129376\n",
      "2019-01-29T02:27:18.525097: step 1920, loss 0.0982325, acc 1, learning_rate 0.000129298\n",
      "2019-01-29T02:27:18.609602: step 1921, loss 0.0800755, acc 1, learning_rate 0.00012922\n",
      "2019-01-29T02:27:18.736649: step 1922, loss 0.107058, acc 1, learning_rate 0.000129142\n",
      "2019-01-29T02:27:18.864529: step 1923, loss 0.0833003, acc 1, learning_rate 0.000129065\n",
      "2019-01-29T02:27:18.990252: step 1924, loss 0.073989, acc 1, learning_rate 0.000128987\n",
      "2019-01-29T02:27:19.090806: step 1925, loss 0.0855778, acc 1, learning_rate 0.00012891\n",
      "2019-01-29T02:27:19.218873: step 1926, loss 0.0851336, acc 1, learning_rate 0.000128833\n",
      "2019-01-29T02:27:19.345655: step 1927, loss 0.121532, acc 0.984375, learning_rate 0.000128756\n",
      "2019-01-29T02:27:19.477141: step 1928, loss 0.104819, acc 0.96875, learning_rate 0.000128679\n",
      "2019-01-29T02:27:19.600099: step 1929, loss 0.0984906, acc 0.984375, learning_rate 0.000128603\n",
      "2019-01-29T02:27:19.717050: step 1930, loss 0.0715023, acc 1, learning_rate 0.000128527\n",
      "2019-01-29T02:27:19.849386: step 1931, loss 0.0886087, acc 0.984375, learning_rate 0.000128451\n",
      "2019-01-29T02:27:19.965879: step 1932, loss 0.0968163, acc 1, learning_rate 0.000128375\n",
      "2019-01-29T02:27:20.094646: step 1933, loss 0.0816918, acc 0.984375, learning_rate 0.000128299\n",
      "2019-01-29T02:27:20.217050: step 1934, loss 0.0825859, acc 0.984375, learning_rate 0.000128224\n",
      "2019-01-29T02:27:20.308367: step 1935, loss 0.0979623, acc 0.984375, learning_rate 0.000128149\n",
      "2019-01-29T02:27:20.414661: step 1936, loss 0.104841, acc 1, learning_rate 0.000128074\n",
      "2019-01-29T02:27:20.542924: step 1937, loss 0.100179, acc 0.984375, learning_rate 0.000127999\n",
      "2019-01-29T02:27:20.665060: step 1938, loss 0.0738987, acc 1, learning_rate 0.000127924\n",
      "2019-01-29T02:27:20.785621: step 1939, loss 0.102942, acc 0.984375, learning_rate 0.00012785\n",
      "2019-01-29T02:27:20.912244: step 1940, loss 0.0738122, acc 1, learning_rate 0.000127776\n",
      "2019-01-29T02:27:21.002386: step 1941, loss 0.0914104, acc 0.984375, learning_rate 0.000127702\n",
      "2019-01-29T02:27:21.134184: step 1942, loss 0.0829921, acc 1, learning_rate 0.000127628\n",
      "2019-01-29T02:27:21.223529: step 1943, loss 0.085809, acc 0.984375, learning_rate 0.000127554\n",
      "2019-01-29T02:27:21.345282: step 1944, loss 0.0980139, acc 0.984375, learning_rate 0.000127481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:27:21.432065: step 1945, loss 0.0872954, acc 1, learning_rate 0.000127408\n",
      "2019-01-29T02:27:21.562040: step 1946, loss 0.118229, acc 0.984375, learning_rate 0.000127335\n",
      "2019-01-29T02:27:21.687748: step 1947, loss 0.0797855, acc 0.984375, learning_rate 0.000127262\n",
      "2019-01-29T02:27:21.819227: step 1948, loss 0.0948273, acc 0.984375, learning_rate 0.000127189\n",
      "2019-01-29T02:27:21.946707: step 1949, loss 0.0889344, acc 1, learning_rate 0.000127117\n",
      "2019-01-29T02:27:22.029846: step 1950, loss 0.0615773, acc 1, learning_rate 0.000127045\n",
      "2019-01-29T02:27:22.151557: step 1951, loss 0.0691579, acc 1, learning_rate 0.000126973\n",
      "2019-01-29T02:27:22.272108: step 1952, loss 0.0915605, acc 1, learning_rate 0.000126901\n",
      "2019-01-29T02:27:22.401299: step 1953, loss 0.0919394, acc 1, learning_rate 0.000126829\n",
      "2019-01-29T02:27:22.525745: step 1954, loss 0.0685589, acc 1, learning_rate 0.000126758\n",
      "2019-01-29T02:27:22.652449: step 1955, loss 0.0606447, acc 1, learning_rate 0.000126686\n",
      "2019-01-29T02:27:22.773924: step 1956, loss 0.117591, acc 0.96875, learning_rate 0.000126615\n",
      "2019-01-29T02:27:22.861948: step 1957, loss 0.100273, acc 0.984375, learning_rate 0.000126544\n",
      "2019-01-29T02:27:22.986843: step 1958, loss 0.101195, acc 0.984375, learning_rate 0.000126474\n",
      "2019-01-29T02:27:23.073734: step 1959, loss 0.123627, acc 0.984375, learning_rate 0.000126403\n",
      "2019-01-29T02:27:23.199070: step 1960, loss 0.0851949, acc 0.984375, learning_rate 0.000126333\n",
      "2019-01-29T02:27:23.318564: step 1961, loss 0.0836917, acc 1, learning_rate 0.000126263\n",
      "2019-01-29T02:27:23.451895: step 1962, loss 0.0861361, acc 0.984375, learning_rate 0.000126193\n",
      "2019-01-29T02:27:23.576607: step 1963, loss 0.106661, acc 0.96875, learning_rate 0.000126123\n",
      "2019-01-29T02:27:23.707725: step 1964, loss 0.0833994, acc 1, learning_rate 0.000126053\n",
      "2019-01-29T02:27:23.838129: step 1965, loss 0.0801437, acc 1, learning_rate 0.000125984\n",
      "2019-01-29T02:27:23.962771: step 1966, loss 0.147124, acc 0.96875, learning_rate 0.000125915\n",
      "2019-01-29T02:27:24.090410: step 1967, loss 0.104244, acc 0.984375, learning_rate 0.000125846\n",
      "2019-01-29T02:27:24.229983: step 1968, loss 0.0757526, acc 1, learning_rate 0.000125777\n",
      "2019-01-29T02:27:24.351471: step 1969, loss 0.110748, acc 0.984375, learning_rate 0.000125708\n",
      "2019-01-29T02:27:24.478065: step 1970, loss 0.078793, acc 1, learning_rate 0.00012564\n",
      "2019-01-29T02:27:24.560948: step 1971, loss 0.0826361, acc 0.984375, learning_rate 0.000125571\n",
      "2019-01-29T02:27:24.691730: step 1972, loss 0.0911857, acc 0.984375, learning_rate 0.000125503\n",
      "2019-01-29T02:27:24.821788: step 1973, loss 0.0834358, acc 1, learning_rate 0.000125435\n",
      "2019-01-29T02:27:24.945687: step 1974, loss 0.102279, acc 0.984375, learning_rate 0.000125367\n",
      "2019-01-29T02:27:25.072468: step 1975, loss 0.0984902, acc 1, learning_rate 0.0001253\n",
      "2019-01-29T02:27:25.200074: step 1976, loss 0.0798856, acc 1, learning_rate 0.000125232\n",
      "2019-01-29T02:27:25.324190: step 1977, loss 0.0880269, acc 1, learning_rate 0.000125165\n",
      "2019-01-29T02:27:25.408910: step 1978, loss 0.0907418, acc 1, learning_rate 0.000125098\n",
      "2019-01-29T02:27:25.536240: step 1979, loss 0.0977801, acc 0.984375, learning_rate 0.000125031\n",
      "2019-01-29T02:27:25.659502: step 1980, loss 0.142913, acc 0.984375, learning_rate 0.000124965\n",
      "2019-01-29T02:27:25.791334: step 1981, loss 0.0720371, acc 1, learning_rate 0.000124898\n",
      "2019-01-29T02:27:25.880395: step 1982, loss 0.100581, acc 0.984375, learning_rate 0.000124832\n",
      "2019-01-29T02:27:26.009299: step 1983, loss 0.0871769, acc 0.984375, learning_rate 0.000124766\n",
      "2019-01-29T02:27:26.093839: step 1984, loss 0.0941626, acc 1, learning_rate 0.0001247\n",
      "2019-01-29T02:27:26.219045: step 1985, loss 0.0892192, acc 1, learning_rate 0.000124634\n",
      "2019-01-29T02:27:26.308866: step 1986, loss 0.0976716, acc 1, learning_rate 0.000124568\n",
      "2019-01-29T02:27:26.435703: step 1987, loss 0.0981216, acc 1, learning_rate 0.000124503\n",
      "2019-01-29T02:27:26.561791: step 1988, loss 0.0793904, acc 1, learning_rate 0.000124437\n",
      "2019-01-29T02:27:26.685643: step 1989, loss 0.0675621, acc 1, learning_rate 0.000124372\n",
      "2019-01-29T02:27:26.815855: step 1990, loss 0.0885343, acc 1, learning_rate 0.000124307\n",
      "2019-01-29T02:27:26.938627: step 1991, loss 0.0847182, acc 1, learning_rate 0.000124243\n",
      "2019-01-29T02:27:27.067676: step 1992, loss 0.102075, acc 1, learning_rate 0.000124178\n",
      "2019-01-29T02:27:27.195677: step 1993, loss 0.0718066, acc 1, learning_rate 0.000124114\n",
      "2019-01-29T02:27:27.323277: step 1994, loss 0.0851361, acc 1, learning_rate 0.000124049\n",
      "2019-01-29T02:27:27.451586: step 1995, loss 0.0960259, acc 0.984375, learning_rate 0.000123985\n",
      "2019-01-29T02:27:27.580748: step 1996, loss 0.0758112, acc 1, learning_rate 0.000123921\n",
      "2019-01-29T02:27:27.703439: step 1997, loss 0.0841654, acc 0.984375, learning_rate 0.000123858\n",
      "2019-01-29T02:27:27.831164: step 1998, loss 0.0783326, acc 1, learning_rate 0.000123794\n",
      "2019-01-29T02:27:27.959244: step 1999, loss 0.0763403, acc 1, learning_rate 0.000123731\n",
      "2019-01-29T02:27:28.047215: step 2000, loss 0.0865059, acc 1, learning_rate 0.000123667\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:27:28.074252: step 2000, loss 0.624585, acc 0.760788\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2000\n",
      "\n",
      "2019-01-29T02:27:28.413777: step 2001, loss 0.0863934, acc 0.984375, learning_rate 0.000123604\n",
      "2019-01-29T02:27:28.541106: step 2002, loss 0.0844237, acc 0.984375, learning_rate 0.000123542\n",
      "2019-01-29T02:27:28.662435: step 2003, loss 0.114858, acc 0.953125, learning_rate 0.000123479\n",
      "2019-01-29T02:27:28.783632: step 2004, loss 0.0820634, acc 1, learning_rate 0.000123416\n",
      "2019-01-29T02:27:28.906210: step 2005, loss 0.0847295, acc 1, learning_rate 0.000123354\n",
      "2019-01-29T02:27:29.036677: step 2006, loss 0.0678694, acc 1, learning_rate 0.000123292\n",
      "2019-01-29T02:27:29.159486: step 2007, loss 0.0741521, acc 1, learning_rate 0.00012323\n",
      "2019-01-29T02:27:29.249977: step 2008, loss 0.0750387, acc 1, learning_rate 0.000123168\n",
      "2019-01-29T02:27:29.376247: step 2009, loss 0.0796887, acc 1, learning_rate 0.000123106\n",
      "2019-01-29T02:27:29.501547: step 2010, loss 0.0904818, acc 1, learning_rate 0.000123044\n",
      "2019-01-29T02:27:29.625707: step 2011, loss 0.0864945, acc 1, learning_rate 0.000122983\n",
      "2019-01-29T02:27:29.757755: step 2012, loss 0.0893869, acc 1, learning_rate 0.000122922\n",
      "2019-01-29T02:27:29.884191: step 2013, loss 0.0794684, acc 1, learning_rate 0.000122861\n",
      "2019-01-29T02:27:30.021100: step 2014, loss 0.0780749, acc 1, learning_rate 0.0001228\n",
      "2019-01-29T02:27:30.147448: step 2015, loss 0.090485, acc 1, learning_rate 0.000122739\n",
      "2019-01-29T02:27:30.277825: step 2016, loss 0.0760613, acc 1, learning_rate 0.000122678\n",
      "2019-01-29T02:27:30.402952: step 2017, loss 0.0837424, acc 1, learning_rate 0.000122618\n",
      "2019-01-29T02:27:30.529753: step 2018, loss 0.105479, acc 0.984375, learning_rate 0.000122558\n",
      "2019-01-29T02:27:30.658383: step 2019, loss 0.0924907, acc 0.984375, learning_rate 0.000122498\n",
      "2019-01-29T02:27:30.782399: step 2020, loss 0.0724762, acc 1, learning_rate 0.000122438\n",
      "2019-01-29T02:27:30.910116: step 2021, loss 0.0830931, acc 1, learning_rate 0.000122378\n",
      "2019-01-29T02:27:31.038531: step 2022, loss 0.0993375, acc 0.984375, learning_rate 0.000122318\n",
      "2019-01-29T02:27:31.155357: step 2023, loss 0.0691056, acc 1, learning_rate 0.000122259\n",
      "2019-01-29T02:27:31.280459: step 2024, loss 0.076016, acc 1, learning_rate 0.0001222\n",
      "2019-01-29T02:27:31.407809: step 2025, loss 0.105589, acc 0.984375, learning_rate 0.00012214\n",
      "2019-01-29T02:27:31.513437: step 2026, loss 0.13084, acc 0.984375, learning_rate 0.000122081\n",
      "2019-01-29T02:27:31.641029: step 2027, loss 0.083874, acc 1, learning_rate 0.000122023\n",
      "2019-01-29T02:27:31.736124: step 2028, loss 0.0734831, acc 1, learning_rate 0.000121964\n",
      "2019-01-29T02:27:31.866185: step 2029, loss 0.0954026, acc 0.984375, learning_rate 0.000121905\n",
      "2019-01-29T02:27:31.996736: step 2030, loss 0.0838456, acc 1, learning_rate 0.000121847\n",
      "2019-01-29T02:27:32.127144: step 2031, loss 0.0720489, acc 1, learning_rate 0.000121789\n",
      "2019-01-29T02:27:32.214418: step 2032, loss 0.103136, acc 1, learning_rate 0.000121731\n",
      "2019-01-29T02:27:32.340412: step 2033, loss 0.0744058, acc 1, learning_rate 0.000121673\n",
      "2019-01-29T02:27:32.467840: step 2034, loss 0.0767065, acc 1, learning_rate 0.000121615\n",
      "2019-01-29T02:27:32.595216: step 2035, loss 0.0914517, acc 1, learning_rate 0.000121558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:27:32.723228: step 2036, loss 0.131337, acc 0.984375, learning_rate 0.0001215\n",
      "2019-01-29T02:27:32.853208: step 2037, loss 0.0820493, acc 1, learning_rate 0.000121443\n",
      "2019-01-29T02:27:32.933529: step 2038, loss 0.0664962, acc 1, learning_rate 0.000121386\n",
      "2019-01-29T02:27:33.019678: step 2039, loss 0.10635, acc 1, learning_rate 0.000121329\n",
      "2019-01-29T02:27:33.105986: step 2040, loss 0.128828, acc 0.953125, learning_rate 0.000121272\n",
      "2019-01-29T02:27:33.194251: step 2041, loss 0.143679, acc 0.984375, learning_rate 0.000121215\n",
      "2019-01-29T02:27:33.326166: step 2042, loss 0.116654, acc 0.984375, learning_rate 0.000121159\n",
      "2019-01-29T02:27:33.413304: step 2043, loss 0.0654427, acc 1, learning_rate 0.000121102\n",
      "2019-01-29T02:27:33.543069: step 2044, loss 0.0968008, acc 1, learning_rate 0.000121046\n",
      "2019-01-29T02:27:33.665850: step 2045, loss 0.0972656, acc 0.984375, learning_rate 0.00012099\n",
      "2019-01-29T02:27:33.751252: step 2046, loss 0.0833559, acc 1, learning_rate 0.000120934\n",
      "2019-01-29T02:27:33.877770: step 2047, loss 0.0687251, acc 1, learning_rate 0.000120878\n",
      "2019-01-29T02:27:34.005319: step 2048, loss 0.0762437, acc 1, learning_rate 0.000120823\n",
      "2019-01-29T02:27:34.127912: step 2049, loss 0.0989662, acc 1, learning_rate 0.000120767\n",
      "2019-01-29T02:27:34.259873: step 2050, loss 0.11583, acc 1, learning_rate 0.000120712\n",
      "2019-01-29T02:27:34.394371: step 2051, loss 0.0626385, acc 1, learning_rate 0.000120657\n",
      "2019-01-29T02:27:34.484599: step 2052, loss 0.0824103, acc 1, learning_rate 0.000120602\n",
      "2019-01-29T02:27:34.616763: step 2053, loss 0.0826842, acc 1, learning_rate 0.000120547\n",
      "2019-01-29T02:27:34.748171: step 2054, loss 0.0915105, acc 1, learning_rate 0.000120492\n",
      "2019-01-29T02:27:34.876608: step 2055, loss 0.0745147, acc 1, learning_rate 0.000120438\n",
      "2019-01-29T02:27:35.001476: step 2056, loss 0.102785, acc 1, learning_rate 0.000120383\n",
      "2019-01-29T02:27:35.130937: step 2057, loss 0.0825412, acc 1, learning_rate 0.000120329\n",
      "2019-01-29T02:27:35.257223: step 2058, loss 0.0863759, acc 1, learning_rate 0.000120275\n",
      "2019-01-29T02:27:35.383316: step 2059, loss 0.0866762, acc 1, learning_rate 0.000120221\n",
      "2019-01-29T02:27:35.486720: step 2060, loss 0.0896621, acc 1, learning_rate 0.000120167\n",
      "2019-01-29T02:27:35.658284: step 2061, loss 0.100294, acc 1, learning_rate 0.000120113\n",
      "2019-01-29T02:27:35.788045: step 2062, loss 0.0719412, acc 1, learning_rate 0.000120059\n",
      "2019-01-29T02:27:35.901437: step 2063, loss 0.0941151, acc 1, learning_rate 0.000120006\n",
      "2019-01-29T02:27:36.027378: step 2064, loss 0.0886574, acc 1, learning_rate 0.000119953\n",
      "2019-01-29T02:27:36.151098: step 2065, loss 0.0795726, acc 1, learning_rate 0.0001199\n",
      "2019-01-29T02:27:36.273162: step 2066, loss 0.09799, acc 0.984375, learning_rate 0.000119847\n",
      "2019-01-29T02:27:36.360326: step 2067, loss 0.0762908, acc 1, learning_rate 0.000119794\n",
      "2019-01-29T02:27:36.483602: step 2068, loss 0.0925929, acc 0.984375, learning_rate 0.000119741\n",
      "2019-01-29T02:27:36.574031: step 2069, loss 0.0893686, acc 0.984375, learning_rate 0.000119688\n",
      "2019-01-29T02:27:36.701500: step 2070, loss 0.0653196, acc 1, learning_rate 0.000119636\n",
      "2019-01-29T02:27:36.828450: step 2071, loss 0.0859289, acc 1, learning_rate 0.000119584\n",
      "2019-01-29T02:27:36.955971: step 2072, loss 0.0944463, acc 0.984375, learning_rate 0.000119531\n",
      "2019-01-29T02:27:37.081160: step 2073, loss 0.147619, acc 0.96875, learning_rate 0.000119479\n",
      "2019-01-29T02:27:37.207292: step 2074, loss 0.0792085, acc 1, learning_rate 0.000119427\n",
      "2019-01-29T02:27:37.332511: step 2075, loss 0.0891321, acc 0.984375, learning_rate 0.000119376\n",
      "2019-01-29T02:27:37.454596: step 2076, loss 0.0902479, acc 1, learning_rate 0.000119324\n",
      "2019-01-29T02:27:37.576172: step 2077, loss 0.0998359, acc 0.984375, learning_rate 0.000119273\n",
      "2019-01-29T02:27:37.663801: step 2078, loss 0.0777957, acc 1, learning_rate 0.000119221\n",
      "2019-01-29T02:27:37.788163: step 2079, loss 0.0724665, acc 1, learning_rate 0.00011917\n",
      "2019-01-29T02:27:37.903230: step 2080, loss 0.0741848, acc 1, learning_rate 0.000119119\n",
      "2019-01-29T02:27:38.033068: step 2081, loss 0.0704033, acc 1, learning_rate 0.000119068\n",
      "2019-01-29T02:27:38.124150: step 2082, loss 0.101244, acc 0.984375, learning_rate 0.000119017\n",
      "2019-01-29T02:27:38.251201: step 2083, loss 0.0690807, acc 1, learning_rate 0.000118967\n",
      "2019-01-29T02:27:38.376446: step 2084, loss 0.0964982, acc 1, learning_rate 0.000118916\n",
      "2019-01-29T02:27:38.499024: step 2085, loss 0.0860519, acc 1, learning_rate 0.000118866\n",
      "2019-01-29T02:27:38.621430: step 2086, loss 0.0974371, acc 1, learning_rate 0.000118815\n",
      "2019-01-29T02:27:38.728599: step 2087, loss 0.085954, acc 1, learning_rate 0.000118765\n",
      "2019-01-29T02:27:38.850975: step 2088, loss 0.0793374, acc 0.984375, learning_rate 0.000118715\n",
      "2019-01-29T02:27:38.975865: step 2089, loss 0.12791, acc 0.96875, learning_rate 0.000118665\n",
      "2019-01-29T02:27:39.100446: step 2090, loss 0.0970738, acc 0.984375, learning_rate 0.000118616\n",
      "2019-01-29T02:27:39.226820: step 2091, loss 0.0788399, acc 1, learning_rate 0.000118566\n",
      "2019-01-29T02:27:39.344335: step 2092, loss 0.070916, acc 1, learning_rate 0.000118517\n",
      "2019-01-29T02:27:39.475286: step 2093, loss 0.0925998, acc 1, learning_rate 0.000118467\n",
      "2019-01-29T02:27:39.564738: step 2094, loss 0.101009, acc 1, learning_rate 0.000118418\n",
      "2019-01-29T02:27:39.693872: step 2095, loss 0.0700984, acc 1, learning_rate 0.000118369\n",
      "2019-01-29T02:27:39.815166: step 2096, loss 0.0762697, acc 1, learning_rate 0.00011832\n",
      "2019-01-29T02:27:39.904512: step 2097, loss 0.079296, acc 1, learning_rate 0.000118271\n",
      "2019-01-29T02:27:39.993912: step 2098, loss 0.0947764, acc 0.96875, learning_rate 0.000118223\n",
      "2019-01-29T02:27:40.122677: step 2099, loss 0.10758, acc 1, learning_rate 0.000118174\n",
      "2019-01-29T02:27:40.212502: step 2100, loss 0.0842538, acc 1, learning_rate 0.000118126\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:27:40.238538: step 2100, loss 0.629184, acc 0.757036\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2100\n",
      "\n",
      "2019-01-29T02:27:40.536196: step 2101, loss 0.0925127, acc 1, learning_rate 0.000118077\n",
      "2019-01-29T02:27:40.660414: step 2102, loss 0.0743814, acc 1, learning_rate 0.000118029\n",
      "2019-01-29T02:27:40.779976: step 2103, loss 0.0789587, acc 0.984375, learning_rate 0.000117981\n",
      "2019-01-29T02:27:40.910973: step 2104, loss 0.0880182, acc 1, learning_rate 0.000117933\n",
      "2019-01-29T02:27:41.041938: step 2105, loss 0.0833005, acc 1, learning_rate 0.000117885\n",
      "2019-01-29T02:27:41.166357: step 2106, loss 0.0954382, acc 0.984375, learning_rate 0.000117838\n",
      "2019-01-29T02:27:41.291209: step 2107, loss 0.0955567, acc 0.984375, learning_rate 0.00011779\n",
      "2019-01-29T02:27:41.376959: step 2108, loss 0.0791273, acc 1, learning_rate 0.000117743\n",
      "2019-01-29T02:27:41.464441: step 2109, loss 0.0730814, acc 1, learning_rate 0.000117696\n",
      "2019-01-29T02:27:41.589403: step 2110, loss 0.0726843, acc 1, learning_rate 0.000117648\n",
      "2019-01-29T02:27:41.677416: step 2111, loss 0.0801282, acc 1, learning_rate 0.000117601\n",
      "2019-01-29T02:27:41.805055: step 2112, loss 0.0914141, acc 1, learning_rate 0.000117555\n",
      "2019-01-29T02:27:41.936899: step 2113, loss 0.0901264, acc 1, learning_rate 0.000117508\n",
      "2019-01-29T02:27:42.067616: step 2114, loss 0.0874652, acc 1, learning_rate 0.000117461\n",
      "2019-01-29T02:27:42.193434: step 2115, loss 0.0984439, acc 0.984375, learning_rate 0.000117415\n",
      "2019-01-29T02:27:42.320017: step 2116, loss 0.0789464, acc 1, learning_rate 0.000117368\n",
      "2019-01-29T02:27:42.444605: step 2117, loss 0.0680333, acc 1, learning_rate 0.000117322\n",
      "2019-01-29T02:27:42.575091: step 2118, loss 0.115828, acc 0.984375, learning_rate 0.000117276\n",
      "2019-01-29T02:27:42.702932: step 2119, loss 0.0830888, acc 1, learning_rate 0.00011723\n",
      "2019-01-29T02:27:42.825194: step 2120, loss 0.157967, acc 0.984375, learning_rate 0.000117184\n",
      "2019-01-29T02:27:42.911911: step 2121, loss 0.0722307, acc 1, learning_rate 0.000117138\n",
      "2019-01-29T02:27:43.031737: step 2122, loss 0.0620532, acc 1, learning_rate 0.000117092\n",
      "2019-01-29T02:27:43.116170: step 2123, loss 0.0877585, acc 1, learning_rate 0.000117047\n",
      "2019-01-29T02:27:43.245612: step 2124, loss 0.0865758, acc 1, learning_rate 0.000117001\n",
      "2019-01-29T02:27:43.373632: step 2125, loss 0.0721296, acc 1, learning_rate 0.000116956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:27:43.503136: step 2126, loss 0.0705007, acc 1, learning_rate 0.000116911\n",
      "2019-01-29T02:27:43.623642: step 2127, loss 0.0760816, acc 1, learning_rate 0.000116866\n",
      "2019-01-29T02:27:43.712641: step 2128, loss 0.0874216, acc 1, learning_rate 0.000116821\n",
      "2019-01-29T02:27:43.843275: step 2129, loss 0.0750894, acc 1, learning_rate 0.000116776\n",
      "2019-01-29T02:27:43.932653: step 2130, loss 0.0774788, acc 1, learning_rate 0.000116731\n",
      "2019-01-29T02:27:44.047314: step 2131, loss 0.0960304, acc 1, learning_rate 0.000116687\n",
      "2019-01-29T02:27:44.177327: step 2132, loss 0.0943985, acc 1, learning_rate 0.000116642\n",
      "2019-01-29T02:27:44.299208: step 2133, loss 0.0790203, acc 1, learning_rate 0.000116598\n",
      "2019-01-29T02:27:44.420251: step 2134, loss 0.0715525, acc 1, learning_rate 0.000116554\n",
      "2019-01-29T02:27:44.551045: step 2135, loss 0.0715619, acc 1, learning_rate 0.00011651\n",
      "2019-01-29T02:27:44.636667: step 2136, loss 0.0666364, acc 1, learning_rate 0.000116466\n",
      "2019-01-29T02:27:44.724940: step 2137, loss 0.0901402, acc 1, learning_rate 0.000116422\n",
      "2019-01-29T02:27:44.849389: step 2138, loss 0.093504, acc 0.96875, learning_rate 0.000116378\n",
      "2019-01-29T02:27:44.975491: step 2139, loss 0.0798136, acc 1, learning_rate 0.000116335\n",
      "2019-01-29T02:27:45.103799: step 2140, loss 0.0977123, acc 1, learning_rate 0.000116291\n",
      "2019-01-29T02:27:45.230208: step 2141, loss 0.0912219, acc 1, learning_rate 0.000116248\n",
      "2019-01-29T02:27:45.360979: step 2142, loss 0.114864, acc 0.984375, learning_rate 0.000116204\n",
      "2019-01-29T02:27:45.491003: step 2143, loss 0.105173, acc 0.984375, learning_rate 0.000116161\n",
      "2019-01-29T02:27:45.616981: step 2144, loss 0.100345, acc 0.984375, learning_rate 0.000116118\n",
      "2019-01-29T02:27:45.810823: step 2145, loss 0.0858814, acc 1, learning_rate 0.000116075\n",
      "2019-01-29T02:27:45.935540: step 2146, loss 0.0674334, acc 1, learning_rate 0.000116032\n",
      "2019-01-29T02:27:46.066076: step 2147, loss 0.0828103, acc 1, learning_rate 0.00011599\n",
      "2019-01-29T02:27:46.187635: step 2148, loss 0.0734431, acc 1, learning_rate 0.000115947\n",
      "2019-01-29T02:27:46.275671: step 2149, loss 0.0775229, acc 1, learning_rate 0.000115905\n",
      "2019-01-29T02:27:46.406071: step 2150, loss 0.0722513, acc 1, learning_rate 0.000115862\n",
      "2019-01-29T02:27:46.529351: step 2151, loss 0.0756843, acc 1, learning_rate 0.00011582\n",
      "2019-01-29T02:27:46.651692: step 2152, loss 0.0820725, acc 1, learning_rate 0.000115778\n",
      "2019-01-29T02:27:46.781856: step 2153, loss 0.0751669, acc 1, learning_rate 0.000115736\n",
      "2019-01-29T02:27:46.904485: step 2154, loss 0.0836874, acc 1, learning_rate 0.000115694\n",
      "2019-01-29T02:27:47.034380: step 2155, loss 0.124399, acc 0.96875, learning_rate 0.000115652\n",
      "2019-01-29T02:27:47.165460: step 2156, loss 0.108413, acc 0.984375, learning_rate 0.00011561\n",
      "2019-01-29T02:27:47.295961: step 2157, loss 0.0734568, acc 1, learning_rate 0.000115569\n",
      "2019-01-29T02:27:47.426964: step 2158, loss 0.128639, acc 0.96875, learning_rate 0.000115527\n",
      "2019-01-29T02:27:47.552984: step 2159, loss 0.0884693, acc 1, learning_rate 0.000115486\n",
      "2019-01-29T02:27:47.681063: step 2160, loss 0.088814, acc 1, learning_rate 0.000115445\n",
      "2019-01-29T02:27:47.792723: step 2161, loss 0.142079, acc 0.96875, learning_rate 0.000115403\n",
      "2019-01-29T02:27:47.918457: step 2162, loss 0.0860879, acc 1, learning_rate 0.000115362\n",
      "2019-01-29T02:27:48.046807: step 2163, loss 0.0777239, acc 1, learning_rate 0.000115321\n",
      "2019-01-29T02:27:48.166737: step 2164, loss 0.0914862, acc 0.984375, learning_rate 0.000115281\n",
      "2019-01-29T02:27:48.296689: step 2165, loss 0.0795473, acc 1, learning_rate 0.00011524\n",
      "2019-01-29T02:27:48.423760: step 2166, loss 0.0763048, acc 1, learning_rate 0.000115199\n",
      "2019-01-29T02:27:48.531042: step 2167, loss 0.0980991, acc 1, learning_rate 0.000115159\n",
      "2019-01-29T02:27:48.635045: step 2168, loss 0.0884964, acc 1, learning_rate 0.000115118\n",
      "2019-01-29T02:27:48.758271: step 2169, loss 0.0950202, acc 0.984375, learning_rate 0.000115078\n",
      "2019-01-29T02:27:48.843236: step 2170, loss 0.086802, acc 1, learning_rate 0.000115038\n",
      "2019-01-29T02:27:48.969914: step 2171, loss 0.0983868, acc 1, learning_rate 0.000114998\n",
      "2019-01-29T02:27:49.092902: step 2172, loss 0.0960128, acc 1, learning_rate 0.000114958\n",
      "2019-01-29T02:27:49.227747: step 2173, loss 0.0754709, acc 1, learning_rate 0.000114918\n",
      "2019-01-29T02:27:49.351022: step 2174, loss 0.107882, acc 0.984375, learning_rate 0.000114878\n",
      "2019-01-29T02:27:49.474679: step 2175, loss 0.0622439, acc 1, learning_rate 0.000114839\n",
      "2019-01-29T02:27:49.602555: step 2176, loss 0.107236, acc 0.984375, learning_rate 0.000114799\n",
      "2019-01-29T02:27:49.713954: step 2177, loss 0.0798056, acc 1, learning_rate 0.00011476\n",
      "2019-01-29T02:27:49.839834: step 2178, loss 0.0746555, acc 1, learning_rate 0.00011472\n",
      "2019-01-29T02:27:49.962453: step 2179, loss 0.0703787, acc 1, learning_rate 0.000114681\n",
      "2019-01-29T02:27:50.090120: step 2180, loss 0.0877664, acc 0.984375, learning_rate 0.000114642\n",
      "2019-01-29T02:27:50.181263: step 2181, loss 0.0967331, acc 1, learning_rate 0.000114603\n",
      "2019-01-29T02:27:50.303167: step 2182, loss 0.0899087, acc 1, learning_rate 0.000114564\n",
      "2019-01-29T02:27:50.432240: step 2183, loss 0.0773033, acc 1, learning_rate 0.000114525\n",
      "2019-01-29T02:27:50.558152: step 2184, loss 0.082204, acc 1, learning_rate 0.000114487\n",
      "2019-01-29T02:27:50.687090: step 2185, loss 0.0664749, acc 1, learning_rate 0.000114448\n",
      "2019-01-29T02:27:50.803976: step 2186, loss 0.0674539, acc 1, learning_rate 0.00011441\n",
      "2019-01-29T02:27:50.932978: step 2187, loss 0.0737372, acc 1, learning_rate 0.000114371\n",
      "2019-01-29T02:27:51.029498: step 2188, loss 0.0789825, acc 1, learning_rate 0.000114333\n",
      "2019-01-29T02:27:51.156647: step 2189, loss 0.103176, acc 0.984375, learning_rate 0.000114295\n",
      "2019-01-29T02:27:51.281714: step 2190, loss 0.0816013, acc 1, learning_rate 0.000114257\n",
      "2019-01-29T02:27:51.400699: step 2191, loss 0.0888936, acc 1, learning_rate 0.000114219\n",
      "2019-01-29T02:27:51.532224: step 2192, loss 0.0972666, acc 1, learning_rate 0.000114181\n",
      "2019-01-29T02:27:51.658537: step 2193, loss 0.0759589, acc 1, learning_rate 0.000114143\n",
      "2019-01-29T02:27:51.744649: step 2194, loss 0.0925936, acc 0.984375, learning_rate 0.000114105\n",
      "2019-01-29T02:27:51.876063: step 2195, loss 0.0748969, acc 1, learning_rate 0.000114068\n",
      "2019-01-29T02:27:51.965929: step 2196, loss 0.0848749, acc 1, learning_rate 0.00011403\n",
      "2019-01-29T02:27:52.090395: step 2197, loss 0.0740158, acc 1, learning_rate 0.000113993\n",
      "2019-01-29T02:27:52.216072: step 2198, loss 0.0820009, acc 0.984375, learning_rate 0.000113956\n",
      "2019-01-29T02:27:52.304197: step 2199, loss 0.0848238, acc 0.984375, learning_rate 0.000113918\n",
      "2019-01-29T02:27:52.432121: step 2200, loss 0.0834465, acc 1, learning_rate 0.000113881\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:27:52.459642: step 2200, loss 0.6315, acc 0.753283\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2200\n",
      "\n",
      "2019-01-29T02:27:52.797503: step 2201, loss 0.0801467, acc 1, learning_rate 0.000113844\n",
      "2019-01-29T02:27:52.886356: step 2202, loss 0.0880955, acc 0.984375, learning_rate 0.000113807\n",
      "2019-01-29T02:27:52.981501: step 2203, loss 0.0872486, acc 1, learning_rate 0.000113771\n",
      "2019-01-29T02:27:53.108378: step 2204, loss 0.0947759, acc 0.984375, learning_rate 0.000113734\n",
      "2019-01-29T02:27:53.236186: step 2205, loss 0.0966475, acc 0.984375, learning_rate 0.000113697\n",
      "2019-01-29T02:27:53.364560: step 2206, loss 0.0770386, acc 0.984375, learning_rate 0.000113661\n",
      "2019-01-29T02:27:53.490773: step 2207, loss 0.0856438, acc 1, learning_rate 0.000113625\n",
      "2019-01-29T02:27:53.614686: step 2208, loss 0.0748492, acc 1, learning_rate 0.000113588\n",
      "2019-01-29T02:27:53.743562: step 2209, loss 0.0627016, acc 1, learning_rate 0.000113552\n",
      "2019-01-29T02:27:53.830900: step 2210, loss 0.0913283, acc 1, learning_rate 0.000113516\n",
      "2019-01-29T02:27:53.950176: step 2211, loss 0.0984763, acc 1, learning_rate 0.00011348\n",
      "2019-01-29T02:27:54.035513: step 2212, loss 0.0932314, acc 1, learning_rate 0.000113444\n",
      "2019-01-29T02:27:54.162484: step 2213, loss 0.0824982, acc 1, learning_rate 0.000113408\n",
      "2019-01-29T02:27:54.286693: step 2214, loss 0.0905479, acc 1, learning_rate 0.000113372\n",
      "2019-01-29T02:27:54.415042: step 2215, loss 0.0904262, acc 0.984375, learning_rate 0.000113337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:27:54.542223: step 2216, loss 0.0764226, acc 1, learning_rate 0.000113301\n",
      "2019-01-29T02:27:54.669846: step 2217, loss 0.0830602, acc 1, learning_rate 0.000113266\n",
      "2019-01-29T02:27:54.758536: step 2218, loss 0.0828797, acc 1, learning_rate 0.000113231\n",
      "2019-01-29T02:27:54.884578: step 2219, loss 0.081798, acc 1, learning_rate 0.000113195\n",
      "2019-01-29T02:27:55.007735: step 2220, loss 0.0755322, acc 1, learning_rate 0.00011316\n",
      "2019-01-29T02:27:55.129797: step 2221, loss 0.105537, acc 1, learning_rate 0.000113125\n",
      "2019-01-29T02:27:55.251283: step 2222, loss 0.0985148, acc 1, learning_rate 0.00011309\n",
      "2019-01-29T02:27:55.338578: step 2223, loss 0.08324, acc 1, learning_rate 0.000113055\n",
      "2019-01-29T02:27:55.431557: step 2224, loss 0.0875694, acc 1, learning_rate 0.00011302\n",
      "2019-01-29T02:27:55.516231: step 2225, loss 0.111351, acc 0.984375, learning_rate 0.000112986\n",
      "2019-01-29T02:27:55.647592: step 2226, loss 0.156823, acc 0.953125, learning_rate 0.000112951\n",
      "2019-01-29T02:27:55.761943: step 2227, loss 0.134576, acc 0.984375, learning_rate 0.000112917\n",
      "2019-01-29T02:27:55.850170: step 2228, loss 0.091702, acc 1, learning_rate 0.000112882\n",
      "2019-01-29T02:27:55.973695: step 2229, loss 0.0860674, acc 1, learning_rate 0.000112848\n",
      "2019-01-29T02:27:56.101379: step 2230, loss 0.0803897, acc 1, learning_rate 0.000112814\n",
      "2019-01-29T02:27:56.228372: step 2231, loss 0.0928577, acc 1, learning_rate 0.00011278\n",
      "2019-01-29T02:27:56.318847: step 2232, loss 0.0862005, acc 1, learning_rate 0.000112745\n",
      "2019-01-29T02:27:56.448108: step 2233, loss 0.0903183, acc 1, learning_rate 0.000112712\n",
      "2019-01-29T02:27:56.574574: step 2234, loss 0.0976204, acc 1, learning_rate 0.000112678\n",
      "2019-01-29T02:27:56.701700: step 2235, loss 0.0753116, acc 0.984375, learning_rate 0.000112644\n",
      "2019-01-29T02:27:56.791762: step 2236, loss 0.0802231, acc 0.984375, learning_rate 0.00011261\n",
      "2019-01-29T02:27:56.917335: step 2237, loss 0.0664628, acc 1, learning_rate 0.000112577\n",
      "2019-01-29T02:27:57.041907: step 2238, loss 0.0809624, acc 1, learning_rate 0.000112543\n",
      "2019-01-29T02:27:57.162255: step 2239, loss 0.0776928, acc 1, learning_rate 0.00011251\n",
      "2019-01-29T02:27:57.245520: step 2240, loss 0.0922193, acc 0.984375, learning_rate 0.000112476\n",
      "2019-01-29T02:27:57.370834: step 2241, loss 0.0741962, acc 1, learning_rate 0.000112443\n",
      "2019-01-29T02:27:57.497206: step 2242, loss 0.0913283, acc 0.984375, learning_rate 0.00011241\n",
      "2019-01-29T02:27:57.621939: step 2243, loss 0.0782495, acc 1, learning_rate 0.000112377\n",
      "2019-01-29T02:27:57.749487: step 2244, loss 0.0822589, acc 1, learning_rate 0.000112344\n",
      "2019-01-29T02:27:57.837353: step 2245, loss 0.107321, acc 0.984375, learning_rate 0.000112311\n",
      "2019-01-29T02:27:57.968819: step 2246, loss 0.0783675, acc 1, learning_rate 0.000112278\n",
      "2019-01-29T02:27:58.094192: step 2247, loss 0.0959873, acc 1, learning_rate 0.000112246\n",
      "2019-01-29T02:27:58.227572: step 2248, loss 0.136648, acc 0.96875, learning_rate 0.000112213\n",
      "2019-01-29T02:27:58.356778: step 2249, loss 0.0845694, acc 1, learning_rate 0.00011218\n",
      "2019-01-29T02:27:58.484028: step 2250, loss 0.0789469, acc 1, learning_rate 0.000112148\n",
      "2019-01-29T02:27:58.605344: step 2251, loss 0.0782489, acc 1, learning_rate 0.000112116\n",
      "2019-01-29T02:27:58.729508: step 2252, loss 0.0661146, acc 1, learning_rate 0.000112083\n",
      "2019-01-29T02:27:58.857818: step 2253, loss 0.084773, acc 1, learning_rate 0.000112051\n",
      "2019-01-29T02:27:58.980812: step 2254, loss 0.0826356, acc 1, learning_rate 0.000112019\n",
      "2019-01-29T02:27:59.101829: step 2255, loss 0.139935, acc 0.96875, learning_rate 0.000111987\n",
      "2019-01-29T02:27:59.225281: step 2256, loss 0.0830173, acc 1, learning_rate 0.000111955\n",
      "2019-01-29T02:27:59.352635: step 2257, loss 0.0813329, acc 1, learning_rate 0.000111923\n",
      "2019-01-29T02:27:59.472933: step 2258, loss 0.11093, acc 0.984375, learning_rate 0.000111891\n",
      "2019-01-29T02:27:59.600282: step 2259, loss 0.0917154, acc 1, learning_rate 0.00011186\n",
      "2019-01-29T02:27:59.732422: step 2260, loss 0.124976, acc 0.984375, learning_rate 0.000111828\n",
      "2019-01-29T02:27:59.863158: step 2261, loss 0.0658571, acc 1, learning_rate 0.000111797\n",
      "2019-01-29T02:27:59.991828: step 2262, loss 0.0875346, acc 1, learning_rate 0.000111765\n",
      "2019-01-29T02:28:00.113346: step 2263, loss 0.110401, acc 0.984375, learning_rate 0.000111734\n",
      "2019-01-29T02:28:00.240076: step 2264, loss 0.0960519, acc 1, learning_rate 0.000111703\n",
      "2019-01-29T02:28:00.330326: step 2265, loss 0.079041, acc 1, learning_rate 0.000111671\n",
      "2019-01-29T02:28:00.456369: step 2266, loss 0.0996831, acc 0.984375, learning_rate 0.00011164\n",
      "2019-01-29T02:28:00.545137: step 2267, loss 0.103332, acc 0.984375, learning_rate 0.000111609\n",
      "2019-01-29T02:28:00.674176: step 2268, loss 0.0734156, acc 1, learning_rate 0.000111578\n",
      "2019-01-29T02:28:00.800588: step 2269, loss 0.0855802, acc 1, learning_rate 0.000111548\n",
      "2019-01-29T02:28:00.928144: step 2270, loss 0.0723557, acc 1, learning_rate 0.000111517\n",
      "2019-01-29T02:28:01.057746: step 2271, loss 0.0879734, acc 1, learning_rate 0.000111486\n",
      "2019-01-29T02:28:01.182374: step 2272, loss 0.0656382, acc 1, learning_rate 0.000111455\n",
      "2019-01-29T02:28:01.307465: step 2273, loss 0.0906926, acc 0.984375, learning_rate 0.000111425\n",
      "2019-01-29T02:28:01.433488: step 2274, loss 0.0847721, acc 1, learning_rate 0.000111394\n",
      "2019-01-29T02:28:01.538560: step 2275, loss 0.102237, acc 0.984375, learning_rate 0.000111364\n",
      "2019-01-29T02:28:01.667681: step 2276, loss 0.0815806, acc 1, learning_rate 0.000111334\n",
      "2019-01-29T02:28:01.757118: step 2277, loss 0.118682, acc 0.984375, learning_rate 0.000111304\n",
      "2019-01-29T02:28:01.880728: step 2278, loss 0.101181, acc 0.984375, learning_rate 0.000111274\n",
      "2019-01-29T02:28:02.014749: step 2279, loss 0.0746471, acc 1, learning_rate 0.000111244\n",
      "2019-01-29T02:28:02.135736: step 2280, loss 0.085273, acc 1, learning_rate 0.000111214\n",
      "2019-01-29T02:28:02.259253: step 2281, loss 0.0848209, acc 0.984375, learning_rate 0.000111184\n",
      "2019-01-29T02:28:02.350541: step 2282, loss 0.10271, acc 1, learning_rate 0.000111154\n",
      "2019-01-29T02:28:02.473528: step 2283, loss 0.0930967, acc 0.984375, learning_rate 0.000111124\n",
      "2019-01-29T02:28:02.582750: step 2284, loss 0.0824315, acc 1, learning_rate 0.000111095\n",
      "2019-01-29T02:28:02.700776: step 2285, loss 0.128762, acc 0.96875, learning_rate 0.000111065\n",
      "2019-01-29T02:28:02.827642: step 2286, loss 0.125882, acc 0.984375, learning_rate 0.000111035\n",
      "2019-01-29T02:28:02.915867: step 2287, loss 0.0891125, acc 0.984375, learning_rate 0.000111006\n",
      "2019-01-29T02:28:03.036699: step 2288, loss 0.0809119, acc 1, learning_rate 0.000110977\n",
      "2019-01-29T02:28:03.158812: step 2289, loss 0.105252, acc 0.984375, learning_rate 0.000110948\n",
      "2019-01-29T02:28:03.284863: step 2290, loss 0.0889643, acc 0.984375, learning_rate 0.000110918\n",
      "2019-01-29T02:28:03.412309: step 2291, loss 0.088078, acc 1, learning_rate 0.000110889\n",
      "2019-01-29T02:28:03.532636: step 2292, loss 0.0791059, acc 1, learning_rate 0.00011086\n",
      "2019-01-29T02:28:03.652579: step 2293, loss 0.0712338, acc 1, learning_rate 0.000110831\n",
      "2019-01-29T02:28:03.743789: step 2294, loss 0.086417, acc 0.984375, learning_rate 0.000110802\n",
      "2019-01-29T02:28:03.869917: step 2295, loss 0.0815579, acc 1, learning_rate 0.000110774\n",
      "2019-01-29T02:28:03.974031: step 2296, loss 0.0745777, acc 1, learning_rate 0.000110745\n",
      "2019-01-29T02:28:04.099833: step 2297, loss 0.0753231, acc 1, learning_rate 0.000110716\n",
      "2019-01-29T02:28:04.230287: step 2298, loss 0.0936557, acc 0.984375, learning_rate 0.000110688\n",
      "2019-01-29T02:28:04.328316: step 2299, loss 0.0643416, acc 1, learning_rate 0.000110659\n",
      "2019-01-29T02:28:04.456037: step 2300, loss 0.101589, acc 0.984375, learning_rate 0.000110631\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:28:04.483721: step 2300, loss 0.630527, acc 0.757036\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2300\n",
      "\n",
      "2019-01-29T02:28:04.776183: step 2301, loss 0.0806102, acc 1, learning_rate 0.000110603\n",
      "2019-01-29T02:28:04.902388: step 2302, loss 0.0709127, acc 1, learning_rate 0.000110574\n",
      "2019-01-29T02:28:04.991721: step 2303, loss 0.0876688, acc 1, learning_rate 0.000110546\n",
      "2019-01-29T02:28:05.075870: step 2304, loss 0.0924176, acc 1, learning_rate 0.000110518\n",
      "2019-01-29T02:28:05.194093: step 2305, loss 0.0855608, acc 0.984375, learning_rate 0.00011049\n",
      "2019-01-29T02:28:05.323181: step 2306, loss 0.097835, acc 0.984375, learning_rate 0.000110462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:28:05.452801: step 2307, loss 0.0897285, acc 1, learning_rate 0.000110434\n",
      "2019-01-29T02:28:05.578878: step 2308, loss 0.0757548, acc 1, learning_rate 0.000110406\n",
      "2019-01-29T02:28:05.706768: step 2309, loss 0.0859806, acc 1, learning_rate 0.000110379\n",
      "2019-01-29T02:28:05.834081: step 2310, loss 0.076062, acc 1, learning_rate 0.000110351\n",
      "2019-01-29T02:28:05.964631: step 2311, loss 0.0778834, acc 1, learning_rate 0.000110323\n",
      "2019-01-29T02:28:06.088966: step 2312, loss 0.0844795, acc 0.984375, learning_rate 0.000110296\n",
      "2019-01-29T02:28:06.209767: step 2313, loss 0.0812052, acc 1, learning_rate 0.000110269\n",
      "2019-01-29T02:28:06.339750: step 2314, loss 0.092939, acc 1, learning_rate 0.000110241\n",
      "2019-01-29T02:28:06.467338: step 2315, loss 0.0794834, acc 1, learning_rate 0.000110214\n",
      "2019-01-29T02:28:06.557204: step 2316, loss 0.0719286, acc 1, learning_rate 0.000110187\n",
      "2019-01-29T02:28:06.680692: step 2317, loss 0.121791, acc 0.96875, learning_rate 0.00011016\n",
      "2019-01-29T02:28:06.811525: step 2318, loss 0.072628, acc 1, learning_rate 0.000110133\n",
      "2019-01-29T02:28:06.940937: step 2319, loss 0.0996106, acc 1, learning_rate 0.000110106\n",
      "2019-01-29T02:28:07.057064: step 2320, loss 0.0786355, acc 1, learning_rate 0.000110079\n",
      "2019-01-29T02:28:07.182735: step 2321, loss 0.103798, acc 1, learning_rate 0.000110052\n",
      "2019-01-29T02:28:07.309624: step 2322, loss 0.0805121, acc 1, learning_rate 0.000110025\n",
      "2019-01-29T02:28:07.399222: step 2323, loss 0.0722884, acc 1, learning_rate 0.000109998\n",
      "2019-01-29T02:28:07.525988: step 2324, loss 0.091486, acc 1, learning_rate 0.000109972\n",
      "2019-01-29T02:28:07.613657: step 2325, loss 0.070997, acc 1, learning_rate 0.000109945\n",
      "2019-01-29T02:28:07.740189: step 2326, loss 0.0710307, acc 1, learning_rate 0.000109919\n",
      "2019-01-29T02:28:07.828331: step 2327, loss 0.068061, acc 1, learning_rate 0.000109892\n",
      "2019-01-29T02:28:07.954246: step 2328, loss 0.0704158, acc 1, learning_rate 0.000109866\n",
      "2019-01-29T02:28:08.047355: step 2329, loss 0.0794929, acc 1, learning_rate 0.000109839\n",
      "2019-01-29T02:28:08.137798: step 2330, loss 0.0915834, acc 1, learning_rate 0.000109813\n",
      "2019-01-29T02:28:08.256939: step 2331, loss 0.0894048, acc 1, learning_rate 0.000109787\n",
      "2019-01-29T02:28:08.347681: step 2332, loss 0.0812784, acc 1, learning_rate 0.000109761\n",
      "2019-01-29T02:28:08.479343: step 2333, loss 0.101495, acc 0.984375, learning_rate 0.000109735\n",
      "2019-01-29T02:28:08.603434: step 2334, loss 0.0913419, acc 0.984375, learning_rate 0.000109709\n",
      "2019-01-29T02:28:08.721323: step 2335, loss 0.0781684, acc 1, learning_rate 0.000109683\n",
      "2019-01-29T02:28:08.852406: step 2336, loss 0.0683163, acc 1, learning_rate 0.000109657\n",
      "2019-01-29T02:28:08.932347: step 2337, loss 0.0850332, acc 1, learning_rate 0.000109632\n",
      "2019-01-29T02:28:09.053570: step 2338, loss 0.114161, acc 1, learning_rate 0.000109606\n",
      "2019-01-29T02:28:09.183908: step 2339, loss 0.0911998, acc 0.984375, learning_rate 0.00010958\n",
      "2019-01-29T02:28:09.273333: step 2340, loss 0.0850148, acc 0.984375, learning_rate 0.000109555\n",
      "2019-01-29T02:28:09.404838: step 2341, loss 0.0718093, acc 1, learning_rate 0.000109529\n",
      "2019-01-29T02:28:09.493267: step 2342, loss 0.07324, acc 1, learning_rate 0.000109504\n",
      "2019-01-29T02:28:09.617681: step 2343, loss 0.0876932, acc 1, learning_rate 0.000109479\n",
      "2019-01-29T02:28:09.707421: step 2344, loss 0.0958926, acc 0.984375, learning_rate 0.000109454\n",
      "2019-01-29T02:28:09.834526: step 2345, loss 0.0611917, acc 1, learning_rate 0.000109428\n",
      "2019-01-29T02:28:09.958285: step 2346, loss 0.0744921, acc 1, learning_rate 0.000109403\n",
      "2019-01-29T02:28:10.075243: step 2347, loss 0.109625, acc 1, learning_rate 0.000109378\n",
      "2019-01-29T02:28:10.200988: step 2348, loss 0.0779793, acc 1, learning_rate 0.000109353\n",
      "2019-01-29T02:28:10.330295: step 2349, loss 0.111731, acc 0.96875, learning_rate 0.000109328\n",
      "2019-01-29T02:28:10.419312: step 2350, loss 0.0725466, acc 1, learning_rate 0.000109303\n",
      "2019-01-29T02:28:10.550208: step 2351, loss 0.0933298, acc 1, learning_rate 0.000109279\n",
      "2019-01-29T02:28:10.674989: step 2352, loss 0.0768752, acc 1, learning_rate 0.000109254\n",
      "2019-01-29T02:28:10.798257: step 2353, loss 0.0733406, acc 1, learning_rate 0.000109229\n",
      "2019-01-29T02:28:10.917985: step 2354, loss 0.0955209, acc 1, learning_rate 0.000109205\n",
      "2019-01-29T02:28:11.002926: step 2355, loss 0.077776, acc 1, learning_rate 0.00010918\n",
      "2019-01-29T02:28:11.122635: step 2356, loss 0.0707513, acc 1, learning_rate 0.000109156\n",
      "2019-01-29T02:28:11.247504: step 2357, loss 0.100999, acc 0.984375, learning_rate 0.000109131\n",
      "2019-01-29T02:28:11.378552: step 2358, loss 0.0899485, acc 1, learning_rate 0.000109107\n",
      "2019-01-29T02:28:11.505115: step 2359, loss 0.0709022, acc 1, learning_rate 0.000109083\n",
      "2019-01-29T02:28:11.629333: step 2360, loss 0.0679349, acc 1, learning_rate 0.000109058\n",
      "2019-01-29T02:28:11.755332: step 2361, loss 0.073072, acc 1, learning_rate 0.000109034\n",
      "2019-01-29T02:28:11.886342: step 2362, loss 0.0820055, acc 1, learning_rate 0.00010901\n",
      "2019-01-29T02:28:12.014838: step 2363, loss 0.106699, acc 0.984375, learning_rate 0.000108986\n",
      "2019-01-29T02:28:12.135174: step 2364, loss 0.0998206, acc 0.984375, learning_rate 0.000108962\n",
      "2019-01-29T02:28:12.258376: step 2365, loss 0.109226, acc 0.984375, learning_rate 0.000108938\n",
      "2019-01-29T02:28:12.391155: step 2366, loss 0.0903876, acc 1, learning_rate 0.000108915\n",
      "2019-01-29T02:28:12.512635: step 2367, loss 0.0801261, acc 0.984375, learning_rate 0.000108891\n",
      "2019-01-29T02:28:12.639878: step 2368, loss 0.0733744, acc 1, learning_rate 0.000108867\n",
      "2019-01-29T02:28:12.763643: step 2369, loss 0.0939914, acc 0.984375, learning_rate 0.000108844\n",
      "2019-01-29T02:28:12.882140: step 2370, loss 0.0879984, acc 1, learning_rate 0.00010882\n",
      "2019-01-29T02:28:13.012632: step 2371, loss 0.11051, acc 0.984375, learning_rate 0.000108797\n",
      "2019-01-29T02:28:13.140887: step 2372, loss 0.0962502, acc 1, learning_rate 0.000108773\n",
      "2019-01-29T02:28:13.263050: step 2373, loss 0.0846784, acc 1, learning_rate 0.00010875\n",
      "2019-01-29T02:28:13.387266: step 2374, loss 0.0898028, acc 0.984375, learning_rate 0.000108726\n",
      "2019-01-29T02:28:13.514732: step 2375, loss 0.0926602, acc 1, learning_rate 0.000108703\n",
      "2019-01-29T02:28:13.640396: step 2376, loss 0.0889181, acc 0.984375, learning_rate 0.00010868\n",
      "2019-01-29T02:28:13.728204: step 2377, loss 0.0952084, acc 1, learning_rate 0.000108657\n",
      "2019-01-29T02:28:13.830887: step 2378, loss 0.0819393, acc 0.984375, learning_rate 0.000108634\n",
      "2019-01-29T02:28:13.961704: step 2379, loss 0.0786803, acc 1, learning_rate 0.000108611\n",
      "2019-01-29T02:28:14.087984: step 2380, loss 0.0972958, acc 1, learning_rate 0.000108588\n",
      "2019-01-29T02:28:14.213320: step 2381, loss 0.0836433, acc 0.984375, learning_rate 0.000108565\n",
      "2019-01-29T02:28:14.303269: step 2382, loss 0.0639773, acc 1, learning_rate 0.000108542\n",
      "2019-01-29T02:28:14.429711: step 2383, loss 0.090594, acc 1, learning_rate 0.000108519\n",
      "2019-01-29T02:28:14.559982: step 2384, loss 0.0665244, acc 1, learning_rate 0.000108497\n",
      "2019-01-29T02:28:14.648427: step 2385, loss 0.0777704, acc 1, learning_rate 0.000108474\n",
      "2019-01-29T02:28:14.746710: step 2386, loss 0.0887167, acc 0.984375, learning_rate 0.000108451\n",
      "2019-01-29T02:28:14.871516: step 2387, loss 0.0647369, acc 1, learning_rate 0.000108429\n",
      "2019-01-29T02:28:14.999287: step 2388, loss 0.0704078, acc 1, learning_rate 0.000108406\n",
      "2019-01-29T02:28:15.124944: step 2389, loss 0.112163, acc 0.984375, learning_rate 0.000108384\n",
      "2019-01-29T02:28:15.215848: step 2390, loss 0.0760928, acc 1, learning_rate 0.000108362\n",
      "2019-01-29T02:28:15.305457: step 2391, loss 0.067568, acc 1, learning_rate 0.000108339\n",
      "2019-01-29T02:28:15.429362: step 2392, loss 0.0801259, acc 1, learning_rate 0.000108317\n",
      "2019-01-29T02:28:15.519383: step 2393, loss 0.0778947, acc 1, learning_rate 0.000108295\n",
      "2019-01-29T02:28:15.644328: step 2394, loss 0.0778867, acc 1, learning_rate 0.000108273\n",
      "2019-01-29T02:28:15.730277: step 2395, loss 0.0930115, acc 1, learning_rate 0.000108251\n",
      "2019-01-29T02:28:15.851600: step 2396, loss 0.0986316, acc 0.96875, learning_rate 0.000108229\n",
      "2019-01-29T02:28:15.981434: step 2397, loss 0.0935177, acc 0.984375, learning_rate 0.000108207\n",
      "2019-01-29T02:28:16.101832: step 2398, loss 0.064935, acc 1, learning_rate 0.000108185\n",
      "2019-01-29T02:28:16.224650: step 2399, loss 0.0850005, acc 0.984375, learning_rate 0.000108163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:28:16.348727: step 2400, loss 0.0850102, acc 1, learning_rate 0.000108142\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:28:16.375680: step 2400, loss 0.631946, acc 0.758912\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2400\n",
      "\n",
      "2019-01-29T02:28:16.767701: step 2401, loss 0.0729882, acc 1, learning_rate 0.00010812\n",
      "2019-01-29T02:28:16.857509: step 2402, loss 0.0811093, acc 1, learning_rate 0.000108098\n",
      "2019-01-29T02:28:16.988322: step 2403, loss 0.108323, acc 0.96875, learning_rate 0.000108077\n",
      "2019-01-29T02:28:17.079780: step 2404, loss 0.0772988, acc 1, learning_rate 0.000108055\n",
      "2019-01-29T02:28:17.207920: step 2405, loss 0.0882789, acc 1, learning_rate 0.000108034\n",
      "2019-01-29T02:28:17.339941: step 2406, loss 0.0834611, acc 1, learning_rate 0.000108012\n",
      "2019-01-29T02:28:17.430392: step 2407, loss 0.0733166, acc 0.984375, learning_rate 0.000107991\n",
      "2019-01-29T02:28:17.555444: step 2408, loss 0.0781501, acc 1, learning_rate 0.00010797\n",
      "2019-01-29T02:28:17.679986: step 2409, loss 0.0728391, acc 1, learning_rate 0.000107948\n",
      "2019-01-29T02:28:17.801451: step 2410, loss 0.0863802, acc 1, learning_rate 0.000107927\n",
      "2019-01-29T02:28:17.925161: step 2411, loss 0.108083, acc 1, learning_rate 0.000107906\n",
      "2019-01-29T02:28:18.052971: step 2412, loss 0.0746006, acc 1, learning_rate 0.000107885\n",
      "2019-01-29T02:28:18.186126: step 2413, loss 0.0759216, acc 1, learning_rate 0.000107864\n",
      "2019-01-29T02:28:18.309718: step 2414, loss 0.127017, acc 0.96875, learning_rate 0.000107843\n",
      "2019-01-29T02:28:18.437993: step 2415, loss 0.0805415, acc 1, learning_rate 0.000107822\n",
      "2019-01-29T02:28:18.567526: step 2416, loss 0.0733575, acc 1, learning_rate 0.000107801\n",
      "2019-01-29T02:28:18.691069: step 2417, loss 0.0670861, acc 1, learning_rate 0.000107781\n",
      "2019-01-29T02:28:18.810830: step 2418, loss 0.0924486, acc 1, learning_rate 0.00010776\n",
      "2019-01-29T02:28:18.939172: step 2419, loss 0.0833153, acc 1, learning_rate 0.000107739\n",
      "2019-01-29T02:28:19.028568: step 2420, loss 0.101446, acc 0.984375, learning_rate 0.000107719\n",
      "2019-01-29T02:28:19.152379: step 2421, loss 0.0813741, acc 1, learning_rate 0.000107698\n",
      "2019-01-29T02:28:19.239538: step 2422, loss 0.0764366, acc 1, learning_rate 0.000107678\n",
      "2019-01-29T02:28:19.371559: step 2423, loss 0.0839465, acc 1, learning_rate 0.000107657\n",
      "2019-01-29T02:28:19.503167: step 2424, loss 0.0965734, acc 0.96875, learning_rate 0.000107637\n",
      "2019-01-29T02:28:19.626652: step 2425, loss 0.0739241, acc 1, learning_rate 0.000107616\n",
      "2019-01-29T02:28:19.753182: step 2426, loss 0.0799198, acc 1, learning_rate 0.000107596\n",
      "2019-01-29T02:28:19.880642: step 2427, loss 0.0965207, acc 0.984375, learning_rate 0.000107576\n",
      "2019-01-29T02:28:19.971722: step 2428, loss 0.0624956, acc 1, learning_rate 0.000107556\n",
      "2019-01-29T02:28:20.068559: step 2429, loss 0.0982226, acc 1, learning_rate 0.000107535\n",
      "2019-01-29T02:28:20.193171: step 2430, loss 0.0697575, acc 1, learning_rate 0.000107515\n",
      "2019-01-29T02:28:20.276458: step 2431, loss 0.0846279, acc 1, learning_rate 0.000107495\n",
      "2019-01-29T02:28:20.368677: step 2432, loss 0.0627426, acc 1, learning_rate 0.000107475\n",
      "2019-01-29T02:28:20.455289: step 2433, loss 0.0737361, acc 1, learning_rate 0.000107456\n",
      "2019-01-29T02:28:20.581955: step 2434, loss 0.0815589, acc 0.984375, learning_rate 0.000107436\n",
      "2019-01-29T02:28:20.713576: step 2435, loss 0.0808077, acc 1, learning_rate 0.000107416\n",
      "2019-01-29T02:28:20.841615: step 2436, loss 0.0581865, acc 1, learning_rate 0.000107396\n",
      "2019-01-29T02:28:20.943734: step 2437, loss 0.0859317, acc 0.984375, learning_rate 0.000107376\n",
      "2019-01-29T02:28:21.068565: step 2438, loss 0.132639, acc 0.984375, learning_rate 0.000107357\n",
      "2019-01-29T02:28:21.195229: step 2439, loss 0.0910706, acc 0.984375, learning_rate 0.000107337\n",
      "2019-01-29T02:28:21.316568: step 2440, loss 0.0668638, acc 1, learning_rate 0.000107318\n",
      "2019-01-29T02:28:21.442275: step 2441, loss 0.0722348, acc 1, learning_rate 0.000107298\n",
      "2019-01-29T02:28:21.568611: step 2442, loss 0.0786785, acc 1, learning_rate 0.000107279\n",
      "2019-01-29T02:28:21.692995: step 2443, loss 0.0654175, acc 1, learning_rate 0.000107259\n",
      "2019-01-29T02:28:21.824151: step 2444, loss 0.111462, acc 0.984375, learning_rate 0.00010724\n",
      "2019-01-29T02:28:21.909613: step 2445, loss 0.0806093, acc 1, learning_rate 0.000107221\n",
      "2019-01-29T02:28:22.033857: step 2446, loss 0.103387, acc 1, learning_rate 0.000107201\n",
      "2019-01-29T02:28:22.159900: step 2447, loss 0.0778002, acc 1, learning_rate 0.000107182\n",
      "2019-01-29T02:28:22.284268: step 2448, loss 0.0912914, acc 0.984375, learning_rate 0.000107163\n",
      "2019-01-29T02:28:22.370989: step 2449, loss 0.0858733, acc 1, learning_rate 0.000107144\n",
      "2019-01-29T02:28:22.460896: step 2450, loss 0.0955666, acc 1, learning_rate 0.000107125\n",
      "2019-01-29T02:28:22.590541: step 2451, loss 0.0861873, acc 1, learning_rate 0.000107106\n",
      "2019-01-29T02:28:22.712120: step 2452, loss 0.0775569, acc 1, learning_rate 0.000107087\n",
      "2019-01-29T02:28:22.838849: step 2453, loss 0.078278, acc 1, learning_rate 0.000107068\n",
      "2019-01-29T02:28:22.967303: step 2454, loss 0.0653656, acc 1, learning_rate 0.000107049\n",
      "2019-01-29T02:28:23.092367: step 2455, loss 0.125301, acc 0.984375, learning_rate 0.000107031\n",
      "2019-01-29T02:28:23.184865: step 2456, loss 0.0807418, acc 1, learning_rate 0.000107012\n",
      "2019-01-29T02:28:23.306561: step 2457, loss 0.0982932, acc 0.984375, learning_rate 0.000106993\n",
      "2019-01-29T02:28:23.437797: step 2458, loss 0.0678562, acc 1, learning_rate 0.000106974\n",
      "2019-01-29T02:28:23.568376: step 2459, loss 0.0709805, acc 1, learning_rate 0.000106956\n",
      "2019-01-29T02:28:23.696173: step 2460, loss 0.0738482, acc 1, learning_rate 0.000106937\n",
      "2019-01-29T02:28:23.820261: step 2461, loss 0.0701472, acc 1, learning_rate 0.000106919\n",
      "2019-01-29T02:28:23.910013: step 2462, loss 0.0842394, acc 0.984375, learning_rate 0.0001069\n",
      "2019-01-29T02:28:24.029862: step 2463, loss 0.0848873, acc 1, learning_rate 0.000106882\n",
      "2019-01-29T02:28:24.152462: step 2464, loss 0.068531, acc 1, learning_rate 0.000106864\n",
      "2019-01-29T02:28:24.281846: step 2465, loss 0.0896952, acc 1, learning_rate 0.000106845\n",
      "2019-01-29T02:28:24.410927: step 2466, loss 0.0745733, acc 1, learning_rate 0.000106827\n",
      "2019-01-29T02:28:24.532739: step 2467, loss 0.107382, acc 0.984375, learning_rate 0.000106809\n",
      "2019-01-29T02:28:24.654721: step 2468, loss 0.1092, acc 1, learning_rate 0.000106791\n",
      "2019-01-29T02:28:24.776193: step 2469, loss 0.0793051, acc 1, learning_rate 0.000106773\n",
      "2019-01-29T02:28:24.899826: step 2470, loss 0.0775361, acc 1, learning_rate 0.000106755\n",
      "2019-01-29T02:28:25.022220: step 2471, loss 0.106751, acc 0.984375, learning_rate 0.000106737\n",
      "2019-01-29T02:28:25.139306: step 2472, loss 0.0847307, acc 0.984375, learning_rate 0.000106719\n",
      "2019-01-29T02:28:25.273025: step 2473, loss 0.0912333, acc 0.984375, learning_rate 0.000106701\n",
      "2019-01-29T02:28:25.399284: step 2474, loss 0.0792598, acc 1, learning_rate 0.000106683\n",
      "2019-01-29T02:28:25.531746: step 2475, loss 0.0744958, acc 1, learning_rate 0.000106665\n",
      "2019-01-29T02:28:25.658560: step 2476, loss 0.064097, acc 1, learning_rate 0.000106647\n",
      "2019-01-29T02:28:25.789723: step 2477, loss 0.0877936, acc 1, learning_rate 0.00010663\n",
      "2019-01-29T02:28:25.908372: step 2478, loss 0.0802473, acc 1, learning_rate 0.000106612\n",
      "2019-01-29T02:28:26.036026: step 2479, loss 0.0608757, acc 1, learning_rate 0.000106595\n",
      "2019-01-29T02:28:26.157987: step 2480, loss 0.0928426, acc 1, learning_rate 0.000106577\n",
      "2019-01-29T02:28:26.249580: step 2481, loss 0.0776759, acc 1, learning_rate 0.000106559\n",
      "2019-01-29T02:28:26.370014: step 2482, loss 0.0770545, acc 1, learning_rate 0.000106542\n",
      "2019-01-29T02:28:26.459546: step 2483, loss 0.103687, acc 0.96875, learning_rate 0.000106525\n",
      "2019-01-29T02:28:26.579807: step 2484, loss 0.0797215, acc 1, learning_rate 0.000106507\n",
      "2019-01-29T02:28:26.711890: step 2485, loss 0.0752262, acc 1, learning_rate 0.00010649\n",
      "2019-01-29T02:28:26.800692: step 2486, loss 0.115511, acc 0.984375, learning_rate 0.000106472\n",
      "2019-01-29T02:28:26.919330: step 2487, loss 0.0955866, acc 0.984375, learning_rate 0.000106455\n",
      "2019-01-29T02:28:27.048694: step 2488, loss 0.0780821, acc 1, learning_rate 0.000106438\n",
      "2019-01-29T02:28:27.172763: step 2489, loss 0.0642934, acc 1, learning_rate 0.000106421\n",
      "2019-01-29T02:28:27.303084: step 2490, loss 0.0908818, acc 0.984375, learning_rate 0.000106404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:28:27.428703: step 2491, loss 0.0875366, acc 1, learning_rate 0.000106387\n",
      "2019-01-29T02:28:27.555399: step 2492, loss 0.083652, acc 1, learning_rate 0.00010637\n",
      "2019-01-29T02:28:27.643501: step 2493, loss 0.0813306, acc 1, learning_rate 0.000106353\n",
      "2019-01-29T02:28:27.770825: step 2494, loss 0.0718514, acc 1, learning_rate 0.000106336\n",
      "2019-01-29T02:28:27.857829: step 2495, loss 0.0647928, acc 1, learning_rate 0.000106319\n",
      "2019-01-29T02:28:27.988130: step 2496, loss 0.121736, acc 0.96875, learning_rate 0.000106302\n",
      "2019-01-29T02:28:28.118524: step 2497, loss 0.0763582, acc 0.984375, learning_rate 0.000106285\n",
      "2019-01-29T02:28:28.244231: step 2498, loss 0.11111, acc 0.96875, learning_rate 0.000106269\n",
      "2019-01-29T02:28:28.361900: step 2499, loss 0.0840388, acc 1, learning_rate 0.000106252\n",
      "2019-01-29T02:28:28.490445: step 2500, loss 0.0887489, acc 0.984375, learning_rate 0.000106235\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:28:28.517526: step 2500, loss 0.636315, acc 0.757974\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2500\n",
      "\n",
      "2019-01-29T02:28:28.847511: step 2501, loss 0.0793904, acc 1, learning_rate 0.000106219\n",
      "2019-01-29T02:28:28.976739: step 2502, loss 0.0693219, acc 1, learning_rate 0.000106202\n",
      "2019-01-29T02:28:29.096486: step 2503, loss 0.0879257, acc 0.984375, learning_rate 0.000106186\n",
      "2019-01-29T02:28:29.218546: step 2504, loss 0.0860694, acc 0.984375, learning_rate 0.000106169\n",
      "2019-01-29T02:28:29.350901: step 2505, loss 0.0819433, acc 1, learning_rate 0.000106153\n",
      "2019-01-29T02:28:29.473654: step 2506, loss 0.0804867, acc 1, learning_rate 0.000106136\n",
      "2019-01-29T02:28:29.603118: step 2507, loss 0.068129, acc 1, learning_rate 0.00010612\n",
      "2019-01-29T02:28:29.729391: step 2508, loss 0.0968345, acc 0.984375, learning_rate 0.000106104\n",
      "2019-01-29T02:28:29.815016: step 2509, loss 0.0858434, acc 1, learning_rate 0.000106087\n",
      "2019-01-29T02:28:29.931033: step 2510, loss 0.0852854, acc 1, learning_rate 0.000106071\n",
      "2019-01-29T02:28:30.016275: step 2511, loss 0.0776985, acc 1, learning_rate 0.000106055\n",
      "2019-01-29T02:28:30.107349: step 2512, loss 0.0696862, acc 1, learning_rate 0.000106039\n",
      "2019-01-29T02:28:30.197444: step 2513, loss 0.0835825, acc 1, learning_rate 0.000106023\n",
      "2019-01-29T02:28:30.323552: step 2514, loss 0.0946718, acc 0.984375, learning_rate 0.000106007\n",
      "2019-01-29T02:28:30.446725: step 2515, loss 0.0805206, acc 1, learning_rate 0.000105991\n",
      "2019-01-29T02:28:30.567443: step 2516, loss 0.0741575, acc 1, learning_rate 0.000105975\n",
      "2019-01-29T02:28:30.686038: step 2517, loss 0.112442, acc 0.984375, learning_rate 0.000105959\n",
      "2019-01-29T02:28:30.809821: step 2518, loss 0.0859485, acc 1, learning_rate 0.000105943\n",
      "2019-01-29T02:28:30.933688: step 2519, loss 0.0910572, acc 0.984375, learning_rate 0.000105927\n",
      "2019-01-29T02:28:31.062878: step 2520, loss 0.0821631, acc 1, learning_rate 0.000105911\n",
      "2019-01-29T02:28:31.189574: step 2521, loss 0.0796875, acc 0.984375, learning_rate 0.000105895\n",
      "2019-01-29T02:28:31.319929: step 2522, loss 0.0903463, acc 0.984375, learning_rate 0.00010588\n",
      "2019-01-29T02:28:31.405585: step 2523, loss 0.0954735, acc 0.984375, learning_rate 0.000105864\n",
      "2019-01-29T02:28:31.528516: step 2524, loss 0.0951626, acc 0.984375, learning_rate 0.000105849\n",
      "2019-01-29T02:28:31.653362: step 2525, loss 0.0823217, acc 1, learning_rate 0.000105833\n",
      "2019-01-29T02:28:31.780269: step 2526, loss 0.0949606, acc 1, learning_rate 0.000105817\n",
      "2019-01-29T02:28:31.908052: step 2527, loss 0.101074, acc 0.984375, learning_rate 0.000105802\n",
      "2019-01-29T02:28:32.038941: step 2528, loss 0.0696159, acc 1, learning_rate 0.000105786\n",
      "2019-01-29T02:28:32.161197: step 2529, loss 0.0680307, acc 1, learning_rate 0.000105771\n",
      "2019-01-29T02:28:32.250955: step 2530, loss 0.10416, acc 0.984375, learning_rate 0.000105756\n",
      "2019-01-29T02:28:32.374512: step 2531, loss 0.100444, acc 0.96875, learning_rate 0.00010574\n",
      "2019-01-29T02:28:32.500548: step 2532, loss 0.0665121, acc 1, learning_rate 0.000105725\n",
      "2019-01-29T02:28:32.628472: step 2533, loss 0.102566, acc 0.984375, learning_rate 0.00010571\n",
      "2019-01-29T02:28:32.751288: step 2534, loss 0.0798049, acc 1, learning_rate 0.000105695\n",
      "2019-01-29T02:28:32.873671: step 2535, loss 0.0846009, acc 1, learning_rate 0.000105679\n",
      "2019-01-29T02:28:32.995178: step 2536, loss 0.0640052, acc 1, learning_rate 0.000105664\n",
      "2019-01-29T02:28:33.077626: step 2537, loss 0.0824252, acc 1, learning_rate 0.000105649\n",
      "2019-01-29T02:28:33.160551: step 2538, loss 0.0836132, acc 1, learning_rate 0.000105634\n",
      "2019-01-29T02:28:33.284944: step 2539, loss 0.0826566, acc 1, learning_rate 0.000105619\n",
      "2019-01-29T02:28:33.376514: step 2540, loss 0.104054, acc 1, learning_rate 0.000105604\n",
      "2019-01-29T02:28:33.464364: step 2541, loss 0.0868943, acc 1, learning_rate 0.000105589\n",
      "2019-01-29T02:28:33.586710: step 2542, loss 0.0810844, acc 1, learning_rate 0.000105574\n",
      "2019-01-29T02:28:33.712208: step 2543, loss 0.058373, acc 1, learning_rate 0.000105559\n",
      "2019-01-29T02:28:33.835377: step 2544, loss 0.0788247, acc 1, learning_rate 0.000105545\n",
      "2019-01-29T02:28:33.926732: step 2545, loss 0.0718699, acc 1, learning_rate 0.00010553\n",
      "2019-01-29T02:28:34.051505: step 2546, loss 0.0804865, acc 1, learning_rate 0.000105515\n",
      "2019-01-29T02:28:34.141990: step 2547, loss 0.0786671, acc 1, learning_rate 0.0001055\n",
      "2019-01-29T02:28:34.267841: step 2548, loss 0.0807928, acc 1, learning_rate 0.000105486\n",
      "2019-01-29T02:28:34.395346: step 2549, loss 0.127871, acc 0.96875, learning_rate 0.000105471\n",
      "2019-01-29T02:28:34.513957: step 2550, loss 0.0796811, acc 1, learning_rate 0.000105457\n",
      "2019-01-29T02:28:34.637454: step 2551, loss 0.0718931, acc 1, learning_rate 0.000105442\n",
      "2019-01-29T02:28:34.771047: step 2552, loss 0.135404, acc 0.96875, learning_rate 0.000105428\n",
      "2019-01-29T02:28:34.892078: step 2553, loss 0.0805365, acc 1, learning_rate 0.000105413\n",
      "2019-01-29T02:28:35.020808: step 2554, loss 0.0818455, acc 0.984375, learning_rate 0.000105399\n",
      "2019-01-29T02:28:35.102599: step 2555, loss 0.0679302, acc 1, learning_rate 0.000105384\n",
      "2019-01-29T02:28:35.224478: step 2556, loss 0.0679553, acc 1, learning_rate 0.00010537\n",
      "2019-01-29T02:28:35.311966: step 2557, loss 0.0981428, acc 0.984375, learning_rate 0.000105356\n",
      "2019-01-29T02:28:35.438831: step 2558, loss 0.0978576, acc 0.984375, learning_rate 0.000105341\n",
      "2019-01-29T02:28:35.529885: step 2559, loss 0.098626, acc 0.984375, learning_rate 0.000105327\n",
      "2019-01-29T02:28:35.656813: step 2560, loss 0.0680667, acc 1, learning_rate 0.000105313\n",
      "2019-01-29T02:28:35.784368: step 2561, loss 0.101513, acc 0.984375, learning_rate 0.000105299\n",
      "2019-01-29T02:28:35.907589: step 2562, loss 0.0667023, acc 1, learning_rate 0.000105285\n",
      "2019-01-29T02:28:36.035877: step 2563, loss 0.0756826, acc 1, learning_rate 0.000105271\n",
      "2019-01-29T02:28:36.155492: step 2564, loss 0.066416, acc 1, learning_rate 0.000105257\n",
      "2019-01-29T02:28:36.284132: step 2565, loss 0.0653404, acc 1, learning_rate 0.000105243\n",
      "2019-01-29T02:28:36.414308: step 2566, loss 0.0744554, acc 1, learning_rate 0.000105229\n",
      "2019-01-29T02:28:36.533444: step 2567, loss 0.0983408, acc 0.984375, learning_rate 0.000105215\n",
      "2019-01-29T02:28:36.619162: step 2568, loss 0.0667701, acc 1, learning_rate 0.000105201\n",
      "2019-01-29T02:28:36.744459: step 2569, loss 0.0821343, acc 0.984375, learning_rate 0.000105187\n",
      "2019-01-29T02:28:36.869742: step 2570, loss 0.0741521, acc 1, learning_rate 0.000105173\n",
      "2019-01-29T02:28:36.999907: step 2571, loss 0.0659111, acc 1, learning_rate 0.000105159\n",
      "2019-01-29T02:28:37.123179: step 2572, loss 0.0986635, acc 0.984375, learning_rate 0.000105146\n",
      "2019-01-29T02:28:37.245458: step 2573, loss 0.0830868, acc 1, learning_rate 0.000105132\n",
      "2019-01-29T02:28:37.372762: step 2574, loss 0.0748215, acc 1, learning_rate 0.000105118\n",
      "2019-01-29T02:28:37.498748: step 2575, loss 0.0726735, acc 1, learning_rate 0.000105105\n",
      "2019-01-29T02:28:37.601051: step 2576, loss 0.0941898, acc 0.984375, learning_rate 0.000105091\n",
      "2019-01-29T02:28:37.727233: step 2577, loss 0.0741502, acc 1, learning_rate 0.000105077\n",
      "2019-01-29T02:28:37.854135: step 2578, loss 0.06368, acc 1, learning_rate 0.000105064\n",
      "2019-01-29T02:28:37.936444: step 2579, loss 0.122239, acc 0.96875, learning_rate 0.00010505\n",
      "2019-01-29T02:28:38.056610: step 2580, loss 0.0740661, acc 1, learning_rate 0.000105037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:28:38.189111: step 2581, loss 0.109557, acc 0.96875, learning_rate 0.000105023\n",
      "2019-01-29T02:28:38.311766: step 2582, loss 0.0824494, acc 1, learning_rate 0.00010501\n",
      "2019-01-29T02:28:38.429916: step 2583, loss 0.0873848, acc 0.984375, learning_rate 0.000104997\n",
      "2019-01-29T02:28:38.552900: step 2584, loss 0.0828905, acc 1, learning_rate 0.000104983\n",
      "2019-01-29T02:28:38.667605: step 2585, loss 0.0874401, acc 1, learning_rate 0.00010497\n",
      "2019-01-29T02:28:38.756407: step 2586, loss 0.0956008, acc 0.984375, learning_rate 0.000104957\n",
      "2019-01-29T02:28:38.886869: step 2587, loss 0.0630944, acc 1, learning_rate 0.000104944\n",
      "2019-01-29T02:28:39.019477: step 2588, loss 0.103995, acc 0.984375, learning_rate 0.000104931\n",
      "2019-01-29T02:28:39.145746: step 2589, loss 0.0743559, acc 1, learning_rate 0.000104917\n",
      "2019-01-29T02:28:39.271055: step 2590, loss 0.0803494, acc 1, learning_rate 0.000104904\n",
      "2019-01-29T02:28:39.401265: step 2591, loss 0.11566, acc 0.984375, learning_rate 0.000104891\n",
      "2019-01-29T02:28:39.524798: step 2592, loss 0.0895466, acc 1, learning_rate 0.000104878\n",
      "2019-01-29T02:28:39.649107: step 2593, loss 0.074881, acc 1, learning_rate 0.000104865\n",
      "2019-01-29T02:28:39.772882: step 2594, loss 0.0911716, acc 1, learning_rate 0.000104852\n",
      "2019-01-29T02:28:39.903559: step 2595, loss 0.0771456, acc 1, learning_rate 0.000104839\n",
      "2019-01-29T02:28:40.030093: step 2596, loss 0.0618106, acc 1, learning_rate 0.000104826\n",
      "2019-01-29T02:28:40.156241: step 2597, loss 0.0894426, acc 1, learning_rate 0.000104814\n",
      "2019-01-29T02:28:40.285316: step 2598, loss 0.0825508, acc 1, learning_rate 0.000104801\n",
      "2019-01-29T02:28:40.416059: step 2599, loss 0.0638434, acc 1, learning_rate 0.000104788\n",
      "2019-01-29T02:28:40.542914: step 2600, loss 0.0920166, acc 1, learning_rate 0.000104775\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:28:40.569197: step 2600, loss 0.638559, acc 0.755159\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2600\n",
      "\n",
      "2019-01-29T02:28:40.914777: step 2601, loss 0.0704657, acc 1, learning_rate 0.000104762\n",
      "2019-01-29T02:28:41.005467: step 2602, loss 0.0831785, acc 1, learning_rate 0.00010475\n",
      "2019-01-29T02:28:41.136516: step 2603, loss 0.0771626, acc 1, learning_rate 0.000104737\n",
      "2019-01-29T02:28:41.263096: step 2604, loss 0.0710932, acc 1, learning_rate 0.000104725\n",
      "2019-01-29T02:28:41.353032: step 2605, loss 0.0715272, acc 1, learning_rate 0.000104712\n",
      "2019-01-29T02:28:41.478430: step 2606, loss 0.114232, acc 0.984375, learning_rate 0.000104699\n",
      "2019-01-29T02:28:41.565512: step 2607, loss 0.0719769, acc 1, learning_rate 0.000104687\n",
      "2019-01-29T02:28:41.653112: step 2608, loss 0.086426, acc 1, learning_rate 0.000104674\n",
      "2019-01-29T02:28:41.778285: step 2609, loss 0.0880351, acc 1, learning_rate 0.000104662\n",
      "2019-01-29T02:28:41.891463: step 2610, loss 0.0680275, acc 1, learning_rate 0.000104649\n",
      "2019-01-29T02:28:42.020795: step 2611, loss 0.0921821, acc 1, learning_rate 0.000104637\n",
      "2019-01-29T02:28:42.144012: step 2612, loss 0.113615, acc 0.953125, learning_rate 0.000104625\n",
      "2019-01-29T02:28:42.273864: step 2613, loss 0.0892283, acc 1, learning_rate 0.000104612\n",
      "2019-01-29T02:28:42.402763: step 2614, loss 0.09128, acc 1, learning_rate 0.0001046\n",
      "2019-01-29T02:28:42.535814: step 2615, loss 0.0658531, acc 1, learning_rate 0.000104588\n",
      "2019-01-29T02:28:42.625612: step 2616, loss 0.0800585, acc 1, learning_rate 0.000104576\n",
      "2019-01-29T02:28:42.750397: step 2617, loss 0.0634717, acc 1, learning_rate 0.000104563\n",
      "2019-01-29T02:28:42.835412: step 2618, loss 0.0917262, acc 0.984375, learning_rate 0.000104551\n",
      "2019-01-29T02:28:42.963473: step 2619, loss 0.0900729, acc 0.984375, learning_rate 0.000104539\n",
      "2019-01-29T02:28:43.094197: step 2620, loss 0.0639496, acc 1, learning_rate 0.000104527\n",
      "2019-01-29T02:28:43.226019: step 2621, loss 0.0761954, acc 1, learning_rate 0.000104515\n",
      "2019-01-29T02:28:43.346701: step 2622, loss 0.0809415, acc 1, learning_rate 0.000104503\n",
      "2019-01-29T02:28:43.467636: step 2623, loss 0.0705205, acc 1, learning_rate 0.000104491\n",
      "2019-01-29T02:28:43.594769: step 2624, loss 0.0595086, acc 1, learning_rate 0.000104479\n",
      "2019-01-29T02:28:43.719186: step 2625, loss 0.090084, acc 1, learning_rate 0.000104467\n",
      "2019-01-29T02:28:43.812919: step 2626, loss 0.0718193, acc 1, learning_rate 0.000104455\n",
      "2019-01-29T02:28:43.936260: step 2627, loss 0.0761271, acc 1, learning_rate 0.000104443\n",
      "2019-01-29T02:28:44.026348: step 2628, loss 0.0748697, acc 1, learning_rate 0.000104431\n",
      "2019-01-29T02:28:44.151325: step 2629, loss 0.0687018, acc 1, learning_rate 0.00010442\n",
      "2019-01-29T02:28:44.280201: step 2630, loss 0.09853, acc 0.96875, learning_rate 0.000104408\n",
      "2019-01-29T02:28:44.412775: step 2631, loss 0.084865, acc 1, learning_rate 0.000104396\n",
      "2019-01-29T02:28:44.536007: step 2632, loss 0.0815919, acc 1, learning_rate 0.000104384\n",
      "2019-01-29T02:28:44.667108: step 2633, loss 0.0584079, acc 1, learning_rate 0.000104373\n",
      "2019-01-29T02:28:44.793873: step 2634, loss 0.109449, acc 0.96875, learning_rate 0.000104361\n",
      "2019-01-29T02:28:44.920373: step 2635, loss 0.0697621, acc 1, learning_rate 0.00010435\n",
      "2019-01-29T02:28:45.032146: step 2636, loss 0.099411, acc 0.984375, learning_rate 0.000104338\n",
      "2019-01-29T02:28:45.161331: step 2637, loss 0.0913599, acc 0.984375, learning_rate 0.000104326\n",
      "2019-01-29T02:28:45.285861: step 2638, loss 0.0730103, acc 1, learning_rate 0.000104315\n",
      "2019-01-29T02:28:45.412174: step 2639, loss 0.06811, acc 1, learning_rate 0.000104303\n",
      "2019-01-29T02:28:45.540142: step 2640, loss 0.0833338, acc 0.984375, learning_rate 0.000104292\n",
      "2019-01-29T02:28:45.670197: step 2641, loss 0.0784933, acc 1, learning_rate 0.00010428\n",
      "2019-01-29T02:28:45.796169: step 2642, loss 0.0792513, acc 1, learning_rate 0.000104269\n",
      "2019-01-29T02:28:45.922670: step 2643, loss 0.0677503, acc 1, learning_rate 0.000104258\n",
      "2019-01-29T02:28:46.050451: step 2644, loss 0.097437, acc 1, learning_rate 0.000104246\n",
      "2019-01-29T02:28:46.182742: step 2645, loss 0.0735843, acc 1, learning_rate 0.000104235\n",
      "2019-01-29T02:28:46.305988: step 2646, loss 0.0923126, acc 0.984375, learning_rate 0.000104224\n",
      "2019-01-29T02:28:46.428607: step 2647, loss 0.100868, acc 0.984375, learning_rate 0.000104212\n",
      "2019-01-29T02:28:46.556716: step 2648, loss 0.0982879, acc 1, learning_rate 0.000104201\n",
      "2019-01-29T02:28:46.686889: step 2649, loss 0.0718179, acc 1, learning_rate 0.00010419\n",
      "2019-01-29T02:28:46.775051: step 2650, loss 0.0861399, acc 1, learning_rate 0.000104179\n",
      "2019-01-29T02:28:46.865221: step 2651, loss 0.075986, acc 1, learning_rate 0.000104168\n",
      "2019-01-29T02:28:46.991286: step 2652, loss 0.0698477, acc 1, learning_rate 0.000104157\n",
      "2019-01-29T02:28:47.121509: step 2653, loss 0.108084, acc 0.984375, learning_rate 0.000104146\n",
      "2019-01-29T02:28:47.251777: step 2654, loss 0.0898206, acc 1, learning_rate 0.000104135\n",
      "2019-01-29T02:28:47.382542: step 2655, loss 0.0854618, acc 1, learning_rate 0.000104124\n",
      "2019-01-29T02:28:47.468189: step 2656, loss 0.0729874, acc 1, learning_rate 0.000104113\n",
      "2019-01-29T02:28:47.596658: step 2657, loss 0.104751, acc 0.984375, learning_rate 0.000104102\n",
      "2019-01-29T02:28:47.685962: step 2658, loss 0.0867259, acc 1, learning_rate 0.000104091\n",
      "2019-01-29T02:28:47.812160: step 2659, loss 0.0858258, acc 1, learning_rate 0.00010408\n",
      "2019-01-29T02:28:47.900639: step 2660, loss 0.07225, acc 1, learning_rate 0.000104069\n",
      "2019-01-29T02:28:47.990835: step 2661, loss 0.0781171, acc 1, learning_rate 0.000104058\n",
      "2019-01-29T02:28:48.109516: step 2662, loss 0.0849724, acc 1, learning_rate 0.000104047\n",
      "2019-01-29T02:28:48.233314: step 2663, loss 0.0747245, acc 1, learning_rate 0.000104036\n",
      "2019-01-29T02:28:48.352037: step 2664, loss 0.0871049, acc 0.984375, learning_rate 0.000104026\n",
      "2019-01-29T02:28:48.479632: step 2665, loss 0.0871285, acc 1, learning_rate 0.000104015\n",
      "2019-01-29T02:28:48.612212: step 2666, loss 0.0766366, acc 1, learning_rate 0.000104004\n",
      "2019-01-29T02:28:48.735825: step 2667, loss 0.0910128, acc 1, learning_rate 0.000103994\n",
      "2019-01-29T02:28:48.823118: step 2668, loss 0.0974776, acc 0.984375, learning_rate 0.000103983\n",
      "2019-01-29T02:28:48.905096: step 2669, loss 0.0983633, acc 0.984375, learning_rate 0.000103972\n",
      "2019-01-29T02:28:49.028771: step 2670, loss 0.0773794, acc 1, learning_rate 0.000103962\n",
      "2019-01-29T02:28:49.153780: step 2671, loss 0.0801086, acc 1, learning_rate 0.000103951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:28:49.243231: step 2672, loss 0.098416, acc 0.984375, learning_rate 0.000103941\n",
      "2019-01-29T02:28:49.370359: step 2673, loss 0.0690577, acc 1, learning_rate 0.00010393\n",
      "2019-01-29T02:28:49.499207: step 2674, loss 0.0862532, acc 1, learning_rate 0.00010392\n",
      "2019-01-29T02:28:49.630924: step 2675, loss 0.0799713, acc 1, learning_rate 0.000103909\n",
      "2019-01-29T02:28:49.753548: step 2676, loss 0.0866022, acc 0.984375, learning_rate 0.000103899\n",
      "2019-01-29T02:28:49.879323: step 2677, loss 0.0729644, acc 1, learning_rate 0.000103888\n",
      "2019-01-29T02:28:50.010712: step 2678, loss 0.0715226, acc 1, learning_rate 0.000103878\n",
      "2019-01-29T02:28:50.143910: step 2679, loss 0.128768, acc 0.96875, learning_rate 0.000103868\n",
      "2019-01-29T02:28:50.266378: step 2680, loss 0.0617934, acc 1, learning_rate 0.000103857\n",
      "2019-01-29T02:28:50.392113: step 2681, loss 0.0805689, acc 1, learning_rate 0.000103847\n",
      "2019-01-29T02:28:50.513682: step 2682, loss 0.0863358, acc 1, learning_rate 0.000103837\n",
      "2019-01-29T02:28:50.639241: step 2683, loss 0.0915003, acc 0.984375, learning_rate 0.000103827\n",
      "2019-01-29T02:28:50.773904: step 2684, loss 0.0758003, acc 1, learning_rate 0.000103817\n",
      "2019-01-29T02:28:50.906092: step 2685, loss 0.0951978, acc 0.984375, learning_rate 0.000103806\n",
      "2019-01-29T02:28:51.030200: step 2686, loss 0.0855169, acc 1, learning_rate 0.000103796\n",
      "2019-01-29T02:28:51.155745: step 2687, loss 0.0671514, acc 1, learning_rate 0.000103786\n",
      "2019-01-29T02:28:51.286258: step 2688, loss 0.0737426, acc 1, learning_rate 0.000103776\n",
      "2019-01-29T02:28:51.419159: step 2689, loss 0.112546, acc 0.984375, learning_rate 0.000103766\n",
      "2019-01-29T02:28:51.542745: step 2690, loss 0.122189, acc 0.96875, learning_rate 0.000103756\n",
      "2019-01-29T02:28:51.632597: step 2691, loss 0.0814979, acc 1, learning_rate 0.000103746\n",
      "2019-01-29T02:28:51.721155: step 2692, loss 0.0689225, acc 1, learning_rate 0.000103736\n",
      "2019-01-29T02:28:51.846135: step 2693, loss 0.0947469, acc 1, learning_rate 0.000103726\n",
      "2019-01-29T02:28:51.976380: step 2694, loss 0.0847324, acc 1, learning_rate 0.000103716\n",
      "2019-01-29T02:28:52.063919: step 2695, loss 0.0784989, acc 1, learning_rate 0.000103706\n",
      "2019-01-29T02:28:52.159414: step 2696, loss 0.0783421, acc 1, learning_rate 0.000103696\n",
      "2019-01-29T02:28:52.249088: step 2697, loss 0.0760935, acc 1, learning_rate 0.000103686\n",
      "2019-01-29T02:28:52.339881: step 2698, loss 0.080643, acc 1, learning_rate 0.000103677\n",
      "2019-01-29T02:28:52.429869: step 2699, loss 0.0885405, acc 1, learning_rate 0.000103667\n",
      "2019-01-29T02:28:52.551621: step 2700, loss 0.0810679, acc 1, learning_rate 0.000103657\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:28:52.578602: step 2700, loss 0.642835, acc 0.754221\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2700\n",
      "\n",
      "2019-01-29T02:28:52.920194: step 2701, loss 0.0880005, acc 0.984375, learning_rate 0.000103647\n",
      "2019-01-29T02:28:53.005382: step 2702, loss 0.0856132, acc 1, learning_rate 0.000103638\n",
      "2019-01-29T02:28:53.095125: step 2703, loss 0.108813, acc 0.96875, learning_rate 0.000103628\n",
      "2019-01-29T02:28:53.179435: step 2704, loss 0.0921858, acc 0.984375, learning_rate 0.000103618\n",
      "2019-01-29T02:28:53.298208: step 2705, loss 0.0865008, acc 0.984375, learning_rate 0.000103609\n",
      "2019-01-29T02:28:53.425800: step 2706, loss 0.0802213, acc 1, learning_rate 0.000103599\n",
      "2019-01-29T02:28:53.548868: step 2707, loss 0.0760081, acc 1, learning_rate 0.000103589\n",
      "2019-01-29T02:28:53.672741: step 2708, loss 0.126749, acc 0.984375, learning_rate 0.00010358\n",
      "2019-01-29T02:28:53.793955: step 2709, loss 0.0633711, acc 1, learning_rate 0.00010357\n",
      "2019-01-29T02:28:53.917152: step 2710, loss 0.0738749, acc 1, learning_rate 0.000103561\n",
      "2019-01-29T02:28:54.035118: step 2711, loss 0.0620095, acc 1, learning_rate 0.000103551\n",
      "2019-01-29T02:28:54.162692: step 2712, loss 0.0864999, acc 0.984375, learning_rate 0.000103542\n",
      "2019-01-29T02:28:54.290195: step 2713, loss 0.0806052, acc 1, learning_rate 0.000103532\n",
      "2019-01-29T02:28:54.378970: step 2714, loss 0.0810276, acc 1, learning_rate 0.000103523\n",
      "2019-01-29T02:28:54.502728: step 2715, loss 0.0786598, acc 1, learning_rate 0.000103514\n",
      "2019-01-29T02:28:54.628568: step 2716, loss 0.0820156, acc 1, learning_rate 0.000103504\n",
      "2019-01-29T02:28:54.757878: step 2717, loss 0.0687319, acc 1, learning_rate 0.000103495\n",
      "2019-01-29T02:28:54.845878: step 2718, loss 0.072863, acc 1, learning_rate 0.000103486\n",
      "2019-01-29T02:28:54.973631: step 2719, loss 0.0814441, acc 1, learning_rate 0.000103476\n",
      "2019-01-29T02:28:55.103028: step 2720, loss 0.0867714, acc 0.984375, learning_rate 0.000103467\n",
      "2019-01-29T02:28:55.222022: step 2721, loss 0.0903708, acc 0.984375, learning_rate 0.000103458\n",
      "2019-01-29T02:28:55.308797: step 2722, loss 0.0847693, acc 1, learning_rate 0.000103449\n",
      "2019-01-29T02:28:55.438037: step 2723, loss 0.0685439, acc 1, learning_rate 0.000103439\n",
      "2019-01-29T02:28:55.527563: step 2724, loss 0.0745888, acc 1, learning_rate 0.00010343\n",
      "2019-01-29T02:28:55.646571: step 2725, loss 0.0756571, acc 1, learning_rate 0.000103421\n",
      "2019-01-29T02:28:55.766394: step 2726, loss 0.084579, acc 1, learning_rate 0.000103412\n",
      "2019-01-29T02:28:55.893582: step 2727, loss 0.0651039, acc 1, learning_rate 0.000103403\n",
      "2019-01-29T02:28:56.022785: step 2728, loss 0.0709793, acc 1, learning_rate 0.000103394\n",
      "2019-01-29T02:28:56.142684: step 2729, loss 0.0745574, acc 1, learning_rate 0.000103385\n",
      "2019-01-29T02:28:56.269072: step 2730, loss 0.105702, acc 1, learning_rate 0.000103376\n",
      "2019-01-29T02:28:56.397182: step 2731, loss 0.0678225, acc 1, learning_rate 0.000103367\n",
      "2019-01-29T02:28:56.514833: step 2732, loss 0.073972, acc 1, learning_rate 0.000103358\n",
      "2019-01-29T02:28:56.642490: step 2733, loss 0.0827222, acc 1, learning_rate 0.000103349\n",
      "2019-01-29T02:28:56.766138: step 2734, loss 0.0718405, acc 1, learning_rate 0.00010334\n",
      "2019-01-29T02:28:56.892455: step 2735, loss 0.0732833, acc 1, learning_rate 0.000103331\n",
      "2019-01-29T02:28:57.019430: step 2736, loss 0.0698468, acc 1, learning_rate 0.000103322\n",
      "2019-01-29T02:28:57.140165: step 2737, loss 0.0768822, acc 1, learning_rate 0.000103313\n",
      "2019-01-29T02:28:57.258465: step 2738, loss 0.0826005, acc 1, learning_rate 0.000103304\n",
      "2019-01-29T02:28:57.378865: step 2739, loss 0.066503, acc 1, learning_rate 0.000103296\n",
      "2019-01-29T02:28:57.501807: step 2740, loss 0.0755227, acc 1, learning_rate 0.000103287\n",
      "2019-01-29T02:28:57.605908: step 2741, loss 0.0720616, acc 1, learning_rate 0.000103278\n",
      "2019-01-29T02:28:57.732302: step 2742, loss 0.0703025, acc 1, learning_rate 0.000103269\n",
      "2019-01-29T02:28:57.863579: step 2743, loss 0.0989107, acc 0.984375, learning_rate 0.000103261\n",
      "2019-01-29T02:28:57.963998: step 2744, loss 0.102225, acc 0.984375, learning_rate 0.000103252\n",
      "2019-01-29T02:28:58.050318: step 2745, loss 0.0698932, acc 1, learning_rate 0.000103243\n",
      "2019-01-29T02:28:58.174373: step 2746, loss 0.0996896, acc 0.984375, learning_rate 0.000103235\n",
      "2019-01-29T02:28:58.264507: step 2747, loss 0.0934615, acc 1, learning_rate 0.000103226\n",
      "2019-01-29T02:28:58.397014: step 2748, loss 0.0883343, acc 1, learning_rate 0.000103217\n",
      "2019-01-29T02:28:58.529254: step 2749, loss 0.0691786, acc 1, learning_rate 0.000103209\n",
      "2019-01-29T02:28:58.618348: step 2750, loss 0.109315, acc 0.984375, learning_rate 0.0001032\n",
      "2019-01-29T02:28:58.703335: step 2751, loss 0.0903769, acc 1, learning_rate 0.000103192\n",
      "2019-01-29T02:28:58.826072: step 2752, loss 0.0706238, acc 1, learning_rate 0.000103183\n",
      "2019-01-29T02:28:58.956639: step 2753, loss 0.0574877, acc 1, learning_rate 0.000103175\n",
      "2019-01-29T02:28:59.059316: step 2754, loss 0.0666261, acc 1, learning_rate 0.000103166\n",
      "2019-01-29T02:28:59.185141: step 2755, loss 0.105113, acc 0.984375, learning_rate 0.000103158\n",
      "2019-01-29T02:28:59.310776: step 2756, loss 0.0796209, acc 1, learning_rate 0.00010315\n",
      "2019-01-29T02:28:59.396986: step 2757, loss 0.0821637, acc 1, learning_rate 0.000103141\n",
      "2019-01-29T02:28:59.526018: step 2758, loss 0.0674622, acc 1, learning_rate 0.000103133\n",
      "2019-01-29T02:28:59.651828: step 2759, loss 0.0892174, acc 0.984375, learning_rate 0.000103124\n",
      "2019-01-29T02:28:59.739446: step 2760, loss 0.0713608, acc 1, learning_rate 0.000103116\n",
      "2019-01-29T02:28:59.868888: step 2761, loss 0.0760385, acc 1, learning_rate 0.000103108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:28:59.993009: step 2762, loss 0.080498, acc 1, learning_rate 0.0001031\n",
      "2019-01-29T02:29:00.122763: step 2763, loss 0.115499, acc 0.984375, learning_rate 0.000103091\n",
      "2019-01-29T02:29:00.246004: step 2764, loss 0.0653393, acc 1, learning_rate 0.000103083\n",
      "2019-01-29T02:29:00.330327: step 2765, loss 0.0809067, acc 1, learning_rate 0.000103075\n",
      "2019-01-29T02:29:00.413058: step 2766, loss 0.061602, acc 1, learning_rate 0.000103067\n",
      "2019-01-29T02:29:00.538062: step 2767, loss 0.0719351, acc 1, learning_rate 0.000103058\n",
      "2019-01-29T02:29:00.665593: step 2768, loss 0.0958774, acc 0.984375, learning_rate 0.00010305\n",
      "2019-01-29T02:29:00.791111: step 2769, loss 0.0871517, acc 1, learning_rate 0.000103042\n",
      "2019-01-29T02:29:00.919235: step 2770, loss 0.0733879, acc 1, learning_rate 0.000103034\n",
      "2019-01-29T02:29:01.047335: step 2771, loss 0.0791511, acc 1, learning_rate 0.000103026\n",
      "2019-01-29T02:29:01.173943: step 2772, loss 0.0819951, acc 1, learning_rate 0.000103018\n",
      "2019-01-29T02:29:01.297838: step 2773, loss 0.0661312, acc 1, learning_rate 0.00010301\n",
      "2019-01-29T02:29:01.417214: step 2774, loss 0.0667598, acc 1, learning_rate 0.000103002\n",
      "2019-01-29T02:29:01.545393: step 2775, loss 0.0981473, acc 1, learning_rate 0.000102994\n",
      "2019-01-29T02:29:01.634847: step 2776, loss 0.092678, acc 0.984375, learning_rate 0.000102986\n",
      "2019-01-29T02:29:01.749805: step 2777, loss 0.072551, acc 1, learning_rate 0.000102978\n",
      "2019-01-29T02:29:01.875154: step 2778, loss 0.0963673, acc 0.984375, learning_rate 0.00010297\n",
      "2019-01-29T02:29:01.994988: step 2779, loss 0.0588579, acc 1, learning_rate 0.000102962\n",
      "2019-01-29T02:29:02.122346: step 2780, loss 0.066903, acc 1, learning_rate 0.000102954\n",
      "2019-01-29T02:29:02.239233: step 2781, loss 0.0894878, acc 1, learning_rate 0.000102946\n",
      "2019-01-29T02:29:02.367682: step 2782, loss 0.0785293, acc 1, learning_rate 0.000102938\n",
      "2019-01-29T02:29:02.497739: step 2783, loss 0.0653344, acc 1, learning_rate 0.000102931\n",
      "2019-01-29T02:29:02.623795: step 2784, loss 0.0828121, acc 0.984375, learning_rate 0.000102923\n",
      "2019-01-29T02:29:02.713824: step 2785, loss 0.06741, acc 1, learning_rate 0.000102915\n",
      "2019-01-29T02:29:02.841363: step 2786, loss 0.0831544, acc 1, learning_rate 0.000102907\n",
      "2019-01-29T02:29:02.927740: step 2787, loss 0.0825913, acc 1, learning_rate 0.0001029\n",
      "2019-01-29T02:29:03.046422: step 2788, loss 0.0613857, acc 1, learning_rate 0.000102892\n",
      "2019-01-29T02:29:03.168936: step 2789, loss 0.0791121, acc 1, learning_rate 0.000102884\n",
      "2019-01-29T02:29:03.293590: step 2790, loss 0.0862093, acc 0.984375, learning_rate 0.000102876\n",
      "2019-01-29T02:29:03.416261: step 2791, loss 0.091297, acc 0.984375, learning_rate 0.000102869\n",
      "2019-01-29T02:29:03.508356: step 2792, loss 0.068893, acc 1, learning_rate 0.000102861\n",
      "2019-01-29T02:29:03.630594: step 2793, loss 0.0925796, acc 1, learning_rate 0.000102854\n",
      "2019-01-29T02:29:03.755617: step 2794, loss 0.0772725, acc 1, learning_rate 0.000102846\n",
      "2019-01-29T02:29:03.878012: step 2795, loss 0.0886838, acc 1, learning_rate 0.000102838\n",
      "2019-01-29T02:29:04.001227: step 2796, loss 0.063341, acc 1, learning_rate 0.000102831\n",
      "2019-01-29T02:29:04.127504: step 2797, loss 0.0939636, acc 1, learning_rate 0.000102823\n",
      "2019-01-29T02:29:04.253037: step 2798, loss 0.0693165, acc 1, learning_rate 0.000102816\n",
      "2019-01-29T02:29:04.338847: step 2799, loss 0.0747118, acc 1, learning_rate 0.000102808\n",
      "2019-01-29T02:29:04.464766: step 2800, loss 0.0737206, acc 1, learning_rate 0.000102801\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:29:04.491551: step 2800, loss 0.644525, acc 0.751407\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2800\n",
      "\n",
      "2019-01-29T02:29:04.837546: step 2801, loss 0.0871238, acc 0.984375, learning_rate 0.000102793\n",
      "2019-01-29T02:29:04.926425: step 2802, loss 0.0593574, acc 1, learning_rate 0.000102786\n",
      "2019-01-29T02:29:05.052944: step 2803, loss 0.0817521, acc 1, learning_rate 0.000102778\n",
      "2019-01-29T02:29:05.183872: step 2804, loss 0.0753883, acc 1, learning_rate 0.000102771\n",
      "2019-01-29T02:29:05.311329: step 2805, loss 0.0878212, acc 1, learning_rate 0.000102764\n",
      "2019-01-29T02:29:05.401427: step 2806, loss 0.0982714, acc 1, learning_rate 0.000102756\n",
      "2019-01-29T02:29:05.529972: step 2807, loss 0.0798952, acc 1, learning_rate 0.000102749\n",
      "2019-01-29T02:29:05.658212: step 2808, loss 0.0593053, acc 1, learning_rate 0.000102742\n",
      "2019-01-29T02:29:05.779921: step 2809, loss 0.0683826, acc 1, learning_rate 0.000102734\n",
      "2019-01-29T02:29:05.908272: step 2810, loss 0.0789742, acc 1, learning_rate 0.000102727\n",
      "2019-01-29T02:29:06.035748: step 2811, loss 0.0834459, acc 1, learning_rate 0.00010272\n",
      "2019-01-29T02:29:06.158009: step 2812, loss 0.0746496, acc 1, learning_rate 0.000102712\n",
      "2019-01-29T02:29:06.285471: step 2813, loss 0.0799922, acc 1, learning_rate 0.000102705\n",
      "2019-01-29T02:29:06.408494: step 2814, loss 0.0728025, acc 0.984375, learning_rate 0.000102698\n",
      "2019-01-29T02:29:06.491410: step 2815, loss 0.106253, acc 1, learning_rate 0.000102691\n",
      "2019-01-29T02:29:06.593422: step 2816, loss 0.0859288, acc 1, learning_rate 0.000102684\n",
      "2019-01-29T02:29:06.724283: step 2817, loss 0.0582878, acc 1, learning_rate 0.000102677\n",
      "2019-01-29T02:29:06.815506: step 2818, loss 0.0705292, acc 1, learning_rate 0.000102669\n",
      "2019-01-29T02:29:06.935003: step 2819, loss 0.0953991, acc 1, learning_rate 0.000102662\n",
      "2019-01-29T02:29:07.038155: step 2820, loss 0.119951, acc 0.96875, learning_rate 0.000102655\n",
      "2019-01-29T02:29:07.168116: step 2821, loss 0.0697074, acc 1, learning_rate 0.000102648\n",
      "2019-01-29T02:29:07.294868: step 2822, loss 0.0684461, acc 1, learning_rate 0.000102641\n",
      "2019-01-29T02:29:07.425233: step 2823, loss 0.0716287, acc 1, learning_rate 0.000102634\n",
      "2019-01-29T02:29:07.553097: step 2824, loss 0.0667313, acc 1, learning_rate 0.000102627\n",
      "2019-01-29T02:29:07.680815: step 2825, loss 0.0689364, acc 1, learning_rate 0.00010262\n",
      "2019-01-29T02:29:07.805491: step 2826, loss 0.0794534, acc 1, learning_rate 0.000102613\n",
      "2019-01-29T02:29:07.938731: step 2827, loss 0.0862784, acc 1, learning_rate 0.000102606\n",
      "2019-01-29T02:29:08.068663: step 2828, loss 0.101624, acc 0.984375, learning_rate 0.000102599\n",
      "2019-01-29T02:29:08.153796: step 2829, loss 0.112003, acc 0.984375, learning_rate 0.000102592\n",
      "2019-01-29T02:29:08.284168: step 2830, loss 0.060489, acc 1, learning_rate 0.000102585\n",
      "2019-01-29T02:29:08.368153: step 2831, loss 0.0666188, acc 1, learning_rate 0.000102578\n",
      "2019-01-29T02:29:08.497448: step 2832, loss 0.0846005, acc 0.984375, learning_rate 0.000102572\n",
      "2019-01-29T02:29:08.624612: step 2833, loss 0.075839, acc 1, learning_rate 0.000102565\n",
      "2019-01-29T02:29:08.753259: step 2834, loss 0.0837313, acc 1, learning_rate 0.000102558\n",
      "2019-01-29T02:29:08.841824: step 2835, loss 0.0768324, acc 1, learning_rate 0.000102551\n",
      "2019-01-29T02:29:08.973671: step 2836, loss 0.0953062, acc 0.984375, learning_rate 0.000102544\n",
      "2019-01-29T02:29:09.102019: step 2837, loss 0.07412, acc 1, learning_rate 0.000102537\n",
      "2019-01-29T02:29:09.223667: step 2838, loss 0.108767, acc 0.96875, learning_rate 0.000102531\n",
      "2019-01-29T02:29:09.309169: step 2839, loss 0.0778366, acc 0.984375, learning_rate 0.000102524\n",
      "2019-01-29T02:29:09.435312: step 2840, loss 0.101548, acc 1, learning_rate 0.000102517\n",
      "2019-01-29T02:29:09.610955: step 2841, loss 0.0759594, acc 1, learning_rate 0.000102511\n",
      "2019-01-29T02:29:09.733872: step 2842, loss 0.0675671, acc 1, learning_rate 0.000102504\n",
      "2019-01-29T02:29:09.859829: step 2843, loss 0.0748823, acc 1, learning_rate 0.000102497\n",
      "2019-01-29T02:29:09.982926: step 2844, loss 0.0891019, acc 1, learning_rate 0.000102491\n",
      "2019-01-29T02:29:10.105178: step 2845, loss 0.0876926, acc 0.984375, learning_rate 0.000102484\n",
      "2019-01-29T02:29:10.235774: step 2846, loss 0.0704134, acc 1, learning_rate 0.000102477\n",
      "2019-01-29T02:29:10.360167: step 2847, loss 0.0728702, acc 1, learning_rate 0.000102471\n",
      "2019-01-29T02:29:10.485840: step 2848, loss 0.0756543, acc 1, learning_rate 0.000102464\n",
      "2019-01-29T02:29:10.614781: step 2849, loss 0.0753446, acc 1, learning_rate 0.000102458\n",
      "2019-01-29T02:29:10.736083: step 2850, loss 0.07306, acc 1, learning_rate 0.000102451\n",
      "2019-01-29T02:29:10.870074: step 2851, loss 0.077666, acc 1, learning_rate 0.000102444\n",
      "2019-01-29T02:29:10.996059: step 2852, loss 0.0606543, acc 1, learning_rate 0.000102438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:29:11.121501: step 2853, loss 0.0824213, acc 1, learning_rate 0.000102431\n",
      "2019-01-29T02:29:11.242431: step 2854, loss 0.0616467, acc 1, learning_rate 0.000102425\n",
      "2019-01-29T02:29:11.374742: step 2855, loss 0.0748366, acc 1, learning_rate 0.000102419\n",
      "2019-01-29T02:29:11.505684: step 2856, loss 0.065745, acc 1, learning_rate 0.000102412\n",
      "2019-01-29T02:29:11.596007: step 2857, loss 0.0848618, acc 1, learning_rate 0.000102406\n",
      "2019-01-29T02:29:11.720783: step 2858, loss 0.0888935, acc 0.984375, learning_rate 0.000102399\n",
      "2019-01-29T02:29:11.846745: step 2859, loss 0.0806071, acc 1, learning_rate 0.000102393\n",
      "2019-01-29T02:29:11.940781: step 2860, loss 0.0619522, acc 1, learning_rate 0.000102386\n",
      "2019-01-29T02:29:12.065294: step 2861, loss 0.0720134, acc 0.984375, learning_rate 0.00010238\n",
      "2019-01-29T02:29:12.187049: step 2862, loss 0.0801037, acc 1, learning_rate 0.000102374\n",
      "2019-01-29T02:29:12.314013: step 2863, loss 0.062859, acc 1, learning_rate 0.000102367\n",
      "2019-01-29T02:29:12.439341: step 2864, loss 0.0633545, acc 1, learning_rate 0.000102361\n",
      "2019-01-29T02:29:12.562984: step 2865, loss 0.0736531, acc 1, learning_rate 0.000102355\n",
      "2019-01-29T02:29:12.704607: step 2866, loss 0.0876545, acc 0.984375, learning_rate 0.000102349\n",
      "2019-01-29T02:29:12.830579: step 2867, loss 0.056019, acc 1, learning_rate 0.000102342\n",
      "2019-01-29T02:29:12.949172: step 2868, loss 0.0947204, acc 0.984375, learning_rate 0.000102336\n",
      "2019-01-29T02:29:13.077582: step 2869, loss 0.0922409, acc 0.984375, learning_rate 0.00010233\n",
      "2019-01-29T02:29:13.199345: step 2870, loss 0.0827736, acc 1, learning_rate 0.000102324\n",
      "2019-01-29T02:29:13.329268: step 2871, loss 0.0591252, acc 1, learning_rate 0.000102317\n",
      "2019-01-29T02:29:13.457290: step 2872, loss 0.105153, acc 0.984375, learning_rate 0.000102311\n",
      "2019-01-29T02:29:13.576643: step 2873, loss 0.0876876, acc 0.984375, learning_rate 0.000102305\n",
      "2019-01-29T02:29:13.697732: step 2874, loss 0.0720306, acc 1, learning_rate 0.000102299\n",
      "2019-01-29T02:29:13.824279: step 2875, loss 0.113907, acc 0.984375, learning_rate 0.000102293\n",
      "2019-01-29T02:29:13.947368: step 2876, loss 0.0673694, acc 1, learning_rate 0.000102287\n",
      "2019-01-29T02:29:14.072539: step 2877, loss 0.0971446, acc 0.984375, learning_rate 0.000102281\n",
      "2019-01-29T02:29:14.204959: step 2878, loss 0.0736383, acc 1, learning_rate 0.000102275\n",
      "2019-01-29T02:29:14.328820: step 2879, loss 0.0584419, acc 1, learning_rate 0.000102269\n",
      "2019-01-29T02:29:14.451186: step 2880, loss 0.0800484, acc 0.984375, learning_rate 0.000102262\n",
      "2019-01-29T02:29:14.571449: step 2881, loss 0.0767132, acc 1, learning_rate 0.000102256\n",
      "2019-01-29T02:29:14.699366: step 2882, loss 0.0936685, acc 0.984375, learning_rate 0.00010225\n",
      "2019-01-29T02:29:14.826465: step 2883, loss 0.0990488, acc 0.984375, learning_rate 0.000102244\n",
      "2019-01-29T02:29:14.955291: step 2884, loss 0.0912017, acc 1, learning_rate 0.000102238\n",
      "2019-01-29T02:29:15.077520: step 2885, loss 0.0912023, acc 1, learning_rate 0.000102232\n",
      "2019-01-29T02:29:15.168171: step 2886, loss 0.0800569, acc 1, learning_rate 0.000102227\n",
      "2019-01-29T02:29:15.293561: step 2887, loss 0.083573, acc 0.96875, learning_rate 0.000102221\n",
      "2019-01-29T02:29:15.418092: step 2888, loss 0.0839813, acc 1, learning_rate 0.000102215\n",
      "2019-01-29T02:29:15.505327: step 2889, loss 0.0957294, acc 0.984375, learning_rate 0.000102209\n",
      "2019-01-29T02:29:15.595387: step 2890, loss 0.0748086, acc 1, learning_rate 0.000102203\n",
      "2019-01-29T02:29:15.714080: step 2891, loss 0.086034, acc 0.984375, learning_rate 0.000102197\n",
      "2019-01-29T02:29:15.839793: step 2892, loss 0.0791249, acc 1, learning_rate 0.000102191\n",
      "2019-01-29T02:29:15.924928: step 2893, loss 0.0801148, acc 1, learning_rate 0.000102185\n",
      "2019-01-29T02:29:16.050973: step 2894, loss 0.0821493, acc 1, learning_rate 0.00010218\n",
      "2019-01-29T02:29:16.169941: step 2895, loss 0.0794006, acc 1, learning_rate 0.000102174\n",
      "2019-01-29T02:29:16.296224: step 2896, loss 0.0672036, acc 1, learning_rate 0.000102168\n",
      "2019-01-29T02:29:16.427814: step 2897, loss 0.075194, acc 1, learning_rate 0.000102162\n",
      "2019-01-29T02:29:16.518077: step 2898, loss 0.0766365, acc 1, learning_rate 0.000102156\n",
      "2019-01-29T02:29:16.636190: step 2899, loss 0.0694476, acc 1, learning_rate 0.000102151\n",
      "2019-01-29T02:29:16.759536: step 2900, loss 0.106123, acc 0.984375, learning_rate 0.000102145\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:29:16.787091: step 2900, loss 0.639177, acc 0.756098\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-2900\n",
      "\n",
      "2019-01-29T02:29:17.140486: step 2901, loss 0.0652208, acc 1, learning_rate 0.000102139\n",
      "2019-01-29T02:29:17.260669: step 2902, loss 0.0695915, acc 1, learning_rate 0.000102134\n",
      "2019-01-29T02:29:17.386085: step 2903, loss 0.0740635, acc 0.984375, learning_rate 0.000102128\n",
      "2019-01-29T02:29:17.509969: step 2904, loss 0.0746826, acc 1, learning_rate 0.000102122\n",
      "2019-01-29T02:29:17.627031: step 2905, loss 0.0834422, acc 0.984375, learning_rate 0.000102116\n",
      "2019-01-29T02:29:17.714484: step 2906, loss 0.125528, acc 0.96875, learning_rate 0.000102111\n",
      "2019-01-29T02:29:17.837451: step 2907, loss 0.0662143, acc 1, learning_rate 0.000102105\n",
      "2019-01-29T02:29:17.966382: step 2908, loss 0.0699342, acc 1, learning_rate 0.0001021\n",
      "2019-01-29T02:29:18.095290: step 2909, loss 0.0693102, acc 1, learning_rate 0.000102094\n",
      "2019-01-29T02:29:18.218961: step 2910, loss 0.0785232, acc 1, learning_rate 0.000102088\n",
      "2019-01-29T02:29:18.345499: step 2911, loss 0.093664, acc 1, learning_rate 0.000102083\n",
      "2019-01-29T02:29:18.472819: step 2912, loss 0.0676347, acc 1, learning_rate 0.000102077\n",
      "2019-01-29T02:29:18.593964: step 2913, loss 0.0766607, acc 1, learning_rate 0.000102072\n",
      "2019-01-29T02:29:18.683334: step 2914, loss 0.0902894, acc 0.984375, learning_rate 0.000102066\n",
      "2019-01-29T02:29:18.815887: step 2915, loss 0.0610893, acc 1, learning_rate 0.000102061\n",
      "2019-01-29T02:29:18.945791: step 2916, loss 0.0897887, acc 1, learning_rate 0.000102055\n",
      "2019-01-29T02:29:19.071675: step 2917, loss 0.0853284, acc 0.984375, learning_rate 0.00010205\n",
      "2019-01-29T02:29:19.159292: step 2918, loss 0.0650029, acc 1, learning_rate 0.000102044\n",
      "2019-01-29T02:29:19.286540: step 2919, loss 0.0788455, acc 1, learning_rate 0.000102039\n",
      "2019-01-29T02:29:19.413299: step 2920, loss 0.0814931, acc 1, learning_rate 0.000102033\n",
      "2019-01-29T02:29:19.533001: step 2921, loss 0.0732509, acc 1, learning_rate 0.000102028\n",
      "2019-01-29T02:29:19.622469: step 2922, loss 0.0716831, acc 1, learning_rate 0.000102023\n",
      "2019-01-29T02:29:19.745258: step 2923, loss 0.0710265, acc 1, learning_rate 0.000102017\n",
      "2019-01-29T02:29:19.835856: step 2924, loss 0.0844293, acc 1, learning_rate 0.000102012\n",
      "2019-01-29T02:29:19.957456: step 2925, loss 0.068063, acc 1, learning_rate 0.000102007\n",
      "2019-01-29T02:29:20.082354: step 2926, loss 0.0832304, acc 1, learning_rate 0.000102001\n",
      "2019-01-29T02:29:20.204394: step 2927, loss 0.0740731, acc 1, learning_rate 0.000101996\n",
      "2019-01-29T02:29:20.317330: step 2928, loss 0.076088, acc 0.984375, learning_rate 0.000101991\n",
      "2019-01-29T02:29:20.446298: step 2929, loss 0.0754198, acc 1, learning_rate 0.000101985\n",
      "2019-01-29T02:29:20.568348: step 2930, loss 0.0907096, acc 1, learning_rate 0.00010198\n",
      "2019-01-29T02:29:20.697500: step 2931, loss 0.0742678, acc 1, learning_rate 0.000101975\n",
      "2019-01-29T02:29:20.821512: step 2932, loss 0.0766558, acc 1, learning_rate 0.000101969\n",
      "2019-01-29T02:29:20.949286: step 2933, loss 0.0961122, acc 1, learning_rate 0.000101964\n",
      "2019-01-29T02:29:21.078743: step 2934, loss 0.0780652, acc 1, learning_rate 0.000101959\n",
      "2019-01-29T02:29:21.210648: step 2935, loss 0.0899567, acc 1, learning_rate 0.000101954\n",
      "2019-01-29T02:29:21.338919: step 2936, loss 0.0778439, acc 1, learning_rate 0.000101949\n",
      "2019-01-29T02:29:21.429316: step 2937, loss 0.0650921, acc 1, learning_rate 0.000101943\n",
      "2019-01-29T02:29:21.518544: step 2938, loss 0.109749, acc 0.984375, learning_rate 0.000101938\n",
      "2019-01-29T02:29:21.605382: step 2939, loss 0.0777983, acc 1, learning_rate 0.000101933\n",
      "2019-01-29T02:29:21.727447: step 2940, loss 0.0965933, acc 1, learning_rate 0.000101928\n",
      "2019-01-29T02:29:21.849818: step 2941, loss 0.076162, acc 1, learning_rate 0.000101923\n",
      "2019-01-29T02:29:21.978410: step 2942, loss 0.0847512, acc 0.984375, learning_rate 0.000101918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:29:22.106058: step 2943, loss 0.103225, acc 0.984375, learning_rate 0.000101912\n",
      "2019-01-29T02:29:22.235746: step 2944, loss 0.0758272, acc 1, learning_rate 0.000101907\n",
      "2019-01-29T02:29:22.359271: step 2945, loss 0.0803797, acc 1, learning_rate 0.000101902\n",
      "2019-01-29T02:29:22.487127: step 2946, loss 0.0727315, acc 1, learning_rate 0.000101897\n",
      "2019-01-29T02:29:22.572476: step 2947, loss 0.0631607, acc 1, learning_rate 0.000101892\n",
      "2019-01-29T02:29:22.694777: step 2948, loss 0.0809865, acc 1, learning_rate 0.000101887\n",
      "2019-01-29T02:29:22.782382: step 2949, loss 0.0699458, acc 1, learning_rate 0.000101882\n",
      "2019-01-29T02:29:22.913048: step 2950, loss 0.0707322, acc 1, learning_rate 0.000101877\n",
      "2019-01-29T02:29:23.040990: step 2951, loss 0.105082, acc 0.984375, learning_rate 0.000101872\n",
      "2019-01-29T02:29:23.160885: step 2952, loss 0.0707661, acc 0.984375, learning_rate 0.000101867\n",
      "2019-01-29T02:29:23.292137: step 2953, loss 0.0924138, acc 0.984375, learning_rate 0.000101862\n",
      "2019-01-29T02:29:23.420207: step 2954, loss 0.0769042, acc 1, learning_rate 0.000101857\n",
      "2019-01-29T02:29:23.543423: step 2955, loss 0.0770012, acc 1, learning_rate 0.000101852\n",
      "2019-01-29T02:29:23.674027: step 2956, loss 0.0729757, acc 1, learning_rate 0.000101847\n",
      "2019-01-29T02:29:23.759990: step 2957, loss 0.0958429, acc 0.984375, learning_rate 0.000101842\n",
      "2019-01-29T02:29:23.887516: step 2958, loss 0.0656933, acc 1, learning_rate 0.000101837\n",
      "2019-01-29T02:29:24.015092: step 2959, loss 0.0871472, acc 0.984375, learning_rate 0.000101833\n",
      "2019-01-29T02:29:24.110217: step 2960, loss 0.111629, acc 0.984375, learning_rate 0.000101828\n",
      "2019-01-29T02:29:24.200207: step 2961, loss 0.0746894, acc 1, learning_rate 0.000101823\n",
      "2019-01-29T02:29:24.327550: step 2962, loss 0.0697715, acc 0.984375, learning_rate 0.000101818\n",
      "2019-01-29T02:29:24.415564: step 2963, loss 0.0739268, acc 0.984375, learning_rate 0.000101813\n",
      "2019-01-29T02:29:24.542567: step 2964, loss 0.0655764, acc 1, learning_rate 0.000101808\n",
      "2019-01-29T02:29:24.666037: step 2965, loss 0.070813, acc 1, learning_rate 0.000101803\n",
      "2019-01-29T02:29:24.754356: step 2966, loss 0.0753812, acc 1, learning_rate 0.000101799\n",
      "2019-01-29T02:29:24.885339: step 2967, loss 0.0698478, acc 1, learning_rate 0.000101794\n",
      "2019-01-29T02:29:25.007210: step 2968, loss 0.0980736, acc 0.984375, learning_rate 0.000101789\n",
      "2019-01-29T02:29:25.130559: step 2969, loss 0.0599297, acc 1, learning_rate 0.000101784\n",
      "2019-01-29T02:29:25.252778: step 2970, loss 0.0633326, acc 1, learning_rate 0.00010178\n",
      "2019-01-29T02:29:25.381044: step 2971, loss 0.104673, acc 0.984375, learning_rate 0.000101775\n",
      "2019-01-29T02:29:25.513762: step 2972, loss 0.0836538, acc 0.984375, learning_rate 0.00010177\n",
      "2019-01-29T02:29:25.641611: step 2973, loss 0.0930288, acc 0.96875, learning_rate 0.000101765\n",
      "2019-01-29T02:29:25.730654: step 2974, loss 0.076138, acc 1, learning_rate 0.000101761\n",
      "2019-01-29T02:29:25.862454: step 2975, loss 0.0782869, acc 1, learning_rate 0.000101756\n",
      "2019-01-29T02:29:25.992596: step 2976, loss 0.0817075, acc 0.984375, learning_rate 0.000101751\n",
      "2019-01-29T02:29:26.117518: step 2977, loss 0.0819144, acc 1, learning_rate 0.000101747\n",
      "2019-01-29T02:29:26.240818: step 2978, loss 0.0668587, acc 1, learning_rate 0.000101742\n",
      "2019-01-29T02:29:26.371101: step 2979, loss 0.0967021, acc 0.984375, learning_rate 0.000101737\n",
      "2019-01-29T02:29:26.497600: step 2980, loss 0.0958657, acc 0.984375, learning_rate 0.000101733\n",
      "2019-01-29T02:29:26.629140: step 2981, loss 0.0797293, acc 1, learning_rate 0.000101728\n",
      "2019-01-29T02:29:26.754216: step 2982, loss 0.087046, acc 0.984375, learning_rate 0.000101723\n",
      "2019-01-29T02:29:26.877651: step 2983, loss 0.103754, acc 0.96875, learning_rate 0.000101719\n",
      "2019-01-29T02:29:27.005698: step 2984, loss 0.0831322, acc 1, learning_rate 0.000101714\n",
      "2019-01-29T02:29:27.126702: step 2985, loss 0.0759984, acc 1, learning_rate 0.00010171\n",
      "2019-01-29T02:29:27.249928: step 2986, loss 0.0722342, acc 1, learning_rate 0.000101705\n",
      "2019-01-29T02:29:27.338058: step 2987, loss 0.0633966, acc 1, learning_rate 0.000101701\n",
      "2019-01-29T02:29:27.425306: step 2988, loss 0.0657772, acc 1, learning_rate 0.000101696\n",
      "2019-01-29T02:29:27.513334: step 2989, loss 0.0869519, acc 0.984375, learning_rate 0.000101692\n",
      "2019-01-29T02:29:27.640947: step 2990, loss 0.078012, acc 1, learning_rate 0.000101687\n",
      "2019-01-29T02:29:27.730705: step 2991, loss 0.0864282, acc 1, learning_rate 0.000101683\n",
      "2019-01-29T02:29:27.863123: step 2992, loss 0.0839462, acc 1, learning_rate 0.000101678\n",
      "2019-01-29T02:29:27.987369: step 2993, loss 0.0636239, acc 1, learning_rate 0.000101674\n",
      "2019-01-29T02:29:28.110696: step 2994, loss 0.065053, acc 1, learning_rate 0.000101669\n",
      "2019-01-29T02:29:28.233661: step 2995, loss 0.0677131, acc 1, learning_rate 0.000101665\n",
      "2019-01-29T02:29:28.354827: step 2996, loss 0.0704439, acc 1, learning_rate 0.00010166\n",
      "2019-01-29T02:29:28.482444: step 2997, loss 0.0848839, acc 0.984375, learning_rate 0.000101656\n",
      "2019-01-29T02:29:28.609648: step 2998, loss 0.0781629, acc 1, learning_rate 0.000101651\n",
      "2019-01-29T02:29:28.737847: step 2999, loss 0.0709436, acc 1, learning_rate 0.000101647\n",
      "2019-01-29T02:29:28.865183: step 3000, loss 0.117066, acc 0.983333, learning_rate 0.000101643\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:29:28.892066: step 3000, loss 0.647259, acc 0.754221\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3000\n",
      "\n",
      "2019-01-29T02:29:29.231227: step 3001, loss 0.150474, acc 0.96875, learning_rate 0.000101638\n",
      "2019-01-29T02:29:29.354556: step 3002, loss 0.0568951, acc 1, learning_rate 0.000101634\n",
      "2019-01-29T02:29:29.483983: step 3003, loss 0.0639927, acc 1, learning_rate 0.00010163\n",
      "2019-01-29T02:29:29.571187: step 3004, loss 0.0718151, acc 1, learning_rate 0.000101625\n",
      "2019-01-29T02:29:29.695069: step 3005, loss 0.0665845, acc 1, learning_rate 0.000101621\n",
      "2019-01-29T02:29:29.822774: step 3006, loss 0.076195, acc 1, learning_rate 0.000101617\n",
      "2019-01-29T02:29:29.946658: step 3007, loss 0.0838703, acc 1, learning_rate 0.000101612\n",
      "2019-01-29T02:29:30.032597: step 3008, loss 0.0924396, acc 0.984375, learning_rate 0.000101608\n",
      "2019-01-29T02:29:30.160288: step 3009, loss 0.0666949, acc 1, learning_rate 0.000101604\n",
      "2019-01-29T02:29:30.284433: step 3010, loss 0.0908902, acc 1, learning_rate 0.000101599\n",
      "2019-01-29T02:29:30.412317: step 3011, loss 0.0742146, acc 1, learning_rate 0.000101595\n",
      "2019-01-29T02:29:30.539269: step 3012, loss 0.0584409, acc 1, learning_rate 0.000101591\n",
      "2019-01-29T02:29:30.665944: step 3013, loss 0.0703448, acc 1, learning_rate 0.000101587\n",
      "2019-01-29T02:29:30.790414: step 3014, loss 0.0955795, acc 1, learning_rate 0.000101582\n",
      "2019-01-29T02:29:30.917467: step 3015, loss 0.0786768, acc 1, learning_rate 0.000101578\n",
      "2019-01-29T02:29:31.040953: step 3016, loss 0.0658163, acc 1, learning_rate 0.000101574\n",
      "2019-01-29T02:29:31.165289: step 3017, loss 0.0712757, acc 1, learning_rate 0.00010157\n",
      "2019-01-29T02:29:31.296795: step 3018, loss 0.0949515, acc 0.984375, learning_rate 0.000101566\n",
      "2019-01-29T02:29:31.378951: step 3019, loss 0.061649, acc 1, learning_rate 0.000101561\n",
      "2019-01-29T02:29:31.508542: step 3020, loss 0.0780319, acc 1, learning_rate 0.000101557\n",
      "2019-01-29T02:29:31.632803: step 3021, loss 0.0750648, acc 1, learning_rate 0.000101553\n",
      "2019-01-29T02:29:31.759991: step 3022, loss 0.0687828, acc 1, learning_rate 0.000101549\n",
      "2019-01-29T02:29:31.891703: step 3023, loss 0.0849713, acc 1, learning_rate 0.000101545\n",
      "2019-01-29T02:29:31.986147: step 3024, loss 0.075841, acc 1, learning_rate 0.000101541\n",
      "2019-01-29T02:29:32.113599: step 3025, loss 0.0979775, acc 0.984375, learning_rate 0.000101537\n",
      "2019-01-29T02:29:32.240255: step 3026, loss 0.0893021, acc 0.984375, learning_rate 0.000101533\n",
      "2019-01-29T02:29:32.363253: step 3027, loss 0.089508, acc 0.984375, learning_rate 0.000101529\n",
      "2019-01-29T02:29:32.453951: step 3028, loss 0.105756, acc 0.96875, learning_rate 0.000101524\n",
      "2019-01-29T02:29:32.572149: step 3029, loss 0.0823887, acc 0.984375, learning_rate 0.00010152\n",
      "2019-01-29T02:29:32.698764: step 3030, loss 0.0816029, acc 1, learning_rate 0.000101516\n",
      "2019-01-29T02:29:32.821915: step 3031, loss 0.0579596, acc 1, learning_rate 0.000101512\n",
      "2019-01-29T02:29:32.948057: step 3032, loss 0.0757812, acc 1, learning_rate 0.000101508\n",
      "2019-01-29T02:29:33.072315: step 3033, loss 0.0859075, acc 0.984375, learning_rate 0.000101504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:29:33.159425: step 3034, loss 0.0659484, acc 1, learning_rate 0.0001015\n",
      "2019-01-29T02:29:33.258946: step 3035, loss 0.075277, acc 1, learning_rate 0.000101496\n",
      "2019-01-29T02:29:33.392533: step 3036, loss 0.0728614, acc 1, learning_rate 0.000101492\n",
      "2019-01-29T02:29:33.483723: step 3037, loss 0.108605, acc 0.984375, learning_rate 0.000101488\n",
      "2019-01-29T02:29:33.605444: step 3038, loss 0.06987, acc 1, learning_rate 0.000101484\n",
      "2019-01-29T02:29:33.731887: step 3039, loss 0.0609151, acc 1, learning_rate 0.00010148\n",
      "2019-01-29T02:29:33.819832: step 3040, loss 0.101238, acc 0.984375, learning_rate 0.000101476\n",
      "2019-01-29T02:29:33.942669: step 3041, loss 0.0755047, acc 0.984375, learning_rate 0.000101472\n",
      "2019-01-29T02:29:34.068117: step 3042, loss 0.075842, acc 0.984375, learning_rate 0.000101469\n",
      "2019-01-29T02:29:34.190360: step 3043, loss 0.0870666, acc 1, learning_rate 0.000101465\n",
      "2019-01-29T02:29:34.318715: step 3044, loss 0.0673093, acc 1, learning_rate 0.000101461\n",
      "2019-01-29T02:29:34.445951: step 3045, loss 0.0822014, acc 1, learning_rate 0.000101457\n",
      "2019-01-29T02:29:34.569327: step 3046, loss 0.0654812, acc 1, learning_rate 0.000101453\n",
      "2019-01-29T02:29:34.690877: step 3047, loss 0.0639231, acc 1, learning_rate 0.000101449\n",
      "2019-01-29T02:29:34.811446: step 3048, loss 0.0629289, acc 1, learning_rate 0.000101445\n",
      "2019-01-29T02:29:34.938668: step 3049, loss 0.0784091, acc 1, learning_rate 0.000101441\n",
      "2019-01-29T02:29:35.063316: step 3050, loss 0.103067, acc 0.984375, learning_rate 0.000101438\n",
      "2019-01-29T02:29:35.193589: step 3051, loss 0.0904073, acc 1, learning_rate 0.000101434\n",
      "2019-01-29T02:29:35.319659: step 3052, loss 0.090337, acc 1, learning_rate 0.00010143\n",
      "2019-01-29T02:29:35.424046: step 3053, loss 0.0720143, acc 1, learning_rate 0.000101426\n",
      "2019-01-29T02:29:35.546849: step 3054, loss 0.0976044, acc 1, learning_rate 0.000101422\n",
      "2019-01-29T02:29:35.676624: step 3055, loss 0.0532915, acc 1, learning_rate 0.000101418\n",
      "2019-01-29T02:29:35.766376: step 3056, loss 0.0661435, acc 1, learning_rate 0.000101415\n",
      "2019-01-29T02:29:35.888876: step 3057, loss 0.0732715, acc 1, learning_rate 0.000101411\n",
      "2019-01-29T02:29:35.995465: step 3058, loss 0.064364, acc 1, learning_rate 0.000101407\n",
      "2019-01-29T02:29:36.121447: step 3059, loss 0.0689092, acc 1, learning_rate 0.000101403\n",
      "2019-01-29T02:29:36.247681: step 3060, loss 0.0621803, acc 1, learning_rate 0.0001014\n",
      "2019-01-29T02:29:36.368046: step 3061, loss 0.0605098, acc 1, learning_rate 0.000101396\n",
      "2019-01-29T02:29:36.495045: step 3062, loss 0.0594181, acc 1, learning_rate 0.000101392\n",
      "2019-01-29T02:29:36.618099: step 3063, loss 0.08884, acc 0.984375, learning_rate 0.000101389\n",
      "2019-01-29T02:29:36.745766: step 3064, loss 0.058446, acc 1, learning_rate 0.000101385\n",
      "2019-01-29T02:29:36.832690: step 3065, loss 0.0799324, acc 1, learning_rate 0.000101381\n",
      "2019-01-29T02:29:36.955023: step 3066, loss 0.0792388, acc 1, learning_rate 0.000101377\n",
      "2019-01-29T02:29:37.043007: step 3067, loss 0.0862611, acc 1, learning_rate 0.000101374\n",
      "2019-01-29T02:29:37.165496: step 3068, loss 0.132411, acc 0.96875, learning_rate 0.00010137\n",
      "2019-01-29T02:29:37.254591: step 3069, loss 0.0790663, acc 1, learning_rate 0.000101366\n",
      "2019-01-29T02:29:37.379264: step 3070, loss 0.0737176, acc 1, learning_rate 0.000101363\n",
      "2019-01-29T02:29:37.469344: step 3071, loss 0.075291, acc 0.984375, learning_rate 0.000101359\n",
      "2019-01-29T02:29:37.592433: step 3072, loss 0.076508, acc 1, learning_rate 0.000101356\n",
      "2019-01-29T02:29:37.688973: step 3073, loss 0.0807193, acc 1, learning_rate 0.000101352\n",
      "2019-01-29T02:29:37.811441: step 3074, loss 0.104389, acc 0.984375, learning_rate 0.000101348\n",
      "2019-01-29T02:29:37.943360: step 3075, loss 0.0705873, acc 1, learning_rate 0.000101345\n",
      "2019-01-29T02:29:38.065800: step 3076, loss 0.107161, acc 0.984375, learning_rate 0.000101341\n",
      "2019-01-29T02:29:38.188824: step 3077, loss 0.0654808, acc 1, learning_rate 0.000101338\n",
      "2019-01-29T02:29:38.304726: step 3078, loss 0.0717323, acc 1, learning_rate 0.000101334\n",
      "2019-01-29T02:29:38.427261: step 3079, loss 0.0779764, acc 1, learning_rate 0.000101331\n",
      "2019-01-29T02:29:38.516134: step 3080, loss 0.0764104, acc 1, learning_rate 0.000101327\n",
      "2019-01-29T02:29:38.643101: step 3081, loss 0.0620503, acc 1, learning_rate 0.000101323\n",
      "2019-01-29T02:29:38.761128: step 3082, loss 0.077451, acc 1, learning_rate 0.00010132\n",
      "2019-01-29T02:29:38.845120: step 3083, loss 0.0860481, acc 1, learning_rate 0.000101316\n",
      "2019-01-29T02:29:38.927950: step 3084, loss 0.0701296, acc 1, learning_rate 0.000101313\n",
      "2019-01-29T02:29:39.051888: step 3085, loss 0.0620306, acc 1, learning_rate 0.000101309\n",
      "2019-01-29T02:29:39.177787: step 3086, loss 0.0935355, acc 0.984375, learning_rate 0.000101306\n",
      "2019-01-29T02:29:39.304713: step 3087, loss 0.0636466, acc 1, learning_rate 0.000101302\n",
      "2019-01-29T02:29:39.431552: step 3088, loss 0.0800231, acc 1, learning_rate 0.000101299\n",
      "2019-01-29T02:29:39.559366: step 3089, loss 0.083772, acc 1, learning_rate 0.000101295\n",
      "2019-01-29T02:29:39.687918: step 3090, loss 0.0814203, acc 1, learning_rate 0.000101292\n",
      "2019-01-29T02:29:39.774930: step 3091, loss 0.0774215, acc 1, learning_rate 0.000101289\n",
      "2019-01-29T02:29:39.902936: step 3092, loss 0.0841895, acc 1, learning_rate 0.000101285\n",
      "2019-01-29T02:29:40.033655: step 3093, loss 0.0714927, acc 0.984375, learning_rate 0.000101282\n",
      "2019-01-29T02:29:40.162172: step 3094, loss 0.0752098, acc 1, learning_rate 0.000101278\n",
      "2019-01-29T02:29:40.292240: step 3095, loss 0.0848744, acc 1, learning_rate 0.000101275\n",
      "2019-01-29T02:29:40.401126: step 3096, loss 0.118465, acc 0.984375, learning_rate 0.000101272\n",
      "2019-01-29T02:29:40.523877: step 3097, loss 0.0760296, acc 1, learning_rate 0.000101268\n",
      "2019-01-29T02:29:40.610388: step 3098, loss 0.0711771, acc 1, learning_rate 0.000101265\n",
      "2019-01-29T02:29:40.735715: step 3099, loss 0.0625064, acc 1, learning_rate 0.000101261\n",
      "2019-01-29T02:29:40.860565: step 3100, loss 0.0661012, acc 1, learning_rate 0.000101258\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:29:40.887466: step 3100, loss 0.643451, acc 0.757974\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3100\n",
      "\n",
      "2019-01-29T02:29:41.198779: step 3101, loss 0.0725749, acc 1, learning_rate 0.000101255\n",
      "2019-01-29T02:29:41.325157: step 3102, loss 0.091796, acc 0.984375, learning_rate 0.000101251\n",
      "2019-01-29T02:29:41.439288: step 3103, loss 0.0836582, acc 1, learning_rate 0.000101248\n",
      "2019-01-29T02:29:41.560660: step 3104, loss 0.0738237, acc 1, learning_rate 0.000101245\n",
      "2019-01-29T02:29:41.692827: step 3105, loss 0.0785668, acc 0.984375, learning_rate 0.000101241\n",
      "2019-01-29T02:29:41.812255: step 3106, loss 0.0625961, acc 1, learning_rate 0.000101238\n",
      "2019-01-29T02:29:41.936149: step 3107, loss 0.0855581, acc 0.984375, learning_rate 0.000101235\n",
      "2019-01-29T02:29:42.059290: step 3108, loss 0.0801442, acc 0.984375, learning_rate 0.000101231\n",
      "2019-01-29T02:29:42.156021: step 3109, loss 0.0840497, acc 1, learning_rate 0.000101228\n",
      "2019-01-29T02:29:42.276760: step 3110, loss 0.120409, acc 0.984375, learning_rate 0.000101225\n",
      "2019-01-29T02:29:42.405394: step 3111, loss 0.0796834, acc 1, learning_rate 0.000101222\n",
      "2019-01-29T02:29:42.535605: step 3112, loss 0.0782313, acc 1, learning_rate 0.000101218\n",
      "2019-01-29T02:29:42.664536: step 3113, loss 0.0898013, acc 0.984375, learning_rate 0.000101215\n",
      "2019-01-29T02:29:42.784624: step 3114, loss 0.109081, acc 0.984375, learning_rate 0.000101212\n",
      "2019-01-29T02:29:42.907265: step 3115, loss 0.0956244, acc 0.984375, learning_rate 0.000101209\n",
      "2019-01-29T02:29:43.039401: step 3116, loss 0.0925123, acc 0.984375, learning_rate 0.000101205\n",
      "2019-01-29T02:29:43.131143: step 3117, loss 0.0717717, acc 0.984375, learning_rate 0.000101202\n",
      "2019-01-29T02:29:43.257822: step 3118, loss 0.0817171, acc 1, learning_rate 0.000101199\n",
      "2019-01-29T02:29:43.386130: step 3119, loss 0.0572464, acc 1, learning_rate 0.000101196\n",
      "2019-01-29T02:29:43.507014: step 3120, loss 0.109083, acc 1, learning_rate 0.000101193\n",
      "2019-01-29T02:29:43.641173: step 3121, loss 0.0598417, acc 1, learning_rate 0.000101189\n",
      "2019-01-29T02:29:43.727183: step 3122, loss 0.073044, acc 1, learning_rate 0.000101186\n",
      "2019-01-29T02:29:43.850500: step 3123, loss 0.0906175, acc 1, learning_rate 0.000101183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:29:43.973578: step 3124, loss 0.0913293, acc 0.984375, learning_rate 0.00010118\n",
      "2019-01-29T02:29:44.071043: step 3125, loss 0.0816559, acc 1, learning_rate 0.000101177\n",
      "2019-01-29T02:29:44.195411: step 3126, loss 0.0851551, acc 0.984375, learning_rate 0.000101174\n",
      "2019-01-29T02:29:44.317904: step 3127, loss 0.0729626, acc 1, learning_rate 0.000101171\n",
      "2019-01-29T02:29:44.443964: step 3128, loss 0.0885728, acc 0.984375, learning_rate 0.000101167\n",
      "2019-01-29T02:29:44.572616: step 3129, loss 0.0828233, acc 0.984375, learning_rate 0.000101164\n",
      "2019-01-29T02:29:44.701596: step 3130, loss 0.109927, acc 0.96875, learning_rate 0.000101161\n",
      "2019-01-29T02:29:44.823810: step 3131, loss 0.0639342, acc 1, learning_rate 0.000101158\n",
      "2019-01-29T02:29:44.950778: step 3132, loss 0.0839799, acc 1, learning_rate 0.000101155\n",
      "2019-01-29T02:29:45.073846: step 3133, loss 0.0605519, acc 1, learning_rate 0.000101152\n",
      "2019-01-29T02:29:45.205866: step 3134, loss 0.0648862, acc 1, learning_rate 0.000101149\n",
      "2019-01-29T02:29:45.333198: step 3135, loss 0.0579243, acc 1, learning_rate 0.000101146\n",
      "2019-01-29T02:29:45.421186: step 3136, loss 0.0824191, acc 1, learning_rate 0.000101143\n",
      "2019-01-29T02:29:45.544473: step 3137, loss 0.0732183, acc 0.984375, learning_rate 0.00010114\n",
      "2019-01-29T02:29:45.632699: step 3138, loss 0.0748357, acc 1, learning_rate 0.000101137\n",
      "2019-01-29T02:29:45.751845: step 3139, loss 0.0573493, acc 1, learning_rate 0.000101134\n",
      "2019-01-29T02:29:45.841425: step 3140, loss 0.0935232, acc 0.984375, learning_rate 0.000101131\n",
      "2019-01-29T02:29:45.966402: step 3141, loss 0.070977, acc 1, learning_rate 0.000101128\n",
      "2019-01-29T02:29:46.068960: step 3142, loss 0.0730051, acc 1, learning_rate 0.000101125\n",
      "2019-01-29T02:29:46.158684: step 3143, loss 0.0879916, acc 0.984375, learning_rate 0.000101122\n",
      "2019-01-29T02:29:46.277203: step 3144, loss 0.081789, acc 1, learning_rate 0.000101119\n",
      "2019-01-29T02:29:46.399891: step 3145, loss 0.0611895, acc 1, learning_rate 0.000101116\n",
      "2019-01-29T02:29:46.518467: step 3146, loss 0.0807165, acc 0.984375, learning_rate 0.000101113\n",
      "2019-01-29T02:29:46.608198: step 3147, loss 0.0676883, acc 1, learning_rate 0.00010111\n",
      "2019-01-29T02:29:46.732444: step 3148, loss 0.0898276, acc 1, learning_rate 0.000101107\n",
      "2019-01-29T02:29:46.858463: step 3149, loss 0.0714569, acc 1, learning_rate 0.000101104\n",
      "2019-01-29T02:29:46.984155: step 3150, loss 0.0753082, acc 1, learning_rate 0.000101101\n",
      "2019-01-29T02:29:47.080962: step 3151, loss 0.11027, acc 0.984375, learning_rate 0.000101098\n",
      "2019-01-29T02:29:47.213309: step 3152, loss 0.0988217, acc 0.984375, learning_rate 0.000101095\n",
      "2019-01-29T02:29:47.344570: step 3153, loss 0.0678095, acc 1, learning_rate 0.000101092\n",
      "2019-01-29T02:29:47.472812: step 3154, loss 0.0770179, acc 0.96875, learning_rate 0.000101089\n",
      "2019-01-29T02:29:47.559905: step 3155, loss 0.0758465, acc 1, learning_rate 0.000101086\n",
      "2019-01-29T02:29:47.645388: step 3156, loss 0.0938577, acc 1, learning_rate 0.000101083\n",
      "2019-01-29T02:29:47.769978: step 3157, loss 0.0788367, acc 1, learning_rate 0.000101081\n",
      "2019-01-29T02:29:47.858413: step 3158, loss 0.0618347, acc 1, learning_rate 0.000101078\n",
      "2019-01-29T02:29:47.983009: step 3159, loss 0.0712319, acc 1, learning_rate 0.000101075\n",
      "2019-01-29T02:29:48.111049: step 3160, loss 0.098055, acc 0.984375, learning_rate 0.000101072\n",
      "2019-01-29T02:29:48.237588: step 3161, loss 0.0749728, acc 0.984375, learning_rate 0.000101069\n",
      "2019-01-29T02:29:48.363215: step 3162, loss 0.0678789, acc 1, learning_rate 0.000101066\n",
      "2019-01-29T02:29:48.491996: step 3163, loss 0.0818948, acc 1, learning_rate 0.000101063\n",
      "2019-01-29T02:29:48.624178: step 3164, loss 0.0698568, acc 1, learning_rate 0.000101061\n",
      "2019-01-29T02:29:48.754744: step 3165, loss 0.122759, acc 0.96875, learning_rate 0.000101058\n",
      "2019-01-29T02:29:48.887161: step 3166, loss 0.0687853, acc 1, learning_rate 0.000101055\n",
      "2019-01-29T02:29:49.011799: step 3167, loss 0.0657044, acc 1, learning_rate 0.000101052\n",
      "2019-01-29T02:29:49.135182: step 3168, loss 0.0758983, acc 1, learning_rate 0.000101049\n",
      "2019-01-29T02:29:49.261053: step 3169, loss 0.0812512, acc 1, learning_rate 0.000101047\n",
      "2019-01-29T02:29:49.392916: step 3170, loss 0.0683857, acc 1, learning_rate 0.000101044\n",
      "2019-01-29T02:29:49.519593: step 3171, loss 0.0788255, acc 1, learning_rate 0.000101041\n",
      "2019-01-29T02:29:49.647123: step 3172, loss 0.0562875, acc 1, learning_rate 0.000101038\n",
      "2019-01-29T02:29:49.776566: step 3173, loss 0.0821571, acc 1, learning_rate 0.000101035\n",
      "2019-01-29T02:29:49.905830: step 3174, loss 0.0730878, acc 1, learning_rate 0.000101033\n",
      "2019-01-29T02:29:50.034875: step 3175, loss 0.060412, acc 1, learning_rate 0.00010103\n",
      "2019-01-29T02:29:50.158838: step 3176, loss 0.066967, acc 1, learning_rate 0.000101027\n",
      "2019-01-29T02:29:50.285947: step 3177, loss 0.0672972, acc 1, learning_rate 0.000101024\n",
      "2019-01-29T02:29:50.375523: step 3178, loss 0.0859871, acc 0.984375, learning_rate 0.000101022\n",
      "2019-01-29T02:29:50.462269: step 3179, loss 0.06709, acc 1, learning_rate 0.000101019\n",
      "2019-01-29T02:29:50.550509: step 3180, loss 0.0798635, acc 1, learning_rate 0.000101016\n",
      "2019-01-29T02:29:50.676253: step 3181, loss 0.0734503, acc 1, learning_rate 0.000101014\n",
      "2019-01-29T02:29:50.758069: step 3182, loss 0.0626208, acc 1, learning_rate 0.000101011\n",
      "2019-01-29T02:29:50.884708: step 3183, loss 0.113314, acc 0.96875, learning_rate 0.000101008\n",
      "2019-01-29T02:29:51.010994: step 3184, loss 0.0699309, acc 1, learning_rate 0.000101005\n",
      "2019-01-29T02:29:51.141809: step 3185, loss 0.071726, acc 1, learning_rate 0.000101003\n",
      "2019-01-29T02:29:51.272835: step 3186, loss 0.0753478, acc 1, learning_rate 0.000101\n",
      "2019-01-29T02:29:51.399143: step 3187, loss 0.0794753, acc 1, learning_rate 0.000100997\n",
      "2019-01-29T02:29:51.487815: step 3188, loss 0.0937642, acc 0.984375, learning_rate 0.000100995\n",
      "2019-01-29T02:29:51.611390: step 3189, loss 0.0658252, acc 1, learning_rate 0.000100992\n",
      "2019-01-29T02:29:51.731396: step 3190, loss 0.0789886, acc 1, learning_rate 0.00010099\n",
      "2019-01-29T02:29:51.859248: step 3191, loss 0.0708068, acc 1, learning_rate 0.000100987\n",
      "2019-01-29T02:29:51.988813: step 3192, loss 0.0629093, acc 1, learning_rate 0.000100984\n",
      "2019-01-29T02:29:52.109474: step 3193, loss 0.0649698, acc 1, learning_rate 0.000100982\n",
      "2019-01-29T02:29:52.235878: step 3194, loss 0.0646004, acc 1, learning_rate 0.000100979\n",
      "2019-01-29T02:29:52.364683: step 3195, loss 0.0967591, acc 1, learning_rate 0.000100976\n",
      "2019-01-29T02:29:52.495614: step 3196, loss 0.0997128, acc 0.96875, learning_rate 0.000100974\n",
      "2019-01-29T02:29:52.585328: step 3197, loss 0.101267, acc 0.984375, learning_rate 0.000100971\n",
      "2019-01-29T02:29:52.706592: step 3198, loss 0.0697855, acc 1, learning_rate 0.000100969\n",
      "2019-01-29T02:29:52.830176: step 3199, loss 0.0682938, acc 1, learning_rate 0.000100966\n",
      "2019-01-29T02:29:52.954128: step 3200, loss 0.0771074, acc 1, learning_rate 0.000100963\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:29:52.979544: step 3200, loss 0.653971, acc 0.751407\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3200\n",
      "\n",
      "2019-01-29T02:29:53.324861: step 3201, loss 0.079341, acc 1, learning_rate 0.000100961\n",
      "2019-01-29T02:29:53.412875: step 3202, loss 0.079231, acc 1, learning_rate 0.000100958\n",
      "2019-01-29T02:29:53.534251: step 3203, loss 0.0743679, acc 1, learning_rate 0.000100956\n",
      "2019-01-29T02:29:53.669565: step 3204, loss 0.0630128, acc 1, learning_rate 0.000100953\n",
      "2019-01-29T02:29:53.760533: step 3205, loss 0.0627301, acc 1, learning_rate 0.000100951\n",
      "2019-01-29T02:29:53.889419: step 3206, loss 0.101922, acc 0.984375, learning_rate 0.000100948\n",
      "2019-01-29T02:29:54.016167: step 3207, loss 0.066738, acc 1, learning_rate 0.000100946\n",
      "2019-01-29T02:29:54.128521: step 3208, loss 0.0689095, acc 1, learning_rate 0.000100943\n",
      "2019-01-29T02:29:54.248431: step 3209, loss 0.0885022, acc 0.96875, learning_rate 0.000100941\n",
      "2019-01-29T02:29:54.376203: step 3210, loss 0.0811342, acc 1, learning_rate 0.000100938\n",
      "2019-01-29T02:29:54.497056: step 3211, loss 0.0719267, acc 0.984375, learning_rate 0.000100936\n",
      "2019-01-29T02:29:54.583210: step 3212, loss 0.0777803, acc 1, learning_rate 0.000100933\n",
      "2019-01-29T02:29:54.709335: step 3213, loss 0.108598, acc 0.984375, learning_rate 0.000100931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:29:54.834943: step 3214, loss 0.0741033, acc 1, learning_rate 0.000100928\n",
      "2019-01-29T02:29:54.961624: step 3215, loss 0.0824009, acc 1, learning_rate 0.000100926\n",
      "2019-01-29T02:29:55.089917: step 3216, loss 0.0638996, acc 1, learning_rate 0.000100923\n",
      "2019-01-29T02:29:55.179319: step 3217, loss 0.0713892, acc 1, learning_rate 0.000100921\n",
      "2019-01-29T02:29:55.303642: step 3218, loss 0.0784703, acc 0.984375, learning_rate 0.000100918\n",
      "2019-01-29T02:29:55.424457: step 3219, loss 0.0772903, acc 1, learning_rate 0.000100916\n",
      "2019-01-29T02:29:55.548630: step 3220, loss 0.0616828, acc 1, learning_rate 0.000100913\n",
      "2019-01-29T02:29:55.675267: step 3221, loss 0.0652833, acc 1, learning_rate 0.000100911\n",
      "2019-01-29T02:29:55.759966: step 3222, loss 0.0675105, acc 1, learning_rate 0.000100909\n",
      "2019-01-29T02:29:55.890019: step 3223, loss 0.0851911, acc 1, learning_rate 0.000100906\n",
      "2019-01-29T02:29:56.017016: step 3224, loss 0.0660482, acc 1, learning_rate 0.000100904\n",
      "2019-01-29T02:29:56.146106: step 3225, loss 0.0816578, acc 1, learning_rate 0.000100901\n",
      "2019-01-29T02:29:56.272552: step 3226, loss 0.0803273, acc 1, learning_rate 0.000100899\n",
      "2019-01-29T02:29:56.400609: step 3227, loss 0.067016, acc 1, learning_rate 0.000100896\n",
      "2019-01-29T02:29:56.519239: step 3228, loss 0.0678316, acc 1, learning_rate 0.000100894\n",
      "2019-01-29T02:29:56.641409: step 3229, loss 0.0675666, acc 1, learning_rate 0.000100892\n",
      "2019-01-29T02:29:56.767406: step 3230, loss 0.0843169, acc 0.984375, learning_rate 0.000100889\n",
      "2019-01-29T02:29:56.895900: step 3231, loss 0.0667492, acc 1, learning_rate 0.000100887\n",
      "2019-01-29T02:29:56.978917: step 3232, loss 0.0656061, acc 1, learning_rate 0.000100885\n",
      "2019-01-29T02:29:57.107492: step 3233, loss 0.108134, acc 0.984375, learning_rate 0.000100882\n",
      "2019-01-29T02:29:57.196608: step 3234, loss 0.0657343, acc 1, learning_rate 0.00010088\n",
      "2019-01-29T02:29:57.323326: step 3235, loss 0.0889469, acc 1, learning_rate 0.000100878\n",
      "2019-01-29T02:29:57.448511: step 3236, loss 0.0669862, acc 1, learning_rate 0.000100875\n",
      "2019-01-29T02:29:57.545906: step 3237, loss 0.0802206, acc 1, learning_rate 0.000100873\n",
      "2019-01-29T02:29:57.637293: step 3238, loss 0.0794229, acc 1, learning_rate 0.000100871\n",
      "2019-01-29T02:29:57.765560: step 3239, loss 0.0726367, acc 1, learning_rate 0.000100868\n",
      "2019-01-29T02:29:57.895149: step 3240, loss 0.0722084, acc 0.984375, learning_rate 0.000100866\n",
      "2019-01-29T02:29:58.023685: step 3241, loss 0.0773768, acc 1, learning_rate 0.000100864\n",
      "2019-01-29T02:29:58.113120: step 3242, loss 0.0638118, acc 1, learning_rate 0.000100861\n",
      "2019-01-29T02:29:58.232733: step 3243, loss 0.0612985, acc 1, learning_rate 0.000100859\n",
      "2019-01-29T02:29:58.359534: step 3244, loss 0.085044, acc 1, learning_rate 0.000100857\n",
      "2019-01-29T02:29:58.450020: step 3245, loss 0.0711829, acc 1, learning_rate 0.000100854\n",
      "2019-01-29T02:29:58.580825: step 3246, loss 0.0753659, acc 1, learning_rate 0.000100852\n",
      "2019-01-29T02:29:58.710471: step 3247, loss 0.0737854, acc 0.984375, learning_rate 0.00010085\n",
      "2019-01-29T02:29:58.837559: step 3248, loss 0.0748118, acc 0.984375, learning_rate 0.000100848\n",
      "2019-01-29T02:29:58.965653: step 3249, loss 0.0676086, acc 1, learning_rate 0.000100845\n",
      "2019-01-29T02:29:59.050086: step 3250, loss 0.0556298, acc 1, learning_rate 0.000100843\n",
      "2019-01-29T02:29:59.140490: step 3251, loss 0.0675628, acc 1, learning_rate 0.000100841\n",
      "2019-01-29T02:29:59.263164: step 3252, loss 0.0928952, acc 0.984375, learning_rate 0.000100839\n",
      "2019-01-29T02:29:59.383569: step 3253, loss 0.0743888, acc 1, learning_rate 0.000100836\n",
      "2019-01-29T02:29:59.514157: step 3254, loss 0.0752752, acc 1, learning_rate 0.000100834\n",
      "2019-01-29T02:29:59.635400: step 3255, loss 0.0769095, acc 1, learning_rate 0.000100832\n",
      "2019-01-29T02:29:59.719753: step 3256, loss 0.0713071, acc 1, learning_rate 0.00010083\n",
      "2019-01-29T02:29:59.842894: step 3257, loss 0.0685723, acc 1, learning_rate 0.000100828\n",
      "2019-01-29T02:29:59.963660: step 3258, loss 0.0613943, acc 1, learning_rate 0.000100825\n",
      "2019-01-29T02:30:00.091914: step 3259, loss 0.0710989, acc 1, learning_rate 0.000100823\n",
      "2019-01-29T02:30:00.217416: step 3260, loss 0.0609038, acc 1, learning_rate 0.000100821\n",
      "2019-01-29T02:30:00.344846: step 3261, loss 0.0597166, acc 1, learning_rate 0.000100819\n",
      "2019-01-29T02:30:00.473133: step 3262, loss 0.0777069, acc 1, learning_rate 0.000100817\n",
      "2019-01-29T02:30:00.605489: step 3263, loss 0.0687352, acc 1, learning_rate 0.000100814\n",
      "2019-01-29T02:30:00.724786: step 3264, loss 0.0606731, acc 1, learning_rate 0.000100812\n",
      "2019-01-29T02:30:00.847829: step 3265, loss 0.0679782, acc 1, learning_rate 0.00010081\n",
      "2019-01-29T02:30:00.979527: step 3266, loss 0.0898956, acc 0.984375, learning_rate 0.000100808\n",
      "2019-01-29T02:30:01.066866: step 3267, loss 0.0648014, acc 1, learning_rate 0.000100806\n",
      "2019-01-29T02:30:01.190917: step 3268, loss 0.0571509, acc 1, learning_rate 0.000100804\n",
      "2019-01-29T02:30:01.321406: step 3269, loss 0.0809643, acc 0.984375, learning_rate 0.000100801\n",
      "2019-01-29T02:30:01.443976: step 3270, loss 0.0722689, acc 1, learning_rate 0.000100799\n",
      "2019-01-29T02:30:01.574508: step 3271, loss 0.0604201, acc 1, learning_rate 0.000100797\n",
      "2019-01-29T02:30:01.694743: step 3272, loss 0.0805896, acc 1, learning_rate 0.000100795\n",
      "2019-01-29T02:30:01.819176: step 3273, loss 0.0724658, acc 1, learning_rate 0.000100793\n",
      "2019-01-29T02:30:01.950624: step 3274, loss 0.0679713, acc 1, learning_rate 0.000100791\n",
      "2019-01-29T02:30:02.080026: step 3275, loss 0.0793364, acc 1, learning_rate 0.000100789\n",
      "2019-01-29T02:30:02.171612: step 3276, loss 0.103632, acc 0.984375, learning_rate 0.000100787\n",
      "2019-01-29T02:30:02.295654: step 3277, loss 0.0674955, acc 1, learning_rate 0.000100785\n",
      "2019-01-29T02:30:02.397611: step 3278, loss 0.0894834, acc 1, learning_rate 0.000100782\n",
      "2019-01-29T02:30:02.487989: step 3279, loss 0.0582913, acc 1, learning_rate 0.00010078\n",
      "2019-01-29T02:30:02.615686: step 3280, loss 0.076407, acc 1, learning_rate 0.000100778\n",
      "2019-01-29T02:30:02.744084: step 3281, loss 0.0954705, acc 0.96875, learning_rate 0.000100776\n",
      "2019-01-29T02:30:02.865509: step 3282, loss 0.0951452, acc 0.96875, learning_rate 0.000100774\n",
      "2019-01-29T02:30:02.953546: step 3283, loss 0.105861, acc 0.984375, learning_rate 0.000100772\n",
      "2019-01-29T02:30:03.082654: step 3284, loss 0.0606944, acc 1, learning_rate 0.00010077\n",
      "2019-01-29T02:30:03.215411: step 3285, loss 0.0784246, acc 1, learning_rate 0.000100768\n",
      "2019-01-29T02:30:03.342597: step 3286, loss 0.124312, acc 0.96875, learning_rate 0.000100766\n",
      "2019-01-29T02:30:03.468532: step 3287, loss 0.0911348, acc 0.984375, learning_rate 0.000100764\n",
      "2019-01-29T02:30:03.556533: step 3288, loss 0.0718819, acc 1, learning_rate 0.000100762\n",
      "2019-01-29T02:30:03.677950: step 3289, loss 0.0730537, acc 1, learning_rate 0.00010076\n",
      "2019-01-29T02:30:03.806686: step 3290, loss 0.0636647, acc 1, learning_rate 0.000100758\n",
      "2019-01-29T02:30:03.935329: step 3291, loss 0.0719962, acc 1, learning_rate 0.000100756\n",
      "2019-01-29T02:30:04.065311: step 3292, loss 0.0834023, acc 0.984375, learning_rate 0.000100754\n",
      "2019-01-29T02:30:04.195992: step 3293, loss 0.0986224, acc 0.96875, learning_rate 0.000100752\n",
      "2019-01-29T02:30:04.314194: step 3294, loss 0.078982, acc 0.984375, learning_rate 0.00010075\n",
      "2019-01-29T02:30:04.442839: step 3295, loss 0.0618758, acc 1, learning_rate 0.000100748\n",
      "2019-01-29T02:30:04.573939: step 3296, loss 0.10537, acc 0.984375, learning_rate 0.000100746\n",
      "2019-01-29T02:30:04.697665: step 3297, loss 0.0835841, acc 1, learning_rate 0.000100744\n",
      "2019-01-29T02:30:04.818872: step 3298, loss 0.0844089, acc 1, learning_rate 0.000100742\n",
      "2019-01-29T02:30:04.948055: step 3299, loss 0.0818855, acc 1, learning_rate 0.00010074\n",
      "2019-01-29T02:30:05.078550: step 3300, loss 0.0725724, acc 1, learning_rate 0.000100738\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:30:05.106212: step 3300, loss 0.646694, acc 0.757036\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3300\n",
      "\n",
      "2019-01-29T02:30:05.439386: step 3301, loss 0.10793, acc 0.984375, learning_rate 0.000100736\n",
      "2019-01-29T02:30:05.563835: step 3302, loss 0.0791665, acc 1, learning_rate 0.000100734\n",
      "2019-01-29T02:30:05.692595: step 3303, loss 0.0714315, acc 1, learning_rate 0.000100732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:30:05.823305: step 3304, loss 0.0692965, acc 1, learning_rate 0.00010073\n",
      "2019-01-29T02:30:05.915358: step 3305, loss 0.0684397, acc 1, learning_rate 0.000100728\n",
      "2019-01-29T02:30:06.047634: step 3306, loss 0.0846537, acc 1, learning_rate 0.000100726\n",
      "2019-01-29T02:30:06.173744: step 3307, loss 0.101651, acc 0.96875, learning_rate 0.000100724\n",
      "2019-01-29T02:30:06.301397: step 3308, loss 0.064761, acc 1, learning_rate 0.000100722\n",
      "2019-01-29T02:30:06.424756: step 3309, loss 0.0793496, acc 0.984375, learning_rate 0.00010072\n",
      "2019-01-29T02:30:06.552313: step 3310, loss 0.0860316, acc 0.984375, learning_rate 0.000100718\n",
      "2019-01-29T02:30:06.676058: step 3311, loss 0.0651826, acc 1, learning_rate 0.000100717\n",
      "2019-01-29T02:30:06.802639: step 3312, loss 0.0726228, acc 1, learning_rate 0.000100715\n",
      "2019-01-29T02:30:06.925643: step 3313, loss 0.0722482, acc 1, learning_rate 0.000100713\n",
      "2019-01-29T02:30:07.053121: step 3314, loss 0.0764969, acc 1, learning_rate 0.000100711\n",
      "2019-01-29T02:30:07.176302: step 3315, loss 0.0756328, acc 1, learning_rate 0.000100709\n",
      "2019-01-29T02:30:07.259091: step 3316, loss 0.0852324, acc 1, learning_rate 0.000100707\n",
      "2019-01-29T02:30:07.347925: step 3317, loss 0.0708983, acc 1, learning_rate 0.000100705\n",
      "2019-01-29T02:30:07.464886: step 3318, loss 0.0668741, acc 0.984375, learning_rate 0.000100703\n",
      "2019-01-29T02:30:07.589446: step 3319, loss 0.074787, acc 1, learning_rate 0.000100701\n",
      "2019-01-29T02:30:07.676689: step 3320, loss 0.0901486, acc 0.984375, learning_rate 0.0001007\n",
      "2019-01-29T02:30:07.795338: step 3321, loss 0.0795357, acc 1, learning_rate 0.000100698\n",
      "2019-01-29T02:30:07.921193: step 3322, loss 0.0692473, acc 1, learning_rate 0.000100696\n",
      "2019-01-29T02:30:08.010319: step 3323, loss 0.0782993, acc 1, learning_rate 0.000100694\n",
      "2019-01-29T02:30:08.131141: step 3324, loss 0.0659187, acc 1, learning_rate 0.000100692\n",
      "2019-01-29T02:30:08.250032: step 3325, loss 0.0795689, acc 1, learning_rate 0.00010069\n",
      "2019-01-29T02:30:08.378284: step 3326, loss 0.0542263, acc 1, learning_rate 0.000100688\n",
      "2019-01-29T02:30:08.501092: step 3327, loss 0.0703619, acc 1, learning_rate 0.000100687\n",
      "2019-01-29T02:30:08.629015: step 3328, loss 0.0755465, acc 1, learning_rate 0.000100685\n",
      "2019-01-29T02:30:08.752126: step 3329, loss 0.061058, acc 1, learning_rate 0.000100683\n",
      "2019-01-29T02:30:08.878226: step 3330, loss 0.0617644, acc 1, learning_rate 0.000100681\n",
      "2019-01-29T02:30:08.966147: step 3331, loss 0.0803886, acc 1, learning_rate 0.000100679\n",
      "2019-01-29T02:30:09.097375: step 3332, loss 0.0608661, acc 1, learning_rate 0.000100677\n",
      "2019-01-29T02:30:09.220845: step 3333, loss 0.063376, acc 1, learning_rate 0.000100676\n",
      "2019-01-29T02:30:09.345825: step 3334, loss 0.0692789, acc 1, learning_rate 0.000100674\n",
      "2019-01-29T02:30:09.473238: step 3335, loss 0.0900998, acc 1, learning_rate 0.000100672\n",
      "2019-01-29T02:30:09.560991: step 3336, loss 0.0708349, acc 1, learning_rate 0.00010067\n",
      "2019-01-29T02:30:09.685684: step 3337, loss 0.064188, acc 1, learning_rate 0.000100669\n",
      "2019-01-29T02:30:09.810103: step 3338, loss 0.0802596, acc 0.984375, learning_rate 0.000100667\n",
      "2019-01-29T02:30:09.932088: step 3339, loss 0.0640855, acc 1, learning_rate 0.000100665\n",
      "2019-01-29T02:30:10.052721: step 3340, loss 0.0698286, acc 1, learning_rate 0.000100663\n",
      "2019-01-29T02:30:10.178764: step 3341, loss 0.0600144, acc 1, learning_rate 0.000100661\n",
      "2019-01-29T02:30:10.305373: step 3342, loss 0.0839529, acc 0.984375, learning_rate 0.00010066\n",
      "2019-01-29T02:30:10.433046: step 3343, loss 0.0721838, acc 1, learning_rate 0.000100658\n",
      "2019-01-29T02:30:10.559298: step 3344, loss 0.0675974, acc 1, learning_rate 0.000100656\n",
      "2019-01-29T02:30:10.644566: step 3345, loss 0.093397, acc 0.984375, learning_rate 0.000100654\n",
      "2019-01-29T02:30:10.763487: step 3346, loss 0.069514, acc 1, learning_rate 0.000100653\n",
      "2019-01-29T02:30:10.886278: step 3347, loss 0.107363, acc 0.984375, learning_rate 0.000100651\n",
      "2019-01-29T02:30:11.015149: step 3348, loss 0.0757862, acc 1, learning_rate 0.000100649\n",
      "2019-01-29T02:30:11.139789: step 3349, loss 0.0597511, acc 1, learning_rate 0.000100647\n",
      "2019-01-29T02:30:11.270846: step 3350, loss 0.077041, acc 1, learning_rate 0.000100646\n",
      "2019-01-29T02:30:11.401235: step 3351, loss 0.0930298, acc 0.984375, learning_rate 0.000100644\n",
      "2019-01-29T02:30:11.527497: step 3352, loss 0.0955929, acc 0.984375, learning_rate 0.000100642\n",
      "2019-01-29T02:30:11.654052: step 3353, loss 0.0636798, acc 1, learning_rate 0.000100641\n",
      "2019-01-29T02:30:11.785124: step 3354, loss 0.101258, acc 0.984375, learning_rate 0.000100639\n",
      "2019-01-29T02:30:11.875440: step 3355, loss 0.0652563, acc 1, learning_rate 0.000100637\n",
      "2019-01-29T02:30:11.996374: step 3356, loss 0.0767607, acc 0.984375, learning_rate 0.000100635\n",
      "2019-01-29T02:30:12.085703: step 3357, loss 0.064621, acc 1, learning_rate 0.000100634\n",
      "2019-01-29T02:30:12.210059: step 3358, loss 0.0807296, acc 1, learning_rate 0.000100632\n",
      "2019-01-29T02:30:12.336103: step 3359, loss 0.081092, acc 0.984375, learning_rate 0.00010063\n",
      "2019-01-29T02:30:12.458346: step 3360, loss 0.0574694, acc 1, learning_rate 0.000100629\n",
      "2019-01-29T02:30:12.582277: step 3361, loss 0.0751711, acc 1, learning_rate 0.000100627\n",
      "2019-01-29T02:30:12.713025: step 3362, loss 0.0946732, acc 0.984375, learning_rate 0.000100625\n",
      "2019-01-29T02:30:12.839842: step 3363, loss 0.063113, acc 1, learning_rate 0.000100624\n",
      "2019-01-29T02:30:12.965701: step 3364, loss 0.0924587, acc 0.984375, learning_rate 0.000100622\n",
      "2019-01-29T02:30:13.088802: step 3365, loss 0.0753415, acc 1, learning_rate 0.00010062\n",
      "2019-01-29T02:30:13.218358: step 3366, loss 0.0852512, acc 1, learning_rate 0.000100619\n",
      "2019-01-29T02:30:13.340900: step 3367, loss 0.0738879, acc 1, learning_rate 0.000100617\n",
      "2019-01-29T02:30:13.469460: step 3368, loss 0.0680482, acc 1, learning_rate 0.000100615\n",
      "2019-01-29T02:30:13.596884: step 3369, loss 0.0632622, acc 1, learning_rate 0.000100614\n",
      "2019-01-29T02:30:13.729578: step 3370, loss 0.072037, acc 1, learning_rate 0.000100612\n",
      "2019-01-29T02:30:13.812381: step 3371, loss 0.0838926, acc 1, learning_rate 0.000100611\n",
      "2019-01-29T02:30:13.911147: step 3372, loss 0.104761, acc 0.984375, learning_rate 0.000100609\n",
      "2019-01-29T02:30:14.034102: step 3373, loss 0.0736351, acc 1, learning_rate 0.000100607\n",
      "2019-01-29T02:30:14.121734: step 3374, loss 0.0732045, acc 1, learning_rate 0.000100606\n",
      "2019-01-29T02:30:14.253627: step 3375, loss 0.0764643, acc 0.984375, learning_rate 0.000100604\n",
      "2019-01-29T02:30:14.377160: step 3376, loss 0.0775998, acc 1, learning_rate 0.000100602\n",
      "2019-01-29T02:30:14.500503: step 3377, loss 0.0735662, acc 1, learning_rate 0.000100601\n",
      "2019-01-29T02:30:14.625059: step 3378, loss 0.0763913, acc 1, learning_rate 0.000100599\n",
      "2019-01-29T02:30:14.745836: step 3379, loss 0.0687118, acc 0.984375, learning_rate 0.000100598\n",
      "2019-01-29T02:30:14.877548: step 3380, loss 0.0657175, acc 1, learning_rate 0.000100596\n",
      "2019-01-29T02:30:14.999562: step 3381, loss 0.0751657, acc 1, learning_rate 0.000100594\n",
      "2019-01-29T02:30:15.124091: step 3382, loss 0.0789912, acc 1, learning_rate 0.000100593\n",
      "2019-01-29T02:30:15.250655: step 3383, loss 0.0822339, acc 1, learning_rate 0.000100591\n",
      "2019-01-29T02:30:15.378679: step 3384, loss 0.0718072, acc 1, learning_rate 0.00010059\n",
      "2019-01-29T02:30:15.500657: step 3385, loss 0.0674288, acc 1, learning_rate 0.000100588\n",
      "2019-01-29T02:30:15.631579: step 3386, loss 0.0993483, acc 0.984375, learning_rate 0.000100587\n",
      "2019-01-29T02:30:15.752094: step 3387, loss 0.0843333, acc 1, learning_rate 0.000100585\n",
      "2019-01-29T02:30:15.878341: step 3388, loss 0.083839, acc 1, learning_rate 0.000100583\n",
      "2019-01-29T02:30:16.003796: step 3389, loss 0.0741441, acc 1, learning_rate 0.000100582\n",
      "2019-01-29T02:30:16.125338: step 3390, loss 0.0773392, acc 1, learning_rate 0.00010058\n",
      "2019-01-29T02:30:16.252186: step 3391, loss 0.0794645, acc 1, learning_rate 0.000100579\n",
      "2019-01-29T02:30:16.353413: step 3392, loss 0.0701921, acc 1, learning_rate 0.000100577\n",
      "2019-01-29T02:30:16.479455: step 3393, loss 0.0922645, acc 0.984375, learning_rate 0.000100576\n",
      "2019-01-29T02:30:16.562881: step 3394, loss 0.0772907, acc 1, learning_rate 0.000100574\n",
      "2019-01-29T02:30:16.692681: step 3395, loss 0.0786088, acc 1, learning_rate 0.000100573\n",
      "2019-01-29T02:30:16.819013: step 3396, loss 0.0783673, acc 0.984375, learning_rate 0.000100571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:30:16.945284: step 3397, loss 0.064765, acc 1, learning_rate 0.00010057\n",
      "2019-01-29T02:30:17.029460: step 3398, loss 0.0625181, acc 1, learning_rate 0.000100568\n",
      "2019-01-29T02:30:17.154669: step 3399, loss 0.0929775, acc 0.984375, learning_rate 0.000100567\n",
      "2019-01-29T02:30:17.275727: step 3400, loss 0.0545687, acc 1, learning_rate 0.000100565\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:30:17.303523: step 3400, loss 0.660021, acc 0.751407\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3400\n",
      "\n",
      "2019-01-29T02:30:17.608535: step 3401, loss 0.0692659, acc 1, learning_rate 0.000100564\n",
      "2019-01-29T02:30:17.728308: step 3402, loss 0.0752147, acc 1, learning_rate 0.000100562\n",
      "2019-01-29T02:30:17.853996: step 3403, loss 0.0626597, acc 1, learning_rate 0.000100561\n",
      "2019-01-29T02:30:17.979383: step 3404, loss 0.0736326, acc 1, learning_rate 0.000100559\n",
      "2019-01-29T02:30:18.110136: step 3405, loss 0.0818303, acc 1, learning_rate 0.000100558\n",
      "2019-01-29T02:30:18.241763: step 3406, loss 0.0781752, acc 1, learning_rate 0.000100556\n",
      "2019-01-29T02:30:18.366609: step 3407, loss 0.0584299, acc 1, learning_rate 0.000100555\n",
      "2019-01-29T02:30:18.485369: step 3408, loss 0.0546505, acc 1, learning_rate 0.000100553\n",
      "2019-01-29T02:30:18.612993: step 3409, loss 0.0929844, acc 0.984375, learning_rate 0.000100552\n",
      "2019-01-29T02:30:18.737981: step 3410, loss 0.0736748, acc 1, learning_rate 0.00010055\n",
      "2019-01-29T02:30:18.864434: step 3411, loss 0.0823667, acc 1, learning_rate 0.000100549\n",
      "2019-01-29T02:30:18.985919: step 3412, loss 0.0860738, acc 0.984375, learning_rate 0.000100547\n",
      "2019-01-29T02:30:19.115357: step 3413, loss 0.0847104, acc 1, learning_rate 0.000100546\n",
      "2019-01-29T02:30:19.231897: step 3414, loss 0.0602143, acc 1, learning_rate 0.000100544\n",
      "2019-01-29T02:30:19.361669: step 3415, loss 0.101733, acc 0.984375, learning_rate 0.000100543\n",
      "2019-01-29T02:30:19.482936: step 3416, loss 0.0863074, acc 1, learning_rate 0.000100541\n",
      "2019-01-29T02:30:19.614012: step 3417, loss 0.0601616, acc 1, learning_rate 0.00010054\n",
      "2019-01-29T02:30:19.740677: step 3418, loss 0.0747227, acc 0.984375, learning_rate 0.000100539\n",
      "2019-01-29T02:30:19.863070: step 3419, loss 0.087777, acc 1, learning_rate 0.000100537\n",
      "2019-01-29T02:30:19.985276: step 3420, loss 0.0662918, acc 1, learning_rate 0.000100536\n",
      "2019-01-29T02:30:20.115324: step 3421, loss 0.067702, acc 1, learning_rate 0.000100534\n",
      "2019-01-29T02:30:20.233628: step 3422, loss 0.0688472, acc 1, learning_rate 0.000100533\n",
      "2019-01-29T02:30:20.365794: step 3423, loss 0.0763623, acc 1, learning_rate 0.000100531\n",
      "2019-01-29T02:30:20.454085: step 3424, loss 0.0517849, acc 1, learning_rate 0.00010053\n",
      "2019-01-29T02:30:20.570622: step 3425, loss 0.0757015, acc 0.984375, learning_rate 0.000100529\n",
      "2019-01-29T02:30:20.693928: step 3426, loss 0.0744076, acc 1, learning_rate 0.000100527\n",
      "2019-01-29T02:30:20.778039: step 3427, loss 0.0954359, acc 0.984375, learning_rate 0.000100526\n",
      "2019-01-29T02:30:20.896525: step 3428, loss 0.0684653, acc 1, learning_rate 0.000100524\n",
      "2019-01-29T02:30:21.025077: step 3429, loss 0.0624796, acc 1, learning_rate 0.000100523\n",
      "2019-01-29T02:30:21.150117: step 3430, loss 0.0701685, acc 1, learning_rate 0.000100522\n",
      "2019-01-29T02:30:21.275942: step 3431, loss 0.0663016, acc 1, learning_rate 0.00010052\n",
      "2019-01-29T02:30:21.404143: step 3432, loss 0.0828696, acc 1, learning_rate 0.000100519\n",
      "2019-01-29T02:30:21.497189: step 3433, loss 0.090712, acc 0.984375, learning_rate 0.000100517\n",
      "2019-01-29T02:30:21.627197: step 3434, loss 0.0775508, acc 1, learning_rate 0.000100516\n",
      "2019-01-29T02:30:21.749342: step 3435, loss 0.0593753, acc 1, learning_rate 0.000100515\n",
      "2019-01-29T02:30:21.878340: step 3436, loss 0.0687139, acc 1, learning_rate 0.000100513\n",
      "2019-01-29T02:30:22.007745: step 3437, loss 0.0588346, acc 1, learning_rate 0.000100512\n",
      "2019-01-29T02:30:22.099004: step 3438, loss 0.0876343, acc 0.984375, learning_rate 0.000100511\n",
      "2019-01-29T02:30:22.228713: step 3439, loss 0.0921921, acc 1, learning_rate 0.000100509\n",
      "2019-01-29T02:30:22.355945: step 3440, loss 0.060616, acc 1, learning_rate 0.000100508\n",
      "2019-01-29T02:30:22.487305: step 3441, loss 0.0711868, acc 1, learning_rate 0.000100507\n",
      "2019-01-29T02:30:22.619528: step 3442, loss 0.0666425, acc 1, learning_rate 0.000100505\n",
      "2019-01-29T02:30:22.737266: step 3443, loss 0.106581, acc 0.984375, learning_rate 0.000100504\n",
      "2019-01-29T02:30:22.860546: step 3444, loss 0.0606877, acc 1, learning_rate 0.000100502\n",
      "2019-01-29T02:30:22.949188: step 3445, loss 0.0623115, acc 1, learning_rate 0.000100501\n",
      "2019-01-29T02:30:23.083599: step 3446, loss 0.0827645, acc 1, learning_rate 0.0001005\n",
      "2019-01-29T02:30:23.211258: step 3447, loss 0.0988643, acc 0.96875, learning_rate 0.000100498\n",
      "2019-01-29T02:30:23.333326: step 3448, loss 0.0736201, acc 1, learning_rate 0.000100497\n",
      "2019-01-29T02:30:23.460620: step 3449, loss 0.0748553, acc 1, learning_rate 0.000100496\n",
      "2019-01-29T02:30:23.587449: step 3450, loss 0.0519357, acc 1, learning_rate 0.000100495\n",
      "2019-01-29T02:30:23.709765: step 3451, loss 0.09972, acc 1, learning_rate 0.000100493\n",
      "2019-01-29T02:30:23.837380: step 3452, loss 0.0663465, acc 1, learning_rate 0.000100492\n",
      "2019-01-29T02:30:23.968139: step 3453, loss 0.0692042, acc 1, learning_rate 0.000100491\n",
      "2019-01-29T02:30:24.103895: step 3454, loss 0.0756116, acc 1, learning_rate 0.000100489\n",
      "2019-01-29T02:30:24.238502: step 3455, loss 0.0693228, acc 1, learning_rate 0.000100488\n",
      "2019-01-29T02:30:24.365281: step 3456, loss 0.0959806, acc 0.984375, learning_rate 0.000100487\n",
      "2019-01-29T02:30:24.491450: step 3457, loss 0.0668414, acc 1, learning_rate 0.000100485\n",
      "2019-01-29T02:30:24.614596: step 3458, loss 0.0777978, acc 0.984375, learning_rate 0.000100484\n",
      "2019-01-29T02:30:24.741598: step 3459, loss 0.0742503, acc 1, learning_rate 0.000100483\n",
      "2019-01-29T02:30:24.825429: step 3460, loss 0.105826, acc 0.96875, learning_rate 0.000100481\n",
      "2019-01-29T02:30:24.950687: step 3461, loss 0.0929062, acc 0.984375, learning_rate 0.00010048\n",
      "2019-01-29T02:30:25.073850: step 3462, loss 0.0711894, acc 0.984375, learning_rate 0.000100479\n",
      "2019-01-29T02:30:25.162022: step 3463, loss 0.0582532, acc 1, learning_rate 0.000100478\n",
      "2019-01-29T02:30:25.287329: step 3464, loss 0.0615783, acc 1, learning_rate 0.000100476\n",
      "2019-01-29T02:30:25.409264: step 3465, loss 0.0612198, acc 1, learning_rate 0.000100475\n",
      "2019-01-29T02:30:25.540675: step 3466, loss 0.0770474, acc 1, learning_rate 0.000100474\n",
      "2019-01-29T02:30:25.666856: step 3467, loss 0.0662116, acc 1, learning_rate 0.000100473\n",
      "2019-01-29T02:30:25.792188: step 3468, loss 0.0552798, acc 1, learning_rate 0.000100471\n",
      "2019-01-29T02:30:25.882698: step 3469, loss 0.0782882, acc 0.984375, learning_rate 0.00010047\n",
      "2019-01-29T02:30:26.009071: step 3470, loss 0.0646345, acc 1, learning_rate 0.000100469\n",
      "2019-01-29T02:30:26.099886: step 3471, loss 0.0807305, acc 1, learning_rate 0.000100468\n",
      "2019-01-29T02:30:26.229554: step 3472, loss 0.0959402, acc 0.984375, learning_rate 0.000100466\n",
      "2019-01-29T02:30:26.356688: step 3473, loss 0.0709427, acc 1, learning_rate 0.000100465\n",
      "2019-01-29T02:30:26.488167: step 3474, loss 0.0669837, acc 1, learning_rate 0.000100464\n",
      "2019-01-29T02:30:26.614245: step 3475, loss 0.0712028, acc 1, learning_rate 0.000100463\n",
      "2019-01-29T02:30:26.736581: step 3476, loss 0.0780652, acc 1, learning_rate 0.000100461\n",
      "2019-01-29T02:30:26.859004: step 3477, loss 0.0643836, acc 1, learning_rate 0.00010046\n",
      "2019-01-29T02:30:26.984657: step 3478, loss 0.0775414, acc 1, learning_rate 0.000100459\n",
      "2019-01-29T02:30:27.110002: step 3479, loss 0.0660874, acc 1, learning_rate 0.000100458\n",
      "2019-01-29T02:30:27.200381: step 3480, loss 0.0663199, acc 1, learning_rate 0.000100456\n",
      "2019-01-29T02:30:27.327973: step 3481, loss 0.0577913, acc 1, learning_rate 0.000100455\n",
      "2019-01-29T02:30:27.458187: step 3482, loss 0.0726003, acc 1, learning_rate 0.000100454\n",
      "2019-01-29T02:30:27.579961: step 3483, loss 0.0692383, acc 1, learning_rate 0.000100453\n",
      "2019-01-29T02:30:27.702845: step 3484, loss 0.0735844, acc 1, learning_rate 0.000100452\n",
      "2019-01-29T02:30:27.831178: step 3485, loss 0.0847352, acc 0.984375, learning_rate 0.00010045\n",
      "2019-01-29T02:30:27.958994: step 3486, loss 0.0682504, acc 1, learning_rate 0.000100449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:30:28.083679: step 3487, loss 0.0601009, acc 1, learning_rate 0.000100448\n",
      "2019-01-29T02:30:28.206978: step 3488, loss 0.0747666, acc 1, learning_rate 0.000100447\n",
      "2019-01-29T02:30:28.291290: step 3489, loss 0.0584214, acc 1, learning_rate 0.000100446\n",
      "2019-01-29T02:30:28.420992: step 3490, loss 0.0872511, acc 0.984375, learning_rate 0.000100444\n",
      "2019-01-29T02:30:28.550836: step 3491, loss 0.0669967, acc 1, learning_rate 0.000100443\n",
      "2019-01-29T02:30:28.645783: step 3492, loss 0.0897326, acc 0.984375, learning_rate 0.000100442\n",
      "2019-01-29T02:30:28.762296: step 3493, loss 0.0799756, acc 0.984375, learning_rate 0.000100441\n",
      "2019-01-29T02:30:28.894031: step 3494, loss 0.0751134, acc 1, learning_rate 0.00010044\n",
      "2019-01-29T02:30:28.983902: step 3495, loss 0.082262, acc 0.984375, learning_rate 0.000100439\n",
      "2019-01-29T02:30:29.107766: step 3496, loss 0.061521, acc 1, learning_rate 0.000100437\n",
      "2019-01-29T02:30:29.231942: step 3497, loss 0.0858182, acc 1, learning_rate 0.000100436\n",
      "2019-01-29T02:30:29.363401: step 3498, loss 0.0668643, acc 1, learning_rate 0.000100435\n",
      "2019-01-29T02:30:29.461581: step 3499, loss 0.0737146, acc 0.984375, learning_rate 0.000100434\n",
      "2019-01-29T02:30:29.587197: step 3500, loss 0.0830189, acc 1, learning_rate 0.000100433\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:30:29.614144: step 3500, loss 0.652431, acc 0.755159\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3500\n",
      "\n",
      "2019-01-29T02:30:29.963191: step 3501, loss 0.0612017, acc 1, learning_rate 0.000100432\n",
      "2019-01-29T02:30:30.084249: step 3502, loss 0.0658928, acc 1, learning_rate 0.00010043\n",
      "2019-01-29T02:30:30.210672: step 3503, loss 0.0691835, acc 1, learning_rate 0.000100429\n",
      "2019-01-29T02:30:30.333453: step 3504, loss 0.0711914, acc 1, learning_rate 0.000100428\n",
      "2019-01-29T02:30:30.423443: step 3505, loss 0.070917, acc 0.984375, learning_rate 0.000100427\n",
      "2019-01-29T02:30:30.549621: step 3506, loss 0.0921938, acc 0.984375, learning_rate 0.000100426\n",
      "2019-01-29T02:30:30.675043: step 3507, loss 0.0644696, acc 1, learning_rate 0.000100425\n",
      "2019-01-29T02:30:30.802164: step 3508, loss 0.0578747, acc 1, learning_rate 0.000100424\n",
      "2019-01-29T02:30:30.926088: step 3509, loss 0.0916125, acc 0.984375, learning_rate 0.000100422\n",
      "2019-01-29T02:30:31.055881: step 3510, loss 0.0730993, acc 1, learning_rate 0.000100421\n",
      "2019-01-29T02:30:31.183376: step 3511, loss 0.0619139, acc 1, learning_rate 0.00010042\n",
      "2019-01-29T02:30:31.270733: step 3512, loss 0.0722058, acc 1, learning_rate 0.000100419\n",
      "2019-01-29T02:30:31.393798: step 3513, loss 0.0977434, acc 0.984375, learning_rate 0.000100418\n",
      "2019-01-29T02:30:31.484190: step 3514, loss 0.0638297, acc 1, learning_rate 0.000100417\n",
      "2019-01-29T02:30:31.611533: step 3515, loss 0.0699838, acc 1, learning_rate 0.000100416\n",
      "2019-01-29T02:30:31.734425: step 3516, loss 0.0930026, acc 0.984375, learning_rate 0.000100415\n",
      "2019-01-29T02:30:31.861440: step 3517, loss 0.0740789, acc 1, learning_rate 0.000100414\n",
      "2019-01-29T02:30:31.986379: step 3518, loss 0.0877006, acc 1, learning_rate 0.000100412\n",
      "2019-01-29T02:30:32.115131: step 3519, loss 0.0661365, acc 1, learning_rate 0.000100411\n",
      "2019-01-29T02:30:32.208347: step 3520, loss 0.0775041, acc 1, learning_rate 0.00010041\n",
      "2019-01-29T02:30:32.298330: step 3521, loss 0.100496, acc 1, learning_rate 0.000100409\n",
      "2019-01-29T02:30:32.422113: step 3522, loss 0.106115, acc 0.953125, learning_rate 0.000100408\n",
      "2019-01-29T02:30:32.547610: step 3523, loss 0.0858239, acc 1, learning_rate 0.000100407\n",
      "2019-01-29T02:30:32.670364: step 3524, loss 0.0626442, acc 1, learning_rate 0.000100406\n",
      "2019-01-29T02:30:32.786916: step 3525, loss 0.0594379, acc 1, learning_rate 0.000100405\n",
      "2019-01-29T02:30:32.909170: step 3526, loss 0.0744703, acc 1, learning_rate 0.000100404\n",
      "2019-01-29T02:30:33.042700: step 3527, loss 0.0743537, acc 1, learning_rate 0.000100403\n",
      "2019-01-29T02:30:33.170152: step 3528, loss 0.07005, acc 1, learning_rate 0.000100402\n",
      "2019-01-29T02:30:33.289142: step 3529, loss 0.0839575, acc 0.984375, learning_rate 0.000100401\n",
      "2019-01-29T02:30:33.406827: step 3530, loss 0.0639371, acc 1, learning_rate 0.000100399\n",
      "2019-01-29T02:30:33.535250: step 3531, loss 0.0827798, acc 0.984375, learning_rate 0.000100398\n",
      "2019-01-29T02:30:33.619032: step 3532, loss 0.0757231, acc 1, learning_rate 0.000100397\n",
      "2019-01-29T02:30:33.748127: step 3533, loss 0.0549999, acc 1, learning_rate 0.000100396\n",
      "2019-01-29T02:30:33.838224: step 3534, loss 0.0714696, acc 1, learning_rate 0.000100395\n",
      "2019-01-29T02:30:33.962822: step 3535, loss 0.0704707, acc 1, learning_rate 0.000100394\n",
      "2019-01-29T02:30:34.057628: step 3536, loss 0.0710346, acc 1, learning_rate 0.000100393\n",
      "2019-01-29T02:30:34.180691: step 3537, loss 0.0738232, acc 1, learning_rate 0.000100392\n",
      "2019-01-29T02:30:34.301910: step 3538, loss 0.0580775, acc 1, learning_rate 0.000100391\n",
      "2019-01-29T02:30:34.389373: step 3539, loss 0.0742437, acc 1, learning_rate 0.00010039\n",
      "2019-01-29T02:30:34.517294: step 3540, loss 0.0967105, acc 1, learning_rate 0.000100389\n",
      "2019-01-29T02:30:34.642631: step 3541, loss 0.0662532, acc 1, learning_rate 0.000100388\n",
      "2019-01-29T02:30:34.765523: step 3542, loss 0.0724947, acc 1, learning_rate 0.000100387\n",
      "2019-01-29T02:30:34.896461: step 3543, loss 0.0718569, acc 1, learning_rate 0.000100386\n",
      "2019-01-29T02:30:35.015488: step 3544, loss 0.0956414, acc 0.984375, learning_rate 0.000100385\n",
      "2019-01-29T02:30:35.143766: step 3545, loss 0.0908345, acc 0.96875, learning_rate 0.000100384\n",
      "2019-01-29T02:30:35.271431: step 3546, loss 0.06592, acc 1, learning_rate 0.000100383\n",
      "2019-01-29T02:30:35.395508: step 3547, loss 0.0866572, acc 0.984375, learning_rate 0.000100382\n",
      "2019-01-29T02:30:35.523853: step 3548, loss 0.0737533, acc 1, learning_rate 0.000100381\n",
      "2019-01-29T02:30:35.611345: step 3549, loss 0.0850295, acc 0.984375, learning_rate 0.00010038\n",
      "2019-01-29T02:30:35.733926: step 3550, loss 0.0591625, acc 1, learning_rate 0.000100379\n",
      "2019-01-29T02:30:35.855280: step 3551, loss 0.0755679, acc 1, learning_rate 0.000100378\n",
      "2019-01-29T02:30:35.956656: step 3552, loss 0.0539548, acc 1, learning_rate 0.000100377\n",
      "2019-01-29T02:30:36.056414: step 3553, loss 0.0718147, acc 0.984375, learning_rate 0.000100376\n",
      "2019-01-29T02:30:36.179100: step 3554, loss 0.0687867, acc 1, learning_rate 0.000100375\n",
      "2019-01-29T02:30:36.306042: step 3555, loss 0.0750799, acc 1, learning_rate 0.000100374\n",
      "2019-01-29T02:30:36.389590: step 3556, loss 0.0626478, acc 1, learning_rate 0.000100373\n",
      "2019-01-29T02:30:36.516330: step 3557, loss 0.0998618, acc 0.984375, learning_rate 0.000100372\n",
      "2019-01-29T02:30:36.645305: step 3558, loss 0.0658381, acc 1, learning_rate 0.000100371\n",
      "2019-01-29T02:30:36.770159: step 3559, loss 0.0691321, acc 1, learning_rate 0.00010037\n",
      "2019-01-29T02:30:36.874399: step 3560, loss 0.0720285, acc 1, learning_rate 0.000100369\n",
      "2019-01-29T02:30:36.996077: step 3561, loss 0.0640492, acc 1, learning_rate 0.000100368\n",
      "2019-01-29T02:30:37.121509: step 3562, loss 0.061425, acc 1, learning_rate 0.000100367\n",
      "2019-01-29T02:30:37.247928: step 3563, loss 0.059625, acc 1, learning_rate 0.000100366\n",
      "2019-01-29T02:30:37.338011: step 3564, loss 0.0723717, acc 1, learning_rate 0.000100365\n",
      "2019-01-29T02:30:37.459791: step 3565, loss 0.0730113, acc 0.984375, learning_rate 0.000100364\n",
      "2019-01-29T02:30:37.550248: step 3566, loss 0.060649, acc 1, learning_rate 0.000100363\n",
      "2019-01-29T02:30:37.673103: step 3567, loss 0.0524992, acc 1, learning_rate 0.000100362\n",
      "2019-01-29T02:30:37.762190: step 3568, loss 0.0598883, acc 1, learning_rate 0.000100361\n",
      "2019-01-29T02:30:37.890710: step 3569, loss 0.082913, acc 0.984375, learning_rate 0.00010036\n",
      "2019-01-29T02:30:38.018312: step 3570, loss 0.0733053, acc 1, learning_rate 0.000100359\n",
      "2019-01-29T02:30:38.142452: step 3571, loss 0.0722106, acc 1, learning_rate 0.000100358\n",
      "2019-01-29T02:30:38.272950: step 3572, loss 0.0957439, acc 0.96875, learning_rate 0.000100357\n",
      "2019-01-29T02:30:38.401398: step 3573, loss 0.0716381, acc 0.984375, learning_rate 0.000100356\n",
      "2019-01-29T02:30:38.520281: step 3574, loss 0.0673824, acc 1, learning_rate 0.000100355\n",
      "2019-01-29T02:30:38.646830: step 3575, loss 0.0719907, acc 1, learning_rate 0.000100354\n",
      "2019-01-29T02:30:38.769432: step 3576, loss 0.0689895, acc 1, learning_rate 0.000100353\n",
      "2019-01-29T02:30:38.886107: step 3577, loss 0.100299, acc 0.984375, learning_rate 0.000100352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:30:39.012476: step 3578, loss 0.0893078, acc 0.984375, learning_rate 0.000100351\n",
      "2019-01-29T02:30:39.134058: step 3579, loss 0.0621321, acc 1, learning_rate 0.000100351\n",
      "2019-01-29T02:30:39.259282: step 3580, loss 0.074917, acc 1, learning_rate 0.00010035\n",
      "2019-01-29T02:30:39.389093: step 3581, loss 0.0917674, acc 0.984375, learning_rate 0.000100349\n",
      "2019-01-29T02:30:39.520873: step 3582, loss 0.0803795, acc 1, learning_rate 0.000100348\n",
      "2019-01-29T02:30:39.611534: step 3583, loss 0.0587811, acc 1, learning_rate 0.000100347\n",
      "2019-01-29T02:30:39.739392: step 3584, loss 0.0694741, acc 1, learning_rate 0.000100346\n",
      "2019-01-29T02:30:39.868944: step 3585, loss 0.0854149, acc 0.984375, learning_rate 0.000100345\n",
      "2019-01-29T02:30:39.995520: step 3586, loss 0.0526695, acc 1, learning_rate 0.000100344\n",
      "2019-01-29T02:30:40.083383: step 3587, loss 0.073571, acc 1, learning_rate 0.000100343\n",
      "2019-01-29T02:30:40.208659: step 3588, loss 0.0806111, acc 0.984375, learning_rate 0.000100342\n",
      "2019-01-29T02:30:40.296392: step 3589, loss 0.088591, acc 0.984375, learning_rate 0.000100341\n",
      "2019-01-29T02:30:40.423426: step 3590, loss 0.0685079, acc 1, learning_rate 0.00010034\n",
      "2019-01-29T02:30:40.546118: step 3591, loss 0.0630742, acc 1, learning_rate 0.000100339\n",
      "2019-01-29T02:30:40.677425: step 3592, loss 0.0738149, acc 1, learning_rate 0.000100339\n",
      "2019-01-29T02:30:40.799807: step 3593, loss 0.0988824, acc 0.984375, learning_rate 0.000100338\n",
      "2019-01-29T02:30:40.886704: step 3594, loss 0.089858, acc 1, learning_rate 0.000100337\n",
      "2019-01-29T02:30:41.007885: step 3595, loss 0.0767816, acc 1, learning_rate 0.000100336\n",
      "2019-01-29T02:30:41.088924: step 3596, loss 0.05495, acc 1, learning_rate 0.000100335\n",
      "2019-01-29T02:30:41.215493: step 3597, loss 0.088595, acc 0.984375, learning_rate 0.000100334\n",
      "2019-01-29T02:30:41.302901: step 3598, loss 0.066349, acc 1, learning_rate 0.000100333\n",
      "2019-01-29T02:30:41.401174: step 3599, loss 0.0744355, acc 1, learning_rate 0.000100332\n",
      "2019-01-29T02:30:41.524929: step 3600, loss 0.0604321, acc 1, learning_rate 0.000100331\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:30:41.552820: step 3600, loss 0.650691, acc 0.757036\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3600\n",
      "\n",
      "2019-01-29T02:30:41.899616: step 3601, loss 0.0752778, acc 1, learning_rate 0.000100331\n",
      "2019-01-29T02:30:41.986426: step 3602, loss 0.0812624, acc 1, learning_rate 0.00010033\n",
      "2019-01-29T02:30:42.075976: step 3603, loss 0.0808265, acc 0.984375, learning_rate 0.000100329\n",
      "2019-01-29T02:30:42.206893: step 3604, loss 0.0739331, acc 1, learning_rate 0.000100328\n",
      "2019-01-29T02:30:42.310739: step 3605, loss 0.059924, acc 1, learning_rate 0.000100327\n",
      "2019-01-29T02:30:42.442277: step 3606, loss 0.0606282, acc 1, learning_rate 0.000100326\n",
      "2019-01-29T02:30:42.565729: step 3607, loss 0.0685882, acc 1, learning_rate 0.000100325\n",
      "2019-01-29T02:30:42.690851: step 3608, loss 0.0570269, acc 1, learning_rate 0.000100324\n",
      "2019-01-29T02:30:42.816169: step 3609, loss 0.0786017, acc 0.984375, learning_rate 0.000100324\n",
      "2019-01-29T02:30:42.940037: step 3610, loss 0.0506386, acc 1, learning_rate 0.000100323\n",
      "2019-01-29T02:30:43.062874: step 3611, loss 0.0695499, acc 1, learning_rate 0.000100322\n",
      "2019-01-29T02:30:43.148735: step 3612, loss 0.0795201, acc 1, learning_rate 0.000100321\n",
      "2019-01-29T02:30:43.272240: step 3613, loss 0.068858, acc 1, learning_rate 0.00010032\n",
      "2019-01-29T02:30:43.398511: step 3614, loss 0.100392, acc 0.984375, learning_rate 0.000100319\n",
      "2019-01-29T02:30:43.520611: step 3615, loss 0.0686058, acc 1, learning_rate 0.000100318\n",
      "2019-01-29T02:30:43.606660: step 3616, loss 0.0819858, acc 0.984375, learning_rate 0.000100318\n",
      "2019-01-29T02:30:43.730490: step 3617, loss 0.0797503, acc 0.984375, learning_rate 0.000100317\n",
      "2019-01-29T02:30:43.815193: step 3618, loss 0.0726039, acc 1, learning_rate 0.000100316\n",
      "2019-01-29T02:30:43.941709: step 3619, loss 0.056662, acc 1, learning_rate 0.000100315\n",
      "2019-01-29T02:30:44.069858: step 3620, loss 0.07333, acc 1, learning_rate 0.000100314\n",
      "2019-01-29T02:30:44.194073: step 3621, loss 0.0702047, acc 1, learning_rate 0.000100313\n",
      "2019-01-29T02:30:44.323760: step 3622, loss 0.0587826, acc 1, learning_rate 0.000100313\n",
      "2019-01-29T02:30:44.404619: step 3623, loss 0.0686594, acc 1, learning_rate 0.000100312\n",
      "2019-01-29T02:30:44.533692: step 3624, loss 0.0848582, acc 0.984375, learning_rate 0.000100311\n",
      "2019-01-29T02:30:44.661143: step 3625, loss 0.0766617, acc 1, learning_rate 0.00010031\n",
      "2019-01-29T02:30:44.785956: step 3626, loss 0.0718053, acc 1, learning_rate 0.000100309\n",
      "2019-01-29T02:30:44.906494: step 3627, loss 0.0769112, acc 1, learning_rate 0.000100308\n",
      "2019-01-29T02:30:45.033623: step 3628, loss 0.073515, acc 1, learning_rate 0.000100308\n",
      "2019-01-29T02:30:45.155601: step 3629, loss 0.0880615, acc 0.984375, learning_rate 0.000100307\n",
      "2019-01-29T02:30:45.279443: step 3630, loss 0.0541812, acc 1, learning_rate 0.000100306\n",
      "2019-01-29T02:30:45.406860: step 3631, loss 0.0708091, acc 1, learning_rate 0.000100305\n",
      "2019-01-29T02:30:45.527542: step 3632, loss 0.0691375, acc 1, learning_rate 0.000100304\n",
      "2019-01-29T02:30:45.622923: step 3633, loss 0.074778, acc 1, learning_rate 0.000100303\n",
      "2019-01-29T02:30:45.746376: step 3634, loss 0.0577345, acc 1, learning_rate 0.000100303\n",
      "2019-01-29T02:30:45.829082: step 3635, loss 0.0609142, acc 1, learning_rate 0.000100302\n",
      "2019-01-29T02:30:45.951092: step 3636, loss 0.0581091, acc 1, learning_rate 0.000100301\n",
      "2019-01-29T02:30:46.041819: step 3637, loss 0.0759651, acc 0.984375, learning_rate 0.0001003\n",
      "2019-01-29T02:30:46.162380: step 3638, loss 0.0675579, acc 1, learning_rate 0.000100299\n",
      "2019-01-29T02:30:46.288591: step 3639, loss 0.0617029, acc 1, learning_rate 0.000100299\n",
      "2019-01-29T02:30:46.412441: step 3640, loss 0.0812321, acc 1, learning_rate 0.000100298\n",
      "2019-01-29T02:30:46.534591: step 3641, loss 0.0680594, acc 1, learning_rate 0.000100297\n",
      "2019-01-29T02:30:46.619851: step 3642, loss 0.0729747, acc 1, learning_rate 0.000100296\n",
      "2019-01-29T02:30:46.746401: step 3643, loss 0.0979175, acc 0.984375, learning_rate 0.000100296\n",
      "2019-01-29T02:30:46.877647: step 3644, loss 0.0649255, acc 1, learning_rate 0.000100295\n",
      "2019-01-29T02:30:47.007896: step 3645, loss 0.0686645, acc 1, learning_rate 0.000100294\n",
      "2019-01-29T02:30:47.134857: step 3646, loss 0.0907918, acc 1, learning_rate 0.000100293\n",
      "2019-01-29T02:30:47.266607: step 3647, loss 0.0707836, acc 1, learning_rate 0.000100292\n",
      "2019-01-29T02:30:47.354948: step 3648, loss 0.0778552, acc 1, learning_rate 0.000100292\n",
      "2019-01-29T02:30:47.478650: step 3649, loss 0.0661395, acc 1, learning_rate 0.000100291\n",
      "2019-01-29T02:30:47.606101: step 3650, loss 0.065676, acc 1, learning_rate 0.00010029\n",
      "2019-01-29T02:30:47.738375: step 3651, loss 0.0799734, acc 0.984375, learning_rate 0.000100289\n",
      "2019-01-29T02:30:47.860863: step 3652, loss 0.0757835, acc 1, learning_rate 0.000100288\n",
      "2019-01-29T02:30:47.987877: step 3653, loss 0.0597406, acc 1, learning_rate 0.000100288\n",
      "2019-01-29T02:30:48.111791: step 3654, loss 0.0690081, acc 1, learning_rate 0.000100287\n",
      "2019-01-29T02:30:48.241725: step 3655, loss 0.0734556, acc 1, learning_rate 0.000100286\n",
      "2019-01-29T02:30:48.365166: step 3656, loss 0.0614474, acc 1, learning_rate 0.000100285\n",
      "2019-01-29T02:30:48.492568: step 3657, loss 0.0847839, acc 1, learning_rate 0.000100285\n",
      "2019-01-29T02:30:48.621010: step 3658, loss 0.0655593, acc 1, learning_rate 0.000100284\n",
      "2019-01-29T02:30:48.742600: step 3659, loss 0.0691215, acc 1, learning_rate 0.000100283\n",
      "2019-01-29T02:30:48.860383: step 3660, loss 0.0678601, acc 1, learning_rate 0.000100282\n",
      "2019-01-29T02:30:48.990419: step 3661, loss 0.082694, acc 0.984375, learning_rate 0.000100282\n",
      "2019-01-29T02:30:49.112775: step 3662, loss 0.0570096, acc 1, learning_rate 0.000100281\n",
      "2019-01-29T02:30:49.202732: step 3663, loss 0.0710899, acc 1, learning_rate 0.00010028\n",
      "2019-01-29T02:30:49.290064: step 3664, loss 0.0848957, acc 1, learning_rate 0.000100279\n",
      "2019-01-29T02:30:49.422645: step 3665, loss 0.0530629, acc 1, learning_rate 0.000100279\n",
      "2019-01-29T02:30:49.514197: step 3666, loss 0.0753783, acc 1, learning_rate 0.000100278\n",
      "2019-01-29T02:30:49.641468: step 3667, loss 0.070305, acc 1, learning_rate 0.000100277\n",
      "2019-01-29T02:30:49.736549: step 3668, loss 0.0684681, acc 1, learning_rate 0.000100276\n",
      "2019-01-29T02:30:49.827320: step 3669, loss 0.0585591, acc 1, learning_rate 0.000100276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:30:49.914881: step 3670, loss 0.0761194, acc 0.984375, learning_rate 0.000100275\n",
      "2019-01-29T02:30:50.004697: step 3671, loss 0.0549093, acc 1, learning_rate 0.000100274\n",
      "2019-01-29T02:30:50.100474: step 3672, loss 0.0728268, acc 0.984375, learning_rate 0.000100274\n",
      "2019-01-29T02:30:50.217436: step 3673, loss 0.101546, acc 1, learning_rate 0.000100273\n",
      "2019-01-29T02:30:50.299445: step 3674, loss 0.0586047, acc 1, learning_rate 0.000100272\n",
      "2019-01-29T02:30:50.432802: step 3675, loss 0.0703092, acc 0.984375, learning_rate 0.000100271\n",
      "2019-01-29T02:30:50.560015: step 3676, loss 0.082075, acc 0.984375, learning_rate 0.000100271\n",
      "2019-01-29T02:30:50.646450: step 3677, loss 0.0583905, acc 1, learning_rate 0.00010027\n",
      "2019-01-29T02:30:50.740021: step 3678, loss 0.0802094, acc 1, learning_rate 0.000100269\n",
      "2019-01-29T02:30:50.864117: step 3679, loss 0.128308, acc 0.96875, learning_rate 0.000100268\n",
      "2019-01-29T02:30:50.969065: step 3680, loss 0.0630977, acc 1, learning_rate 0.000100268\n",
      "2019-01-29T02:30:51.098429: step 3681, loss 0.0877532, acc 0.984375, learning_rate 0.000100267\n",
      "2019-01-29T02:30:51.227177: step 3682, loss 0.0868903, acc 0.984375, learning_rate 0.000100266\n",
      "2019-01-29T02:30:51.355010: step 3683, loss 0.0563245, acc 1, learning_rate 0.000100266\n",
      "2019-01-29T02:30:51.482457: step 3684, loss 0.0823506, acc 0.984375, learning_rate 0.000100265\n",
      "2019-01-29T02:30:51.577091: step 3685, loss 0.0561171, acc 1, learning_rate 0.000100264\n",
      "2019-01-29T02:30:51.701232: step 3686, loss 0.0817695, acc 1, learning_rate 0.000100263\n",
      "2019-01-29T02:30:51.819278: step 3687, loss 0.0584357, acc 1, learning_rate 0.000100263\n",
      "2019-01-29T02:30:51.945499: step 3688, loss 0.073557, acc 1, learning_rate 0.000100262\n",
      "2019-01-29T02:30:52.073963: step 3689, loss 0.0581491, acc 1, learning_rate 0.000100261\n",
      "2019-01-29T02:30:52.204860: step 3690, loss 0.0764889, acc 1, learning_rate 0.000100261\n",
      "2019-01-29T02:30:52.323489: step 3691, loss 0.0660447, acc 1, learning_rate 0.00010026\n",
      "2019-01-29T02:30:52.448553: step 3692, loss 0.0952115, acc 0.984375, learning_rate 0.000100259\n",
      "2019-01-29T02:30:52.572635: step 3693, loss 0.0759332, acc 1, learning_rate 0.000100259\n",
      "2019-01-29T02:30:52.698799: step 3694, loss 0.0680577, acc 1, learning_rate 0.000100258\n",
      "2019-01-29T02:30:52.820363: step 3695, loss 0.0672304, acc 1, learning_rate 0.000100257\n",
      "2019-01-29T02:30:52.945833: step 3696, loss 0.0559797, acc 1, learning_rate 0.000100257\n",
      "2019-01-29T02:30:53.075545: step 3697, loss 0.0617751, acc 1, learning_rate 0.000100256\n",
      "2019-01-29T02:30:53.199008: step 3698, loss 0.080425, acc 1, learning_rate 0.000100255\n",
      "2019-01-29T02:30:53.320920: step 3699, loss 0.0889438, acc 1, learning_rate 0.000100255\n",
      "2019-01-29T02:30:53.443874: step 3700, loss 0.0775707, acc 1, learning_rate 0.000100254\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:30:53.470940: step 3700, loss 0.654341, acc 0.751407\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3700\n",
      "\n",
      "2019-01-29T02:30:53.815286: step 3701, loss 0.0750006, acc 1, learning_rate 0.000100253\n",
      "2019-01-29T02:30:53.936844: step 3702, loss 0.0723864, acc 1, learning_rate 0.000100252\n",
      "2019-01-29T02:30:54.024849: step 3703, loss 0.0815253, acc 1, learning_rate 0.000100252\n",
      "2019-01-29T02:30:54.148813: step 3704, loss 0.0943279, acc 0.984375, learning_rate 0.000100251\n",
      "2019-01-29T02:30:54.268567: step 3705, loss 0.0830115, acc 0.984375, learning_rate 0.00010025\n",
      "2019-01-29T02:30:54.354213: step 3706, loss 0.0627552, acc 1, learning_rate 0.00010025\n",
      "2019-01-29T02:30:54.471885: step 3707, loss 0.0578722, acc 1, learning_rate 0.000100249\n",
      "2019-01-29T02:30:54.602281: step 3708, loss 0.100815, acc 0.984375, learning_rate 0.000100248\n",
      "2019-01-29T02:30:54.691439: step 3709, loss 0.0594805, acc 1, learning_rate 0.000100248\n",
      "2019-01-29T02:30:54.804466: step 3710, loss 0.0592228, acc 1, learning_rate 0.000100247\n",
      "2019-01-29T02:30:54.927011: step 3711, loss 0.0667467, acc 1, learning_rate 0.000100246\n",
      "2019-01-29T02:30:55.053774: step 3712, loss 0.0615716, acc 1, learning_rate 0.000100246\n",
      "2019-01-29T02:30:55.185470: step 3713, loss 0.0821211, acc 0.984375, learning_rate 0.000100245\n",
      "2019-01-29T02:30:55.310565: step 3714, loss 0.0819568, acc 0.984375, learning_rate 0.000100245\n",
      "2019-01-29T02:30:55.433809: step 3715, loss 0.0743742, acc 1, learning_rate 0.000100244\n",
      "2019-01-29T02:30:55.560307: step 3716, loss 0.0645924, acc 1, learning_rate 0.000100243\n",
      "2019-01-29T02:30:55.679158: step 3717, loss 0.0756537, acc 1, learning_rate 0.000100243\n",
      "2019-01-29T02:30:55.803654: step 3718, loss 0.0735094, acc 0.984375, learning_rate 0.000100242\n",
      "2019-01-29T02:30:55.936230: step 3719, loss 0.0805408, acc 1, learning_rate 0.000100241\n",
      "2019-01-29T02:30:56.023795: step 3720, loss 0.0658039, acc 1, learning_rate 0.000100241\n",
      "2019-01-29T02:30:56.152299: step 3721, loss 0.0776679, acc 0.984375, learning_rate 0.00010024\n",
      "2019-01-29T02:30:56.275512: step 3722, loss 0.0873788, acc 0.984375, learning_rate 0.000100239\n",
      "2019-01-29T02:30:56.399475: step 3723, loss 0.082479, acc 1, learning_rate 0.000100239\n",
      "2019-01-29T02:30:56.525486: step 3724, loss 0.0687354, acc 1, learning_rate 0.000100238\n",
      "2019-01-29T02:30:56.653466: step 3725, loss 0.078128, acc 1, learning_rate 0.000100237\n",
      "2019-01-29T02:30:56.778685: step 3726, loss 0.0578349, acc 1, learning_rate 0.000100237\n",
      "2019-01-29T02:30:56.903349: step 3727, loss 0.0543589, acc 1, learning_rate 0.000100236\n",
      "2019-01-29T02:30:57.030442: step 3728, loss 0.0650114, acc 1, learning_rate 0.000100236\n",
      "2019-01-29T02:30:57.157477: step 3729, loss 0.0959683, acc 0.96875, learning_rate 0.000100235\n",
      "2019-01-29T02:30:57.284635: step 3730, loss 0.089934, acc 0.984375, learning_rate 0.000100234\n",
      "2019-01-29T02:30:57.375601: step 3731, loss 0.0751944, acc 1, learning_rate 0.000100234\n",
      "2019-01-29T02:30:57.500068: step 3732, loss 0.0718488, acc 1, learning_rate 0.000100233\n",
      "2019-01-29T02:30:57.587400: step 3733, loss 0.0665862, acc 1, learning_rate 0.000100232\n",
      "2019-01-29T02:30:57.714935: step 3734, loss 0.0804679, acc 0.984375, learning_rate 0.000100232\n",
      "2019-01-29T02:30:57.842453: step 3735, loss 0.0671586, acc 1, learning_rate 0.000100231\n",
      "2019-01-29T02:30:57.963727: step 3736, loss 0.0722522, acc 1, learning_rate 0.000100231\n",
      "2019-01-29T02:30:58.095150: step 3737, loss 0.0592076, acc 1, learning_rate 0.00010023\n",
      "2019-01-29T02:30:58.214204: step 3738, loss 0.0838026, acc 0.984375, learning_rate 0.000100229\n",
      "2019-01-29T02:30:58.345903: step 3739, loss 0.0605999, acc 1, learning_rate 0.000100229\n",
      "2019-01-29T02:30:58.475286: step 3740, loss 0.058981, acc 1, learning_rate 0.000100228\n",
      "2019-01-29T02:30:58.605424: step 3741, loss 0.0625102, acc 1, learning_rate 0.000100228\n",
      "2019-01-29T02:30:58.736472: step 3742, loss 0.0765491, acc 0.984375, learning_rate 0.000100227\n",
      "2019-01-29T02:30:58.865776: step 3743, loss 0.085324, acc 0.984375, learning_rate 0.000100226\n",
      "2019-01-29T02:30:58.951223: step 3744, loss 0.0711505, acc 1, learning_rate 0.000100226\n",
      "2019-01-29T02:30:59.035473: step 3745, loss 0.0742198, acc 1, learning_rate 0.000100225\n",
      "2019-01-29T02:30:59.154577: step 3746, loss 0.0700822, acc 1, learning_rate 0.000100225\n",
      "2019-01-29T02:30:59.281357: step 3747, loss 0.0694004, acc 1, learning_rate 0.000100224\n",
      "2019-01-29T02:30:59.369142: step 3748, loss 0.0522383, acc 1, learning_rate 0.000100223\n",
      "2019-01-29T02:30:59.495590: step 3749, loss 0.0722713, acc 1, learning_rate 0.000100223\n",
      "2019-01-29T02:30:59.615848: step 3750, loss 0.0917135, acc 0.983333, learning_rate 0.000100222\n",
      "2019-01-29T02:30:59.705534: step 3751, loss 0.0826276, acc 1, learning_rate 0.000100222\n",
      "2019-01-29T02:30:59.794058: step 3752, loss 0.0623566, acc 1, learning_rate 0.000100221\n",
      "2019-01-29T02:30:59.882136: step 3753, loss 0.0551911, acc 1, learning_rate 0.00010022\n",
      "2019-01-29T02:31:00.012067: step 3754, loss 0.0670632, acc 1, learning_rate 0.00010022\n",
      "2019-01-29T02:31:00.136999: step 3755, loss 0.0723141, acc 1, learning_rate 0.000100219\n",
      "2019-01-29T02:31:00.227425: step 3756, loss 0.0741403, acc 0.984375, learning_rate 0.000100219\n",
      "2019-01-29T02:31:00.316995: step 3757, loss 0.0711436, acc 1, learning_rate 0.000100218\n",
      "2019-01-29T02:31:00.406823: step 3758, loss 0.140607, acc 0.953125, learning_rate 0.000100217\n",
      "2019-01-29T02:31:00.496340: step 3759, loss 0.0755531, acc 1, learning_rate 0.000100217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:31:00.623788: step 3760, loss 0.0913965, acc 0.984375, learning_rate 0.000100216\n",
      "2019-01-29T02:31:00.746865: step 3761, loss 0.050444, acc 1, learning_rate 0.000100216\n",
      "2019-01-29T02:31:00.837096: step 3762, loss 0.0719101, acc 1, learning_rate 0.000100215\n",
      "2019-01-29T02:31:00.956615: step 3763, loss 0.0651081, acc 1, learning_rate 0.000100215\n",
      "2019-01-29T02:31:01.080867: step 3764, loss 0.0575094, acc 1, learning_rate 0.000100214\n",
      "2019-01-29T02:31:01.212165: step 3765, loss 0.0741316, acc 1, learning_rate 0.000100213\n",
      "2019-01-29T02:31:01.292628: step 3766, loss 0.0659236, acc 1, learning_rate 0.000100213\n",
      "2019-01-29T02:31:01.383179: step 3767, loss 0.0916103, acc 1, learning_rate 0.000100212\n",
      "2019-01-29T02:31:01.469244: step 3768, loss 0.0768007, acc 1, learning_rate 0.000100212\n",
      "2019-01-29T02:31:01.557696: step 3769, loss 0.0611304, acc 1, learning_rate 0.000100211\n",
      "2019-01-29T02:31:01.683333: step 3770, loss 0.103079, acc 0.984375, learning_rate 0.000100211\n",
      "2019-01-29T02:31:01.771831: step 3771, loss 0.049825, acc 1, learning_rate 0.00010021\n",
      "2019-01-29T02:31:01.897355: step 3772, loss 0.0652903, acc 1, learning_rate 0.000100209\n",
      "2019-01-29T02:31:02.021485: step 3773, loss 0.0670455, acc 1, learning_rate 0.000100209\n",
      "2019-01-29T02:31:02.107004: step 3774, loss 0.0656938, acc 1, learning_rate 0.000100208\n",
      "2019-01-29T02:31:02.222280: step 3775, loss 0.0695614, acc 1, learning_rate 0.000100208\n",
      "2019-01-29T02:31:02.349323: step 3776, loss 0.0804838, acc 0.984375, learning_rate 0.000100207\n",
      "2019-01-29T02:31:02.477226: step 3777, loss 0.0597973, acc 1, learning_rate 0.000100207\n",
      "2019-01-29T02:31:02.601631: step 3778, loss 0.0629814, acc 1, learning_rate 0.000100206\n",
      "2019-01-29T02:31:02.690073: step 3779, loss 0.072053, acc 1, learning_rate 0.000100206\n",
      "2019-01-29T02:31:02.813878: step 3780, loss 0.0686911, acc 1, learning_rate 0.000100205\n",
      "2019-01-29T02:31:02.930659: step 3781, loss 0.0896295, acc 0.96875, learning_rate 0.000100204\n",
      "2019-01-29T02:31:03.053014: step 3782, loss 0.0652121, acc 1, learning_rate 0.000100204\n",
      "2019-01-29T02:31:03.179197: step 3783, loss 0.0668408, acc 1, learning_rate 0.000100203\n",
      "2019-01-29T02:31:03.310953: step 3784, loss 0.0818616, acc 1, learning_rate 0.000100203\n",
      "2019-01-29T02:31:03.437958: step 3785, loss 0.0968594, acc 0.984375, learning_rate 0.000100202\n",
      "2019-01-29T02:31:03.562050: step 3786, loss 0.0619698, acc 1, learning_rate 0.000100202\n",
      "2019-01-29T02:31:03.689444: step 3787, loss 0.0911359, acc 0.984375, learning_rate 0.000100201\n",
      "2019-01-29T02:31:03.775486: step 3788, loss 0.082205, acc 1, learning_rate 0.000100201\n",
      "2019-01-29T02:31:03.899979: step 3789, loss 0.0603465, acc 1, learning_rate 0.0001002\n",
      "2019-01-29T02:31:04.027501: step 3790, loss 0.0593896, acc 1, learning_rate 0.0001002\n",
      "2019-01-29T02:31:04.158187: step 3791, loss 0.0550423, acc 1, learning_rate 0.000100199\n",
      "2019-01-29T02:31:04.285201: step 3792, loss 0.0701255, acc 1, learning_rate 0.000100199\n",
      "2019-01-29T02:31:04.374348: step 3793, loss 0.0683663, acc 1, learning_rate 0.000100198\n",
      "2019-01-29T02:31:04.499416: step 3794, loss 0.0661201, acc 1, learning_rate 0.000100198\n",
      "2019-01-29T02:31:04.624033: step 3795, loss 0.0619664, acc 1, learning_rate 0.000100197\n",
      "2019-01-29T02:31:04.743902: step 3796, loss 0.0610957, acc 1, learning_rate 0.000100196\n",
      "2019-01-29T02:31:04.852767: step 3797, loss 0.131892, acc 0.984375, learning_rate 0.000100196\n",
      "2019-01-29T02:31:04.979699: step 3798, loss 0.0722533, acc 1, learning_rate 0.000100195\n",
      "2019-01-29T02:31:05.061307: step 3799, loss 0.07579, acc 1, learning_rate 0.000100195\n",
      "2019-01-29T02:31:05.187100: step 3800, loss 0.0524743, acc 1, learning_rate 0.000100194\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:31:05.214544: step 3800, loss 0.655644, acc 0.753283\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3800\n",
      "\n",
      "2019-01-29T02:31:05.493186: step 3801, loss 0.0762576, acc 1, learning_rate 0.000100194\n",
      "2019-01-29T02:31:05.621519: step 3802, loss 0.0816006, acc 1, learning_rate 0.000100193\n",
      "2019-01-29T02:31:05.748452: step 3803, loss 0.0661801, acc 1, learning_rate 0.000100193\n",
      "2019-01-29T02:31:05.879436: step 3804, loss 0.0716389, acc 1, learning_rate 0.000100192\n",
      "2019-01-29T02:31:06.007358: step 3805, loss 0.0840671, acc 0.984375, learning_rate 0.000100192\n",
      "2019-01-29T02:31:06.136934: step 3806, loss 0.0722454, acc 1, learning_rate 0.000100191\n",
      "2019-01-29T02:31:06.265146: step 3807, loss 0.0763811, acc 1, learning_rate 0.000100191\n",
      "2019-01-29T02:31:06.382007: step 3808, loss 0.059529, acc 1, learning_rate 0.00010019\n",
      "2019-01-29T02:31:06.506424: step 3809, loss 0.0624293, acc 1, learning_rate 0.00010019\n",
      "2019-01-29T02:31:06.631048: step 3810, loss 0.063491, acc 1, learning_rate 0.000100189\n",
      "2019-01-29T02:31:06.756552: step 3811, loss 0.0626303, acc 1, learning_rate 0.000100189\n",
      "2019-01-29T02:31:06.840614: step 3812, loss 0.0575438, acc 1, learning_rate 0.000100188\n",
      "2019-01-29T02:31:06.924896: step 3813, loss 0.0600308, acc 1, learning_rate 0.000100188\n",
      "2019-01-29T02:31:07.050844: step 3814, loss 0.116133, acc 0.96875, learning_rate 0.000100187\n",
      "2019-01-29T02:31:07.176984: step 3815, loss 0.0807695, acc 1, learning_rate 0.000100187\n",
      "2019-01-29T02:31:07.306294: step 3816, loss 0.0573571, acc 1, learning_rate 0.000100186\n",
      "2019-01-29T02:31:07.429728: step 3817, loss 0.0869889, acc 0.984375, learning_rate 0.000100186\n",
      "2019-01-29T02:31:07.547016: step 3818, loss 0.0662378, acc 1, learning_rate 0.000100185\n",
      "2019-01-29T02:31:07.677361: step 3819, loss 0.0619124, acc 1, learning_rate 0.000100185\n",
      "2019-01-29T02:31:07.802953: step 3820, loss 0.0902712, acc 0.984375, learning_rate 0.000100184\n",
      "2019-01-29T02:31:07.934955: step 3821, loss 0.0678489, acc 1, learning_rate 0.000100184\n",
      "2019-01-29T02:31:08.066088: step 3822, loss 0.0696097, acc 1, learning_rate 0.000100183\n",
      "2019-01-29T02:31:08.193624: step 3823, loss 0.0652173, acc 1, learning_rate 0.000100183\n",
      "2019-01-29T02:31:08.316674: step 3824, loss 0.0647329, acc 1, learning_rate 0.000100182\n",
      "2019-01-29T02:31:08.444660: step 3825, loss 0.0629907, acc 1, learning_rate 0.000100182\n",
      "2019-01-29T02:31:08.573769: step 3826, loss 0.0898593, acc 0.984375, learning_rate 0.000100181\n",
      "2019-01-29T02:31:08.694859: step 3827, loss 0.0850764, acc 0.984375, learning_rate 0.000100181\n",
      "2019-01-29T02:31:08.819712: step 3828, loss 0.0633159, acc 1, learning_rate 0.00010018\n",
      "2019-01-29T02:31:08.907960: step 3829, loss 0.0662435, acc 1, learning_rate 0.00010018\n",
      "2019-01-29T02:31:09.035054: step 3830, loss 0.0642537, acc 1, learning_rate 0.000100179\n",
      "2019-01-29T02:31:09.156341: step 3831, loss 0.0755934, acc 1, learning_rate 0.000100179\n",
      "2019-01-29T02:31:09.286825: step 3832, loss 0.0622743, acc 1, learning_rate 0.000100178\n",
      "2019-01-29T02:31:09.415206: step 3833, loss 0.0736769, acc 1, learning_rate 0.000100178\n",
      "2019-01-29T02:31:09.532975: step 3834, loss 0.051034, acc 1, learning_rate 0.000100178\n",
      "2019-01-29T02:31:09.621671: step 3835, loss 0.0602846, acc 1, learning_rate 0.000100177\n",
      "2019-01-29T02:31:09.710856: step 3836, loss 0.0623601, acc 1, learning_rate 0.000100177\n",
      "2019-01-29T02:31:09.800394: step 3837, loss 0.0701146, acc 1, learning_rate 0.000100176\n",
      "2019-01-29T02:31:09.885492: step 3838, loss 0.0630001, acc 1, learning_rate 0.000100176\n",
      "2019-01-29T02:31:10.006668: step 3839, loss 0.0947665, acc 0.984375, learning_rate 0.000100175\n",
      "2019-01-29T02:31:10.138883: step 3840, loss 0.081577, acc 1, learning_rate 0.000100175\n",
      "2019-01-29T02:31:10.264555: step 3841, loss 0.0729629, acc 1, learning_rate 0.000100174\n",
      "2019-01-29T02:31:10.396019: step 3842, loss 0.0867013, acc 0.984375, learning_rate 0.000100174\n",
      "2019-01-29T02:31:10.518208: step 3843, loss 0.0612904, acc 1, learning_rate 0.000100173\n",
      "2019-01-29T02:31:10.649619: step 3844, loss 0.0758105, acc 1, learning_rate 0.000100173\n",
      "2019-01-29T02:31:10.777893: step 3845, loss 0.0690521, acc 1, learning_rate 0.000100172\n",
      "2019-01-29T02:31:10.908764: step 3846, loss 0.0902757, acc 0.984375, learning_rate 0.000100172\n",
      "2019-01-29T02:31:11.039456: step 3847, loss 0.0789211, acc 1, learning_rate 0.000100171\n",
      "2019-01-29T02:31:11.168511: step 3848, loss 0.05432, acc 1, learning_rate 0.000100171\n",
      "2019-01-29T02:31:11.297252: step 3849, loss 0.0841341, acc 0.984375, learning_rate 0.000100171\n",
      "2019-01-29T02:31:11.425209: step 3850, loss 0.0812285, acc 1, learning_rate 0.00010017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:31:11.549293: step 3851, loss 0.0721708, acc 1, learning_rate 0.00010017\n",
      "2019-01-29T02:31:11.675517: step 3852, loss 0.084586, acc 0.984375, learning_rate 0.000100169\n",
      "2019-01-29T02:31:11.807321: step 3853, loss 0.0752944, acc 1, learning_rate 0.000100169\n",
      "2019-01-29T02:31:11.893376: step 3854, loss 0.0712567, acc 1, learning_rate 0.000100168\n",
      "2019-01-29T02:31:12.019287: step 3855, loss 0.0502932, acc 1, learning_rate 0.000100168\n",
      "2019-01-29T02:31:12.140346: step 3856, loss 0.0676104, acc 1, learning_rate 0.000100167\n",
      "2019-01-29T02:31:12.270072: step 3857, loss 0.0640211, acc 1, learning_rate 0.000100167\n",
      "2019-01-29T02:31:12.398198: step 3858, loss 0.0884043, acc 0.984375, learning_rate 0.000100167\n",
      "2019-01-29T02:31:12.518140: step 3859, loss 0.0778994, acc 1, learning_rate 0.000100166\n",
      "2019-01-29T02:31:12.605115: step 3860, loss 0.0549375, acc 1, learning_rate 0.000100166\n",
      "2019-01-29T02:31:12.731218: step 3861, loss 0.0545266, acc 1, learning_rate 0.000100165\n",
      "2019-01-29T02:31:12.854455: step 3862, loss 0.0665743, acc 1, learning_rate 0.000100165\n",
      "2019-01-29T02:31:12.985107: step 3863, loss 0.0509582, acc 1, learning_rate 0.000100164\n",
      "2019-01-29T02:31:13.110328: step 3864, loss 0.063702, acc 1, learning_rate 0.000100164\n",
      "2019-01-29T02:31:13.232987: step 3865, loss 0.080887, acc 1, learning_rate 0.000100163\n",
      "2019-01-29T02:31:13.358554: step 3866, loss 0.0787677, acc 0.984375, learning_rate 0.000100163\n",
      "2019-01-29T02:31:13.485305: step 3867, loss 0.0633671, acc 1, learning_rate 0.000100163\n",
      "2019-01-29T02:31:13.573934: step 3868, loss 0.0745847, acc 1, learning_rate 0.000100162\n",
      "2019-01-29T02:31:13.702311: step 3869, loss 0.0561329, acc 1, learning_rate 0.000100162\n",
      "2019-01-29T02:31:13.827888: step 3870, loss 0.0770538, acc 1, learning_rate 0.000100161\n",
      "2019-01-29T02:31:13.918360: step 3871, loss 0.0841141, acc 1, learning_rate 0.000100161\n",
      "2019-01-29T02:31:14.042880: step 3872, loss 0.0816433, acc 0.984375, learning_rate 0.00010016\n",
      "2019-01-29T02:31:14.167618: step 3873, loss 0.082017, acc 1, learning_rate 0.00010016\n",
      "2019-01-29T02:31:14.292372: step 3874, loss 0.0545691, acc 1, learning_rate 0.00010016\n",
      "2019-01-29T02:31:14.421942: step 3875, loss 0.0640404, acc 1, learning_rate 0.000100159\n",
      "2019-01-29T02:31:14.509602: step 3876, loss 0.0658156, acc 1, learning_rate 0.000100159\n",
      "2019-01-29T02:31:14.637081: step 3877, loss 0.0601871, acc 1, learning_rate 0.000100158\n",
      "2019-01-29T02:31:14.762626: step 3878, loss 0.0727087, acc 1, learning_rate 0.000100158\n",
      "2019-01-29T02:31:14.884949: step 3879, loss 0.0661644, acc 1, learning_rate 0.000100157\n",
      "2019-01-29T02:31:15.009229: step 3880, loss 0.0650666, acc 1, learning_rate 0.000100157\n",
      "2019-01-29T02:31:15.134838: step 3881, loss 0.0617105, acc 1, learning_rate 0.000100157\n",
      "2019-01-29T02:31:15.256358: step 3882, loss 0.0691803, acc 1, learning_rate 0.000100156\n",
      "2019-01-29T02:31:15.388230: step 3883, loss 0.0723519, acc 1, learning_rate 0.000100156\n",
      "2019-01-29T02:31:15.474118: step 3884, loss 0.066152, acc 1, learning_rate 0.000100155\n",
      "2019-01-29T02:31:15.596885: step 3885, loss 0.0701641, acc 0.984375, learning_rate 0.000100155\n",
      "2019-01-29T02:31:15.686384: step 3886, loss 0.0848021, acc 1, learning_rate 0.000100155\n",
      "2019-01-29T02:31:15.776766: step 3887, loss 0.0835491, acc 0.984375, learning_rate 0.000100154\n",
      "2019-01-29T02:31:15.901535: step 3888, loss 0.0612929, acc 1, learning_rate 0.000100154\n",
      "2019-01-29T02:31:16.032554: step 3889, loss 0.0698641, acc 1, learning_rate 0.000100153\n",
      "2019-01-29T02:31:16.122580: step 3890, loss 0.0684521, acc 1, learning_rate 0.000100153\n",
      "2019-01-29T02:31:16.255155: step 3891, loss 0.0625145, acc 1, learning_rate 0.000100152\n",
      "2019-01-29T02:31:16.342999: step 3892, loss 0.0634765, acc 1, learning_rate 0.000100152\n",
      "2019-01-29T02:31:16.433007: step 3893, loss 0.0804638, acc 0.984375, learning_rate 0.000100152\n",
      "2019-01-29T02:31:16.560081: step 3894, loss 0.0679095, acc 1, learning_rate 0.000100151\n",
      "2019-01-29T02:31:16.677939: step 3895, loss 0.0647776, acc 1, learning_rate 0.000100151\n",
      "2019-01-29T02:31:16.794363: step 3896, loss 0.0733021, acc 0.984375, learning_rate 0.00010015\n",
      "2019-01-29T02:31:16.921587: step 3897, loss 0.0587555, acc 1, learning_rate 0.00010015\n",
      "2019-01-29T02:31:17.007408: step 3898, loss 0.0678114, acc 1, learning_rate 0.00010015\n",
      "2019-01-29T02:31:17.097489: step 3899, loss 0.0812972, acc 0.984375, learning_rate 0.000100149\n",
      "2019-01-29T02:31:17.217730: step 3900, loss 0.0605729, acc 1, learning_rate 0.000100149\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:31:17.246210: step 3900, loss 0.663431, acc 0.751407\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-3900\n",
      "\n",
      "2019-01-29T02:31:17.587465: step 3901, loss 0.0743518, acc 1, learning_rate 0.000100148\n",
      "2019-01-29T02:31:17.687770: step 3902, loss 0.0622987, acc 1, learning_rate 0.000100148\n",
      "2019-01-29T02:31:17.818121: step 3903, loss 0.0613425, acc 1, learning_rate 0.000100148\n",
      "2019-01-29T02:31:17.942174: step 3904, loss 0.0774361, acc 0.984375, learning_rate 0.000100147\n",
      "2019-01-29T02:31:18.067363: step 3905, loss 0.0749279, acc 1, learning_rate 0.000100147\n",
      "2019-01-29T02:31:18.195191: step 3906, loss 0.0541794, acc 1, learning_rate 0.000100147\n",
      "2019-01-29T02:31:18.322862: step 3907, loss 0.079073, acc 0.984375, learning_rate 0.000100146\n",
      "2019-01-29T02:31:18.445280: step 3908, loss 0.0940926, acc 0.984375, learning_rate 0.000100146\n",
      "2019-01-29T02:31:18.532586: step 3909, loss 0.0688234, acc 1, learning_rate 0.000100145\n",
      "2019-01-29T02:31:18.658857: step 3910, loss 0.0659777, acc 1, learning_rate 0.000100145\n",
      "2019-01-29T02:31:18.784857: step 3911, loss 0.070864, acc 1, learning_rate 0.000100145\n",
      "2019-01-29T02:31:18.909642: step 3912, loss 0.0578913, acc 1, learning_rate 0.000100144\n",
      "2019-01-29T02:31:19.037025: step 3913, loss 0.0708253, acc 1, learning_rate 0.000100144\n",
      "2019-01-29T02:31:19.161754: step 3914, loss 0.0722701, acc 1, learning_rate 0.000100143\n",
      "2019-01-29T02:31:19.289408: step 3915, loss 0.0775405, acc 0.984375, learning_rate 0.000100143\n",
      "2019-01-29T02:31:19.413243: step 3916, loss 0.063299, acc 1, learning_rate 0.000100143\n",
      "2019-01-29T02:31:19.503381: step 3917, loss 0.0664075, acc 1, learning_rate 0.000100142\n",
      "2019-01-29T02:31:19.626209: step 3918, loss 0.0560827, acc 1, learning_rate 0.000100142\n",
      "2019-01-29T02:31:19.752886: step 3919, loss 0.0538021, acc 1, learning_rate 0.000100142\n",
      "2019-01-29T02:31:19.875464: step 3920, loss 0.0772564, acc 1, learning_rate 0.000100141\n",
      "2019-01-29T02:31:19.998926: step 3921, loss 0.0645747, acc 1, learning_rate 0.000100141\n",
      "2019-01-29T02:31:20.122738: step 3922, loss 0.0747067, acc 1, learning_rate 0.00010014\n",
      "2019-01-29T02:31:20.244721: step 3923, loss 0.0612167, acc 1, learning_rate 0.00010014\n",
      "2019-01-29T02:31:20.324954: step 3924, loss 0.069159, acc 0.984375, learning_rate 0.00010014\n",
      "2019-01-29T02:31:20.449005: step 3925, loss 0.0669056, acc 0.984375, learning_rate 0.000100139\n",
      "2019-01-29T02:31:20.539213: step 3926, loss 0.0529594, acc 1, learning_rate 0.000100139\n",
      "2019-01-29T02:31:20.666508: step 3927, loss 0.0620442, acc 1, learning_rate 0.000100139\n",
      "2019-01-29T02:31:20.792711: step 3928, loss 0.067253, acc 1, learning_rate 0.000100138\n",
      "2019-01-29T02:31:20.914684: step 3929, loss 0.0913633, acc 0.984375, learning_rate 0.000100138\n",
      "2019-01-29T02:31:21.040851: step 3930, loss 0.0647206, acc 1, learning_rate 0.000100137\n",
      "2019-01-29T02:31:21.172065: step 3931, loss 0.0928679, acc 0.984375, learning_rate 0.000100137\n",
      "2019-01-29T02:31:21.296228: step 3932, loss 0.0600634, acc 1, learning_rate 0.000100137\n",
      "2019-01-29T02:31:21.424246: step 3933, loss 0.054446, acc 1, learning_rate 0.000100136\n",
      "2019-01-29T02:31:21.546047: step 3934, loss 0.0689233, acc 1, learning_rate 0.000100136\n",
      "2019-01-29T02:31:21.673786: step 3935, loss 0.0762519, acc 1, learning_rate 0.000100136\n",
      "2019-01-29T02:31:21.795766: step 3936, loss 0.0611476, acc 1, learning_rate 0.000100135\n",
      "2019-01-29T02:31:21.924194: step 3937, loss 0.0670133, acc 1, learning_rate 0.000100135\n",
      "2019-01-29T02:31:22.053664: step 3938, loss 0.0720703, acc 1, learning_rate 0.000100135\n",
      "2019-01-29T02:31:22.173243: step 3939, loss 0.0649139, acc 1, learning_rate 0.000100134\n",
      "2019-01-29T02:31:22.300273: step 3940, loss 0.0816837, acc 1, learning_rate 0.000100134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:31:22.423341: step 3941, loss 0.0572209, acc 1, learning_rate 0.000100133\n",
      "2019-01-29T02:31:22.548585: step 3942, loss 0.0593302, acc 1, learning_rate 0.000100133\n",
      "2019-01-29T02:31:22.671759: step 3943, loss 0.0560553, acc 1, learning_rate 0.000100133\n",
      "2019-01-29T02:31:22.797526: step 3944, loss 0.068782, acc 1, learning_rate 0.000100132\n",
      "2019-01-29T02:31:22.884804: step 3945, loss 0.0593327, acc 1, learning_rate 0.000100132\n",
      "2019-01-29T02:31:23.011953: step 3946, loss 0.0682638, acc 1, learning_rate 0.000100132\n",
      "2019-01-29T02:31:23.140692: step 3947, loss 0.0513903, acc 1, learning_rate 0.000100131\n",
      "2019-01-29T02:31:23.265700: step 3948, loss 0.0758597, acc 1, learning_rate 0.000100131\n",
      "2019-01-29T02:31:23.351584: step 3949, loss 0.080179, acc 1, learning_rate 0.000100131\n",
      "2019-01-29T02:31:23.474190: step 3950, loss 0.0834695, acc 1, learning_rate 0.00010013\n",
      "2019-01-29T02:31:23.604353: step 3951, loss 0.0545946, acc 1, learning_rate 0.00010013\n",
      "2019-01-29T02:31:23.732315: step 3952, loss 0.0662291, acc 1, learning_rate 0.00010013\n",
      "2019-01-29T02:31:23.857848: step 3953, loss 0.0662634, acc 1, learning_rate 0.000100129\n",
      "2019-01-29T02:31:23.974036: step 3954, loss 0.0779228, acc 1, learning_rate 0.000100129\n",
      "2019-01-29T02:31:24.100465: step 3955, loss 0.0764234, acc 1, learning_rate 0.000100129\n",
      "2019-01-29T02:31:24.186314: step 3956, loss 0.0766159, acc 0.984375, learning_rate 0.000100128\n",
      "2019-01-29T02:31:24.359664: step 3957, loss 0.0603112, acc 1, learning_rate 0.000100128\n",
      "2019-01-29T02:31:24.491679: step 3958, loss 0.110931, acc 0.984375, learning_rate 0.000100128\n",
      "2019-01-29T02:31:24.610596: step 3959, loss 0.0604993, acc 1, learning_rate 0.000100127\n",
      "2019-01-29T02:31:24.740307: step 3960, loss 0.0666934, acc 1, learning_rate 0.000100127\n",
      "2019-01-29T02:31:24.864851: step 3961, loss 0.0624735, acc 1, learning_rate 0.000100127\n",
      "2019-01-29T02:31:24.995360: step 3962, loss 0.0655196, acc 1, learning_rate 0.000100126\n",
      "2019-01-29T02:31:25.118244: step 3963, loss 0.0548965, acc 1, learning_rate 0.000100126\n",
      "2019-01-29T02:31:25.248279: step 3964, loss 0.0997857, acc 0.984375, learning_rate 0.000100126\n",
      "2019-01-29T02:31:25.376291: step 3965, loss 0.0699598, acc 1, learning_rate 0.000100125\n",
      "2019-01-29T02:31:25.490334: step 3966, loss 0.060171, acc 1, learning_rate 0.000100125\n",
      "2019-01-29T02:31:25.622534: step 3967, loss 0.0876753, acc 0.984375, learning_rate 0.000100125\n",
      "2019-01-29T02:31:25.715623: step 3968, loss 0.0679049, acc 1, learning_rate 0.000100124\n",
      "2019-01-29T02:31:25.845075: step 3969, loss 0.0986599, acc 0.96875, learning_rate 0.000100124\n",
      "2019-01-29T02:31:25.936012: step 3970, loss 0.0991943, acc 0.96875, learning_rate 0.000100124\n",
      "2019-01-29T02:31:26.058956: step 3971, loss 0.0750363, acc 1, learning_rate 0.000100123\n",
      "2019-01-29T02:31:26.177543: step 3972, loss 0.06243, acc 1, learning_rate 0.000100123\n",
      "2019-01-29T02:31:26.300986: step 3973, loss 0.0689006, acc 1, learning_rate 0.000100123\n",
      "2019-01-29T02:31:26.390999: step 3974, loss 0.0623058, acc 1, learning_rate 0.000100122\n",
      "2019-01-29T02:31:26.516397: step 3975, loss 0.0682526, acc 1, learning_rate 0.000100122\n",
      "2019-01-29T02:31:26.645339: step 3976, loss 0.0608542, acc 1, learning_rate 0.000100122\n",
      "2019-01-29T02:31:26.775088: step 3977, loss 0.0670234, acc 1, learning_rate 0.000100121\n",
      "2019-01-29T02:31:26.896249: step 3978, loss 0.080428, acc 0.984375, learning_rate 0.000100121\n",
      "2019-01-29T02:31:27.019418: step 3979, loss 0.0706346, acc 1, learning_rate 0.000100121\n",
      "2019-01-29T02:31:27.103600: step 3980, loss 0.0607106, acc 1, learning_rate 0.00010012\n",
      "2019-01-29T02:31:27.193334: step 3981, loss 0.0907498, acc 0.984375, learning_rate 0.00010012\n",
      "2019-01-29T02:31:27.315669: step 3982, loss 0.0671178, acc 1, learning_rate 0.00010012\n",
      "2019-01-29T02:31:27.450524: step 3983, loss 0.0601139, acc 1, learning_rate 0.000100119\n",
      "2019-01-29T02:31:27.580685: step 3984, loss 0.0625252, acc 1, learning_rate 0.000100119\n",
      "2019-01-29T02:31:27.670868: step 3985, loss 0.0617013, acc 1, learning_rate 0.000100119\n",
      "2019-01-29T02:31:27.791205: step 3986, loss 0.0694537, acc 1, learning_rate 0.000100118\n",
      "2019-01-29T02:31:27.921000: step 3987, loss 0.0662261, acc 1, learning_rate 0.000100118\n",
      "2019-01-29T02:31:28.049124: step 3988, loss 0.067306, acc 1, learning_rate 0.000100118\n",
      "2019-01-29T02:31:28.175148: step 3989, loss 0.0642567, acc 1, learning_rate 0.000100117\n",
      "2019-01-29T02:31:28.301562: step 3990, loss 0.0837232, acc 1, learning_rate 0.000100117\n",
      "2019-01-29T02:31:28.426891: step 3991, loss 0.0670354, acc 1, learning_rate 0.000100117\n",
      "2019-01-29T02:31:28.554598: step 3992, loss 0.0756769, acc 0.984375, learning_rate 0.000100116\n",
      "2019-01-29T02:31:28.642578: step 3993, loss 0.0675916, acc 1, learning_rate 0.000100116\n",
      "2019-01-29T02:31:28.769065: step 3994, loss 0.066523, acc 1, learning_rate 0.000100116\n",
      "2019-01-29T02:31:28.890145: step 3995, loss 0.0594807, acc 1, learning_rate 0.000100116\n",
      "2019-01-29T02:31:29.015089: step 3996, loss 0.0613275, acc 1, learning_rate 0.000100115\n",
      "2019-01-29T02:31:29.108364: step 3997, loss 0.0804404, acc 1, learning_rate 0.000100115\n",
      "2019-01-29T02:31:29.197703: step 3998, loss 0.0876991, acc 0.96875, learning_rate 0.000100115\n",
      "2019-01-29T02:31:29.322899: step 3999, loss 0.0826269, acc 1, learning_rate 0.000100114\n",
      "2019-01-29T02:31:29.448343: step 4000, loss 0.0702109, acc 1, learning_rate 0.000100114\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:31:29.476990: step 4000, loss 0.668254, acc 0.751407\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-4000\n",
      "\n",
      "2019-01-29T02:31:29.823665: step 4001, loss 0.0801139, acc 0.96875, learning_rate 0.000100114\n",
      "2019-01-29T02:31:29.950397: step 4002, loss 0.0691723, acc 0.984375, learning_rate 0.000100113\n",
      "2019-01-29T02:31:30.078419: step 4003, loss 0.0566512, acc 1, learning_rate 0.000100113\n",
      "2019-01-29T02:31:30.163037: step 4004, loss 0.068388, acc 1, learning_rate 0.000100113\n",
      "2019-01-29T02:31:30.282584: step 4005, loss 0.0614866, acc 1, learning_rate 0.000100113\n",
      "2019-01-29T02:31:30.408117: step 4006, loss 0.0621874, acc 1, learning_rate 0.000100112\n",
      "2019-01-29T02:31:30.529500: step 4007, loss 0.0648297, acc 1, learning_rate 0.000100112\n",
      "2019-01-29T02:31:30.655537: step 4008, loss 0.0950942, acc 0.984375, learning_rate 0.000100112\n",
      "2019-01-29T02:31:30.780185: step 4009, loss 0.0618909, acc 1, learning_rate 0.000100111\n",
      "2019-01-29T02:31:30.903987: step 4010, loss 0.0722073, acc 1, learning_rate 0.000100111\n",
      "2019-01-29T02:31:31.028248: step 4011, loss 0.0751375, acc 1, learning_rate 0.000100111\n",
      "2019-01-29T02:31:31.152473: step 4012, loss 0.0561691, acc 1, learning_rate 0.00010011\n",
      "2019-01-29T02:31:31.241984: step 4013, loss 0.079219, acc 0.984375, learning_rate 0.00010011\n",
      "2019-01-29T02:31:31.362522: step 4014, loss 0.0799669, acc 1, learning_rate 0.00010011\n",
      "2019-01-29T02:31:31.490669: step 4015, loss 0.0742092, acc 0.984375, learning_rate 0.00010011\n",
      "2019-01-29T02:31:31.578743: step 4016, loss 0.0621956, acc 1, learning_rate 0.000100109\n",
      "2019-01-29T02:31:31.669622: step 4017, loss 0.0732968, acc 1, learning_rate 0.000100109\n",
      "2019-01-29T02:31:31.792646: step 4018, loss 0.060347, acc 0.984375, learning_rate 0.000100109\n",
      "2019-01-29T02:31:31.877222: step 4019, loss 0.0626559, acc 1, learning_rate 0.000100108\n",
      "2019-01-29T02:31:32.007868: step 4020, loss 0.0845822, acc 1, learning_rate 0.000100108\n",
      "2019-01-29T02:31:32.134695: step 4021, loss 0.0619373, acc 1, learning_rate 0.000100108\n",
      "2019-01-29T02:31:32.237102: step 4022, loss 0.0889271, acc 1, learning_rate 0.000100108\n",
      "2019-01-29T02:31:32.364004: step 4023, loss 0.055462, acc 1, learning_rate 0.000100107\n",
      "2019-01-29T02:31:32.494823: step 4024, loss 0.084956, acc 1, learning_rate 0.000100107\n",
      "2019-01-29T02:31:32.620999: step 4025, loss 0.0733637, acc 0.984375, learning_rate 0.000100107\n",
      "2019-01-29T02:31:32.748555: step 4026, loss 0.0632588, acc 1, learning_rate 0.000100106\n",
      "2019-01-29T02:31:32.870629: step 4027, loss 0.0520902, acc 1, learning_rate 0.000100106\n",
      "2019-01-29T02:31:33.002391: step 4028, loss 0.0785748, acc 1, learning_rate 0.000100106\n",
      "2019-01-29T02:31:33.129469: step 4029, loss 0.0508074, acc 1, learning_rate 0.000100106\n",
      "2019-01-29T02:31:33.257697: step 4030, loss 0.0628921, acc 1, learning_rate 0.000100105\n",
      "2019-01-29T02:31:33.347303: step 4031, loss 0.0556631, acc 1, learning_rate 0.000100105\n",
      "2019-01-29T02:31:33.442336: step 4032, loss 0.0613137, acc 1, learning_rate 0.000100105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:31:33.528650: step 4033, loss 0.0706972, acc 1, learning_rate 0.000100104\n",
      "2019-01-29T02:31:33.652928: step 4034, loss 0.079199, acc 0.984375, learning_rate 0.000100104\n",
      "2019-01-29T02:31:33.779062: step 4035, loss 0.0695222, acc 1, learning_rate 0.000100104\n",
      "2019-01-29T02:31:33.869264: step 4036, loss 0.0650534, acc 1, learning_rate 0.000100104\n",
      "2019-01-29T02:31:33.954870: step 4037, loss 0.0658912, acc 1, learning_rate 0.000100103\n",
      "2019-01-29T02:31:34.080700: step 4038, loss 0.0755015, acc 1, learning_rate 0.000100103\n",
      "2019-01-29T02:31:34.199588: step 4039, loss 0.0797089, acc 1, learning_rate 0.000100103\n",
      "2019-01-29T02:31:34.328688: step 4040, loss 0.0674353, acc 1, learning_rate 0.000100102\n",
      "2019-01-29T02:31:34.455218: step 4041, loss 0.0642674, acc 1, learning_rate 0.000100102\n",
      "2019-01-29T02:31:34.584989: step 4042, loss 0.0813491, acc 0.984375, learning_rate 0.000100102\n",
      "2019-01-29T02:31:34.674715: step 4043, loss 0.0653312, acc 1, learning_rate 0.000100102\n",
      "2019-01-29T02:31:34.763028: step 4044, loss 0.0667451, acc 1, learning_rate 0.000100101\n",
      "2019-01-29T02:31:34.895242: step 4045, loss 0.0745899, acc 0.984375, learning_rate 0.000100101\n",
      "2019-01-29T02:31:35.021873: step 4046, loss 0.0674509, acc 0.984375, learning_rate 0.000100101\n",
      "2019-01-29T02:31:35.150246: step 4047, loss 0.0685087, acc 1, learning_rate 0.000100101\n",
      "2019-01-29T02:31:35.274138: step 4048, loss 0.0636181, acc 1, learning_rate 0.0001001\n",
      "2019-01-29T02:31:35.396101: step 4049, loss 0.0543182, acc 1, learning_rate 0.0001001\n",
      "2019-01-29T02:31:35.489123: step 4050, loss 0.103785, acc 0.966667, learning_rate 0.0001001\n",
      "2019-01-29T02:31:35.609628: step 4051, loss 0.056307, acc 1, learning_rate 0.0001001\n",
      "2019-01-29T02:31:35.700235: step 4052, loss 0.0747156, acc 1, learning_rate 0.000100099\n",
      "2019-01-29T02:31:35.828085: step 4053, loss 0.0820435, acc 1, learning_rate 0.000100099\n",
      "2019-01-29T02:31:35.952251: step 4054, loss 0.0693054, acc 1, learning_rate 0.000100099\n",
      "2019-01-29T02:31:36.074528: step 4055, loss 0.0696721, acc 1, learning_rate 0.000100098\n",
      "2019-01-29T02:31:36.204920: step 4056, loss 0.0683253, acc 1, learning_rate 0.000100098\n",
      "2019-01-29T02:31:36.331434: step 4057, loss 0.070429, acc 1, learning_rate 0.000100098\n",
      "2019-01-29T02:31:36.453528: step 4058, loss 0.0793399, acc 0.984375, learning_rate 0.000100098\n",
      "2019-01-29T02:31:36.541133: step 4059, loss 0.0659036, acc 1, learning_rate 0.000100097\n",
      "2019-01-29T02:31:36.624809: step 4060, loss 0.0704213, acc 1, learning_rate 0.000100097\n",
      "2019-01-29T02:31:36.714952: step 4061, loss 0.0664319, acc 1, learning_rate 0.000100097\n",
      "2019-01-29T02:31:36.833955: step 4062, loss 0.0616456, acc 1, learning_rate 0.000100097\n",
      "2019-01-29T02:31:36.959290: step 4063, loss 0.0805616, acc 1, learning_rate 0.000100096\n",
      "2019-01-29T02:31:37.048189: step 4064, loss 0.0760868, acc 1, learning_rate 0.000100096\n",
      "2019-01-29T02:31:37.174890: step 4065, loss 0.0639144, acc 1, learning_rate 0.000100096\n",
      "2019-01-29T02:31:37.263285: step 4066, loss 0.0540942, acc 1, learning_rate 0.000100096\n",
      "2019-01-29T02:31:37.351894: step 4067, loss 0.0693841, acc 1, learning_rate 0.000100095\n",
      "2019-01-29T02:31:37.473075: step 4068, loss 0.069776, acc 1, learning_rate 0.000100095\n",
      "2019-01-29T02:31:37.562114: step 4069, loss 0.0746097, acc 1, learning_rate 0.000100095\n",
      "2019-01-29T02:31:37.688327: step 4070, loss 0.059277, acc 1, learning_rate 0.000100095\n",
      "2019-01-29T02:31:37.813491: step 4071, loss 0.0627024, acc 1, learning_rate 0.000100094\n",
      "2019-01-29T02:31:37.939754: step 4072, loss 0.0725853, acc 1, learning_rate 0.000100094\n",
      "2019-01-29T02:31:38.066253: step 4073, loss 0.0747648, acc 1, learning_rate 0.000100094\n",
      "2019-01-29T02:31:38.193706: step 4074, loss 0.0627854, acc 1, learning_rate 0.000100094\n",
      "2019-01-29T02:31:38.283941: step 4075, loss 0.0587503, acc 1, learning_rate 0.000100093\n",
      "2019-01-29T02:31:38.373812: step 4076, loss 0.0541848, acc 1, learning_rate 0.000100093\n",
      "2019-01-29T02:31:38.504321: step 4077, loss 0.072722, acc 1, learning_rate 0.000100093\n",
      "2019-01-29T02:31:38.631054: step 4078, loss 0.0533278, acc 1, learning_rate 0.000100093\n",
      "2019-01-29T02:31:38.751709: step 4079, loss 0.0776609, acc 0.984375, learning_rate 0.000100092\n",
      "2019-01-29T02:31:38.881054: step 4080, loss 0.0589359, acc 1, learning_rate 0.000100092\n",
      "2019-01-29T02:31:38.999470: step 4081, loss 0.0553296, acc 1, learning_rate 0.000100092\n",
      "2019-01-29T02:31:39.123175: step 4082, loss 0.0637975, acc 1, learning_rate 0.000100092\n",
      "2019-01-29T02:31:39.247262: step 4083, loss 0.0735278, acc 0.984375, learning_rate 0.000100091\n",
      "2019-01-29T02:31:39.377276: step 4084, loss 0.0664662, acc 1, learning_rate 0.000100091\n",
      "2019-01-29T02:31:39.501858: step 4085, loss 0.060215, acc 1, learning_rate 0.000100091\n",
      "2019-01-29T02:31:39.630076: step 4086, loss 0.0632012, acc 1, learning_rate 0.000100091\n",
      "2019-01-29T02:31:39.759431: step 4087, loss 0.0535102, acc 1, learning_rate 0.00010009\n",
      "2019-01-29T02:31:39.848794: step 4088, loss 0.0533637, acc 1, learning_rate 0.00010009\n",
      "2019-01-29T02:31:39.936611: step 4089, loss 0.0699283, acc 0.984375, learning_rate 0.00010009\n",
      "2019-01-29T02:31:40.026215: step 4090, loss 0.0704603, acc 1, learning_rate 0.00010009\n",
      "2019-01-29T02:31:40.114442: step 4091, loss 0.0770469, acc 1, learning_rate 0.000100089\n",
      "2019-01-29T02:31:40.243395: step 4092, loss 0.0632834, acc 1, learning_rate 0.000100089\n",
      "2019-01-29T02:31:40.375579: step 4093, loss 0.0837924, acc 0.984375, learning_rate 0.000100089\n",
      "2019-01-29T02:31:40.502424: step 4094, loss 0.0653884, acc 1, learning_rate 0.000100089\n",
      "2019-01-29T02:31:40.628541: step 4095, loss 0.0675824, acc 1, learning_rate 0.000100088\n",
      "2019-01-29T02:31:40.753143: step 4096, loss 0.0598047, acc 1, learning_rate 0.000100088\n",
      "2019-01-29T02:31:40.859987: step 4097, loss 0.0723195, acc 0.984375, learning_rate 0.000100088\n",
      "2019-01-29T02:31:40.990895: step 4098, loss 0.059416, acc 1, learning_rate 0.000100088\n",
      "2019-01-29T02:31:41.114204: step 4099, loss 0.0580735, acc 1, learning_rate 0.000100088\n",
      "2019-01-29T02:31:41.241264: step 4100, loss 0.0494595, acc 1, learning_rate 0.000100087\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:31:41.269012: step 4100, loss 0.664984, acc 0.755159\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-4100\n",
      "\n",
      "2019-01-29T02:31:41.594649: step 4101, loss 0.0776505, acc 1, learning_rate 0.000100087\n",
      "2019-01-29T02:31:41.718574: step 4102, loss 0.0891846, acc 0.984375, learning_rate 0.000100087\n",
      "2019-01-29T02:31:41.806597: step 4103, loss 0.0595511, acc 1, learning_rate 0.000100087\n",
      "2019-01-29T02:31:41.922125: step 4104, loss 0.0788316, acc 1, learning_rate 0.000100086\n",
      "2019-01-29T02:31:42.045287: step 4105, loss 0.0845059, acc 0.96875, learning_rate 0.000100086\n",
      "2019-01-29T02:31:42.172153: step 4106, loss 0.0734319, acc 1, learning_rate 0.000100086\n",
      "2019-01-29T02:31:42.302977: step 4107, loss 0.065733, acc 1, learning_rate 0.000100086\n",
      "2019-01-29T02:31:42.435348: step 4108, loss 0.062065, acc 1, learning_rate 0.000100085\n",
      "2019-01-29T02:31:42.563832: step 4109, loss 0.0893374, acc 0.984375, learning_rate 0.000100085\n",
      "2019-01-29T02:31:42.653653: step 4110, loss 0.076161, acc 1, learning_rate 0.000100085\n",
      "2019-01-29T02:31:42.780648: step 4111, loss 0.0700406, acc 1, learning_rate 0.000100085\n",
      "2019-01-29T02:31:42.912065: step 4112, loss 0.0716546, acc 1, learning_rate 0.000100085\n",
      "2019-01-29T02:31:43.000292: step 4113, loss 0.0586454, acc 1, learning_rate 0.000100084\n",
      "2019-01-29T02:31:43.124557: step 4114, loss 0.0593304, acc 1, learning_rate 0.000100084\n",
      "2019-01-29T02:31:43.247343: step 4115, loss 0.0656394, acc 1, learning_rate 0.000100084\n",
      "2019-01-29T02:31:43.368054: step 4116, loss 0.0900204, acc 1, learning_rate 0.000100084\n",
      "2019-01-29T02:31:43.487245: step 4117, loss 0.0518475, acc 1, learning_rate 0.000100083\n",
      "2019-01-29T02:31:43.613383: step 4118, loss 0.0511869, acc 1, learning_rate 0.000100083\n",
      "2019-01-29T02:31:43.737334: step 4119, loss 0.0767266, acc 0.984375, learning_rate 0.000100083\n",
      "2019-01-29T02:31:43.856357: step 4120, loss 0.0634127, acc 1, learning_rate 0.000100083\n",
      "2019-01-29T02:31:43.983978: step 4121, loss 0.0594974, acc 1, learning_rate 0.000100083\n",
      "2019-01-29T02:31:44.101223: step 4122, loss 0.0611093, acc 1, learning_rate 0.000100082\n",
      "2019-01-29T02:31:44.233162: step 4123, loss 0.0542763, acc 1, learning_rate 0.000100082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:31:44.320952: step 4124, loss 0.0710458, acc 1, learning_rate 0.000100082\n",
      "2019-01-29T02:31:44.446276: step 4125, loss 0.0527916, acc 1, learning_rate 0.000100082\n",
      "2019-01-29T02:31:44.577678: step 4126, loss 0.0719952, acc 1, learning_rate 0.000100081\n",
      "2019-01-29T02:31:44.699639: step 4127, loss 0.0648714, acc 1, learning_rate 0.000100081\n",
      "2019-01-29T02:31:44.791419: step 4128, loss 0.0638353, acc 1, learning_rate 0.000100081\n",
      "2019-01-29T02:31:44.919502: step 4129, loss 0.0730306, acc 1, learning_rate 0.000100081\n",
      "2019-01-29T02:31:45.045605: step 4130, loss 0.0680168, acc 1, learning_rate 0.000100081\n",
      "2019-01-29T02:31:45.131163: step 4131, loss 0.0797622, acc 1, learning_rate 0.00010008\n",
      "2019-01-29T02:31:45.253686: step 4132, loss 0.0888378, acc 1, learning_rate 0.00010008\n",
      "2019-01-29T02:31:45.380493: step 4133, loss 0.0691624, acc 1, learning_rate 0.00010008\n",
      "2019-01-29T02:31:45.489120: step 4134, loss 0.0558121, acc 1, learning_rate 0.00010008\n",
      "2019-01-29T02:31:45.610401: step 4135, loss 0.0528851, acc 1, learning_rate 0.00010008\n",
      "2019-01-29T02:31:45.740833: step 4136, loss 0.0482535, acc 1, learning_rate 0.000100079\n",
      "2019-01-29T02:31:45.864748: step 4137, loss 0.0639531, acc 1, learning_rate 0.000100079\n",
      "2019-01-29T02:31:45.982282: step 4138, loss 0.0522037, acc 1, learning_rate 0.000100079\n",
      "2019-01-29T02:31:46.105321: step 4139, loss 0.0823305, acc 1, learning_rate 0.000100079\n",
      "2019-01-29T02:31:46.228299: step 4140, loss 0.061351, acc 1, learning_rate 0.000100078\n",
      "2019-01-29T02:31:46.354394: step 4141, loss 0.0586549, acc 1, learning_rate 0.000100078\n",
      "2019-01-29T02:31:46.485225: step 4142, loss 0.0826608, acc 0.984375, learning_rate 0.000100078\n",
      "2019-01-29T02:31:46.606440: step 4143, loss 0.0623609, acc 1, learning_rate 0.000100078\n",
      "2019-01-29T02:31:46.727965: step 4144, loss 0.0672031, acc 1, learning_rate 0.000100078\n",
      "2019-01-29T02:31:46.816194: step 4145, loss 0.0614317, acc 1, learning_rate 0.000100077\n",
      "2019-01-29T02:31:46.941121: step 4146, loss 0.0672899, acc 1, learning_rate 0.000100077\n",
      "2019-01-29T02:31:47.069283: step 4147, loss 0.0605403, acc 1, learning_rate 0.000100077\n",
      "2019-01-29T02:31:47.199882: step 4148, loss 0.0650939, acc 1, learning_rate 0.000100077\n",
      "2019-01-29T02:31:47.287707: step 4149, loss 0.0854235, acc 1, learning_rate 0.000100077\n",
      "2019-01-29T02:31:47.418807: step 4150, loss 0.0547228, acc 1, learning_rate 0.000100076\n",
      "2019-01-29T02:31:47.505665: step 4151, loss 0.0568729, acc 1, learning_rate 0.000100076\n",
      "2019-01-29T02:31:47.630145: step 4152, loss 0.0592169, acc 1, learning_rate 0.000100076\n",
      "2019-01-29T02:31:47.758253: step 4153, loss 0.0725876, acc 1, learning_rate 0.000100076\n",
      "2019-01-29T02:31:47.851809: step 4154, loss 0.0656257, acc 1, learning_rate 0.000100076\n",
      "2019-01-29T02:31:47.979523: step 4155, loss 0.054762, acc 1, learning_rate 0.000100075\n",
      "2019-01-29T02:31:48.065671: step 4156, loss 0.0782957, acc 0.984375, learning_rate 0.000100075\n",
      "2019-01-29T02:31:48.198705: step 4157, loss 0.0883416, acc 0.984375, learning_rate 0.000100075\n",
      "2019-01-29T02:31:48.329991: step 4158, loss 0.0820646, acc 1, learning_rate 0.000100075\n",
      "2019-01-29T02:31:48.456134: step 4159, loss 0.095073, acc 0.96875, learning_rate 0.000100075\n",
      "2019-01-29T02:31:48.546160: step 4160, loss 0.0764423, acc 1, learning_rate 0.000100074\n",
      "2019-01-29T02:31:48.635307: step 4161, loss 0.103437, acc 0.984375, learning_rate 0.000100074\n",
      "2019-01-29T02:31:48.761687: step 4162, loss 0.0742374, acc 0.984375, learning_rate 0.000100074\n",
      "2019-01-29T02:31:48.886620: step 4163, loss 0.074946, acc 1, learning_rate 0.000100074\n",
      "2019-01-29T02:31:48.975865: step 4164, loss 0.100245, acc 0.984375, learning_rate 0.000100074\n",
      "2019-01-29T02:31:49.101811: step 4165, loss 0.0597273, acc 1, learning_rate 0.000100073\n",
      "2019-01-29T02:31:49.228989: step 4166, loss 0.0744785, acc 0.984375, learning_rate 0.000100073\n",
      "2019-01-29T02:31:49.354570: step 4167, loss 0.0681181, acc 0.984375, learning_rate 0.000100073\n",
      "2019-01-29T02:31:49.443285: step 4168, loss 0.081184, acc 0.984375, learning_rate 0.000100073\n",
      "2019-01-29T02:31:49.566486: step 4169, loss 0.0587998, acc 1, learning_rate 0.000100073\n",
      "2019-01-29T02:31:49.692743: step 4170, loss 0.0840161, acc 1, learning_rate 0.000100072\n",
      "2019-01-29T02:31:49.819075: step 4171, loss 0.0525873, acc 1, learning_rate 0.000100072\n",
      "2019-01-29T02:31:49.945185: step 4172, loss 0.0517825, acc 1, learning_rate 0.000100072\n",
      "2019-01-29T02:31:50.071828: step 4173, loss 0.0861617, acc 1, learning_rate 0.000100072\n",
      "2019-01-29T02:31:50.161281: step 4174, loss 0.0584502, acc 1, learning_rate 0.000100072\n",
      "2019-01-29T02:31:50.254459: step 4175, loss 0.0535322, acc 1, learning_rate 0.000100071\n",
      "2019-01-29T02:31:50.379679: step 4176, loss 0.0610954, acc 1, learning_rate 0.000100071\n",
      "2019-01-29T02:31:50.506900: step 4177, loss 0.0936147, acc 0.984375, learning_rate 0.000100071\n",
      "2019-01-29T02:31:50.632130: step 4178, loss 0.0781926, acc 0.96875, learning_rate 0.000100071\n",
      "2019-01-29T02:31:50.720054: step 4179, loss 0.0976676, acc 0.984375, learning_rate 0.000100071\n",
      "2019-01-29T02:31:50.847490: step 4180, loss 0.0727692, acc 1, learning_rate 0.000100071\n",
      "2019-01-29T02:31:50.930994: step 4181, loss 0.0590137, acc 1, learning_rate 0.00010007\n",
      "2019-01-29T02:31:51.057733: step 4182, loss 0.0569438, acc 1, learning_rate 0.00010007\n",
      "2019-01-29T02:31:51.144954: step 4183, loss 0.066329, acc 1, learning_rate 0.00010007\n",
      "2019-01-29T02:31:51.268422: step 4184, loss 0.054685, acc 1, learning_rate 0.00010007\n",
      "2019-01-29T02:31:51.395925: step 4185, loss 0.0598481, acc 1, learning_rate 0.00010007\n",
      "2019-01-29T02:31:51.525046: step 4186, loss 0.0629226, acc 1, learning_rate 0.000100069\n",
      "2019-01-29T02:31:51.649986: step 4187, loss 0.0778115, acc 0.984375, learning_rate 0.000100069\n",
      "2019-01-29T02:31:51.777901: step 4188, loss 0.0812649, acc 1, learning_rate 0.000100069\n",
      "2019-01-29T02:31:51.884137: step 4189, loss 0.0524685, acc 1, learning_rate 0.000100069\n",
      "2019-01-29T02:31:51.973042: step 4190, loss 0.0738221, acc 1, learning_rate 0.000100069\n",
      "2019-01-29T02:31:52.103220: step 4191, loss 0.0797253, acc 0.984375, learning_rate 0.000100068\n",
      "2019-01-29T02:31:52.228522: step 4192, loss 0.0641124, acc 1, learning_rate 0.000100068\n",
      "2019-01-29T02:31:52.316751: step 4193, loss 0.070527, acc 1, learning_rate 0.000100068\n",
      "2019-01-29T02:31:52.405010: step 4194, loss 0.0623308, acc 1, learning_rate 0.000100068\n",
      "2019-01-29T02:31:52.494077: step 4195, loss 0.0763932, acc 0.984375, learning_rate 0.000100068\n",
      "2019-01-29T02:31:52.626063: step 4196, loss 0.0942061, acc 0.984375, learning_rate 0.000100068\n",
      "2019-01-29T02:31:52.751257: step 4197, loss 0.0841479, acc 0.984375, learning_rate 0.000100067\n",
      "2019-01-29T02:31:52.882076: step 4198, loss 0.0527601, acc 1, learning_rate 0.000100067\n",
      "2019-01-29T02:31:53.005621: step 4199, loss 0.0845167, acc 0.984375, learning_rate 0.000100067\n",
      "2019-01-29T02:31:53.131461: step 4200, loss 0.0700807, acc 1, learning_rate 0.000100067\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:31:53.159168: step 4200, loss 0.664992, acc 0.753283\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-4200\n",
      "\n",
      "2019-01-29T02:31:53.492168: step 4201, loss 0.0535001, acc 1, learning_rate 0.000100067\n",
      "2019-01-29T02:31:53.614543: step 4202, loss 0.0644724, acc 1, learning_rate 0.000100067\n",
      "2019-01-29T02:31:53.744800: step 4203, loss 0.0733083, acc 1, learning_rate 0.000100066\n",
      "2019-01-29T02:31:53.872737: step 4204, loss 0.0692207, acc 1, learning_rate 0.000100066\n",
      "2019-01-29T02:31:53.998265: step 4205, loss 0.0847918, acc 0.984375, learning_rate 0.000100066\n",
      "2019-01-29T02:31:54.121968: step 4206, loss 0.092761, acc 0.984375, learning_rate 0.000100066\n",
      "2019-01-29T02:31:54.250904: step 4207, loss 0.0617345, acc 1, learning_rate 0.000100066\n",
      "2019-01-29T02:31:54.382898: step 4208, loss 0.0788883, acc 1, learning_rate 0.000100065\n",
      "2019-01-29T02:31:54.506574: step 4209, loss 0.081555, acc 0.984375, learning_rate 0.000100065\n",
      "2019-01-29T02:31:54.633592: step 4210, loss 0.0787464, acc 1, learning_rate 0.000100065\n",
      "2019-01-29T02:31:54.755046: step 4211, loss 0.0589679, acc 1, learning_rate 0.000100065\n",
      "2019-01-29T02:31:54.878587: step 4212, loss 0.0735834, acc 0.984375, learning_rate 0.000100065\n",
      "2019-01-29T02:31:55.005674: step 4213, loss 0.0765742, acc 1, learning_rate 0.000100065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:31:55.129491: step 4214, loss 0.0994961, acc 0.984375, learning_rate 0.000100064\n",
      "2019-01-29T02:31:55.214740: step 4215, loss 0.0529496, acc 1, learning_rate 0.000100064\n",
      "2019-01-29T02:31:55.337912: step 4216, loss 0.0585208, acc 1, learning_rate 0.000100064\n",
      "2019-01-29T02:31:55.424974: step 4217, loss 0.0668084, acc 0.984375, learning_rate 0.000100064\n",
      "2019-01-29T02:31:55.552144: step 4218, loss 0.105156, acc 0.984375, learning_rate 0.000100064\n",
      "2019-01-29T02:31:55.678108: step 4219, loss 0.0619047, acc 1, learning_rate 0.000100064\n",
      "2019-01-29T02:31:55.806325: step 4220, loss 0.0728481, acc 0.984375, learning_rate 0.000100063\n",
      "2019-01-29T02:31:55.934571: step 4221, loss 0.0568617, acc 1, learning_rate 0.000100063\n",
      "2019-01-29T02:31:56.063931: step 4222, loss 0.0697042, acc 0.984375, learning_rate 0.000100063\n",
      "2019-01-29T02:31:56.154209: step 4223, loss 0.0618914, acc 1, learning_rate 0.000100063\n",
      "2019-01-29T02:31:56.284243: step 4224, loss 0.0609754, acc 1, learning_rate 0.000100063\n",
      "2019-01-29T02:31:56.416387: step 4225, loss 0.0605733, acc 1, learning_rate 0.000100063\n",
      "2019-01-29T02:31:56.542045: step 4226, loss 0.0594667, acc 1, learning_rate 0.000100062\n",
      "2019-01-29T02:31:56.664842: step 4227, loss 0.0846526, acc 0.984375, learning_rate 0.000100062\n",
      "2019-01-29T02:31:56.796445: step 4228, loss 0.0648387, acc 1, learning_rate 0.000100062\n",
      "2019-01-29T02:31:56.923644: step 4229, loss 0.0615714, acc 1, learning_rate 0.000100062\n",
      "2019-01-29T02:31:57.051290: step 4230, loss 0.0982453, acc 0.984375, learning_rate 0.000100062\n",
      "2019-01-29T02:31:57.142416: step 4231, loss 0.0720863, acc 1, learning_rate 0.000100062\n",
      "2019-01-29T02:31:57.265975: step 4232, loss 0.0730006, acc 1, learning_rate 0.000100061\n",
      "2019-01-29T02:31:57.389057: step 4233, loss 0.0627156, acc 1, learning_rate 0.000100061\n",
      "2019-01-29T02:31:57.520408: step 4234, loss 0.0657831, acc 0.984375, learning_rate 0.000100061\n",
      "2019-01-29T02:31:57.640798: step 4235, loss 0.0898238, acc 0.984375, learning_rate 0.000100061\n",
      "2019-01-29T02:31:57.763926: step 4236, loss 0.0500318, acc 1, learning_rate 0.000100061\n",
      "2019-01-29T02:31:57.889150: step 4237, loss 0.0759778, acc 0.984375, learning_rate 0.000100061\n",
      "2019-01-29T02:31:58.013574: step 4238, loss 0.0727853, acc 0.984375, learning_rate 0.00010006\n",
      "2019-01-29T02:31:58.141549: step 4239, loss 0.0736146, acc 1, learning_rate 0.00010006\n",
      "2019-01-29T02:31:58.228447: step 4240, loss 0.0552206, acc 1, learning_rate 0.00010006\n",
      "2019-01-29T02:31:58.352254: step 4241, loss 0.0623664, acc 1, learning_rate 0.00010006\n",
      "2019-01-29T02:31:58.479420: step 4242, loss 0.0634013, acc 1, learning_rate 0.00010006\n",
      "2019-01-29T02:31:58.604558: step 4243, loss 0.0610647, acc 1, learning_rate 0.00010006\n",
      "2019-01-29T02:31:58.729470: step 4244, loss 0.0771444, acc 1, learning_rate 0.000100059\n",
      "2019-01-29T02:31:58.846015: step 4245, loss 0.059253, acc 1, learning_rate 0.000100059\n",
      "2019-01-29T02:31:58.932387: step 4246, loss 0.0614782, acc 1, learning_rate 0.000100059\n",
      "2019-01-29T02:31:59.057102: step 4247, loss 0.0562261, acc 1, learning_rate 0.000100059\n",
      "2019-01-29T02:31:59.183578: step 4248, loss 0.0697691, acc 1, learning_rate 0.000100059\n",
      "2019-01-29T02:31:59.305306: step 4249, loss 0.061247, acc 1, learning_rate 0.000100059\n",
      "2019-01-29T02:31:59.394061: step 4250, loss 0.0567214, acc 1, learning_rate 0.000100059\n",
      "2019-01-29T02:31:59.520066: step 4251, loss 0.0669871, acc 1, learning_rate 0.000100058\n",
      "2019-01-29T02:31:59.648899: step 4252, loss 0.0975397, acc 0.984375, learning_rate 0.000100058\n",
      "2019-01-29T02:31:59.774738: step 4253, loss 0.0677472, acc 0.984375, learning_rate 0.000100058\n",
      "2019-01-29T02:31:59.892407: step 4254, loss 0.0805906, acc 0.984375, learning_rate 0.000100058\n",
      "2019-01-29T02:32:00.013910: step 4255, loss 0.0576455, acc 1, learning_rate 0.000100058\n",
      "2019-01-29T02:32:00.136652: step 4256, loss 0.0546442, acc 1, learning_rate 0.000100058\n",
      "2019-01-29T02:32:00.261714: step 4257, loss 0.0549229, acc 1, learning_rate 0.000100057\n",
      "2019-01-29T02:32:00.344658: step 4258, loss 0.0839039, acc 0.984375, learning_rate 0.000100057\n",
      "2019-01-29T02:32:00.467741: step 4259, loss 0.0605663, acc 1, learning_rate 0.000100057\n",
      "2019-01-29T02:32:00.597153: step 4260, loss 0.0700434, acc 1, learning_rate 0.000100057\n",
      "2019-01-29T02:32:00.723942: step 4261, loss 0.0609031, acc 1, learning_rate 0.000100057\n",
      "2019-01-29T02:32:00.847561: step 4262, loss 0.0512077, acc 1, learning_rate 0.000100057\n",
      "2019-01-29T02:32:00.975353: step 4263, loss 0.0526032, acc 1, learning_rate 0.000100057\n",
      "2019-01-29T02:32:01.102001: step 4264, loss 0.0579132, acc 1, learning_rate 0.000100056\n",
      "2019-01-29T02:32:01.226282: step 4265, loss 0.0597728, acc 1, learning_rate 0.000100056\n",
      "2019-01-29T02:32:01.306760: step 4266, loss 0.0540451, acc 1, learning_rate 0.000100056\n",
      "2019-01-29T02:32:01.432035: step 4267, loss 0.0625052, acc 1, learning_rate 0.000100056\n",
      "2019-01-29T02:32:01.554130: step 4268, loss 0.0771754, acc 0.984375, learning_rate 0.000100056\n",
      "2019-01-29T02:32:01.640906: step 4269, loss 0.0677561, acc 1, learning_rate 0.000100056\n",
      "2019-01-29T02:32:01.761543: step 4270, loss 0.0733986, acc 1, learning_rate 0.000100055\n",
      "2019-01-29T02:32:01.861335: step 4271, loss 0.0752837, acc 0.96875, learning_rate 0.000100055\n",
      "2019-01-29T02:32:01.989224: step 4272, loss 0.122176, acc 0.96875, learning_rate 0.000100055\n",
      "2019-01-29T02:32:02.117233: step 4273, loss 0.0645599, acc 1, learning_rate 0.000100055\n",
      "2019-01-29T02:32:02.247134: step 4274, loss 0.0705017, acc 1, learning_rate 0.000100055\n",
      "2019-01-29T02:32:02.373959: step 4275, loss 0.0706845, acc 1, learning_rate 0.000100055\n",
      "2019-01-29T02:32:02.496538: step 4276, loss 0.056263, acc 1, learning_rate 0.000100055\n",
      "2019-01-29T02:32:02.628612: step 4277, loss 0.0880267, acc 0.96875, learning_rate 0.000100054\n",
      "2019-01-29T02:32:02.749262: step 4278, loss 0.0608169, acc 1, learning_rate 0.000100054\n",
      "2019-01-29T02:32:02.872204: step 4279, loss 0.0842468, acc 1, learning_rate 0.000100054\n",
      "2019-01-29T02:32:02.999456: step 4280, loss 0.0514784, acc 1, learning_rate 0.000100054\n",
      "2019-01-29T02:32:03.117020: step 4281, loss 0.0765214, acc 1, learning_rate 0.000100054\n",
      "2019-01-29T02:32:03.246516: step 4282, loss 0.0913066, acc 1, learning_rate 0.000100054\n",
      "2019-01-29T02:32:03.371313: step 4283, loss 0.0655366, acc 1, learning_rate 0.000100054\n",
      "2019-01-29T02:32:03.495695: step 4284, loss 0.0536072, acc 1, learning_rate 0.000100053\n",
      "2019-01-29T02:32:03.577802: step 4285, loss 0.0725018, acc 0.984375, learning_rate 0.000100053\n",
      "2019-01-29T02:32:03.701860: step 4286, loss 0.065586, acc 1, learning_rate 0.000100053\n",
      "2019-01-29T02:32:03.825693: step 4287, loss 0.0665411, acc 1, learning_rate 0.000100053\n",
      "2019-01-29T02:32:03.954160: step 4288, loss 0.0632246, acc 1, learning_rate 0.000100053\n",
      "2019-01-29T02:32:04.080513: step 4289, loss 0.0561882, acc 1, learning_rate 0.000100053\n",
      "2019-01-29T02:32:04.201922: step 4290, loss 0.0708274, acc 1, learning_rate 0.000100053\n",
      "2019-01-29T02:32:04.327542: step 4291, loss 0.0607926, acc 1, learning_rate 0.000100052\n",
      "2019-01-29T02:32:04.417036: step 4292, loss 0.05302, acc 1, learning_rate 0.000100052\n",
      "2019-01-29T02:32:04.543351: step 4293, loss 0.0555735, acc 1, learning_rate 0.000100052\n",
      "2019-01-29T02:32:04.663319: step 4294, loss 0.073167, acc 1, learning_rate 0.000100052\n",
      "2019-01-29T02:32:04.790199: step 4295, loss 0.0642769, acc 1, learning_rate 0.000100052\n",
      "2019-01-29T02:32:04.918650: step 4296, loss 0.0586307, acc 1, learning_rate 0.000100052\n",
      "2019-01-29T02:32:05.046292: step 4297, loss 0.0641831, acc 1, learning_rate 0.000100052\n",
      "2019-01-29T02:32:05.134026: step 4298, loss 0.051457, acc 1, learning_rate 0.000100051\n",
      "2019-01-29T02:32:05.258422: step 4299, loss 0.0641498, acc 1, learning_rate 0.000100051\n",
      "2019-01-29T02:32:05.383738: step 4300, loss 0.094007, acc 0.96875, learning_rate 0.000100051\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:32:05.410445: step 4300, loss 0.67082, acc 0.751407\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-4300\n",
      "\n",
      "2019-01-29T02:32:05.773119: step 4301, loss 0.0597152, acc 1, learning_rate 0.000100051\n",
      "2019-01-29T02:32:05.900249: step 4302, loss 0.0615163, acc 1, learning_rate 0.000100051\n",
      "2019-01-29T02:32:05.986654: step 4303, loss 0.0703459, acc 1, learning_rate 0.000100051\n",
      "2019-01-29T02:32:06.072722: step 4304, loss 0.0569517, acc 1, learning_rate 0.000100051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:32:06.198197: step 4305, loss 0.0618779, acc 1, learning_rate 0.000100051\n",
      "2019-01-29T02:32:06.326504: step 4306, loss 0.0570975, acc 1, learning_rate 0.00010005\n",
      "2019-01-29T02:32:06.416420: step 4307, loss 0.0649861, acc 1, learning_rate 0.00010005\n",
      "2019-01-29T02:32:06.549168: step 4308, loss 0.0792641, acc 0.984375, learning_rate 0.00010005\n",
      "2019-01-29T02:32:06.636762: step 4309, loss 0.0595093, acc 1, learning_rate 0.00010005\n",
      "2019-01-29T02:32:06.759985: step 4310, loss 0.0891253, acc 0.984375, learning_rate 0.00010005\n",
      "2019-01-29T02:32:06.848701: step 4311, loss 0.070132, acc 1, learning_rate 0.00010005\n",
      "2019-01-29T02:32:06.976228: step 4312, loss 0.0907589, acc 0.984375, learning_rate 0.00010005\n",
      "2019-01-29T02:32:07.059621: step 4313, loss 0.0792364, acc 1, learning_rate 0.000100049\n",
      "2019-01-29T02:32:07.185171: step 4314, loss 0.0670692, acc 1, learning_rate 0.000100049\n",
      "2019-01-29T02:32:07.306476: step 4315, loss 0.0586087, acc 1, learning_rate 0.000100049\n",
      "2019-01-29T02:32:07.438496: step 4316, loss 0.0869223, acc 0.984375, learning_rate 0.000100049\n",
      "2019-01-29T02:32:07.567053: step 4317, loss 0.0610283, acc 1, learning_rate 0.000100049\n",
      "2019-01-29T02:32:07.655491: step 4318, loss 0.0674445, acc 1, learning_rate 0.000100049\n",
      "2019-01-29T02:32:07.778680: step 4319, loss 0.050242, acc 1, learning_rate 0.000100049\n",
      "2019-01-29T02:32:07.904105: step 4320, loss 0.0742292, acc 0.984375, learning_rate 0.000100049\n",
      "2019-01-29T02:32:08.031422: step 4321, loss 0.0567673, acc 1, learning_rate 0.000100048\n",
      "2019-01-29T02:32:08.161995: step 4322, loss 0.0673677, acc 1, learning_rate 0.000100048\n",
      "2019-01-29T02:32:08.291407: step 4323, loss 0.0585041, acc 1, learning_rate 0.000100048\n",
      "2019-01-29T02:32:08.411499: step 4324, loss 0.0553601, acc 1, learning_rate 0.000100048\n",
      "2019-01-29T02:32:08.535491: step 4325, loss 0.0614777, acc 1, learning_rate 0.000100048\n",
      "2019-01-29T02:32:08.657773: step 4326, loss 0.0571932, acc 1, learning_rate 0.000100048\n",
      "2019-01-29T02:32:08.780989: step 4327, loss 0.0498124, acc 1, learning_rate 0.000100048\n",
      "2019-01-29T02:32:08.907923: step 4328, loss 0.0675566, acc 1, learning_rate 0.000100048\n",
      "2019-01-29T02:32:08.998621: step 4329, loss 0.0693938, acc 1, learning_rate 0.000100047\n",
      "2019-01-29T02:32:09.121989: step 4330, loss 0.0611599, acc 1, learning_rate 0.000100047\n",
      "2019-01-29T02:32:09.246179: step 4331, loss 0.0573545, acc 1, learning_rate 0.000100047\n",
      "2019-01-29T02:32:09.367143: step 4332, loss 0.083344, acc 0.984375, learning_rate 0.000100047\n",
      "2019-01-29T02:32:09.452665: step 4333, loss 0.059862, acc 1, learning_rate 0.000100047\n",
      "2019-01-29T02:32:09.574083: step 4334, loss 0.0631711, acc 1, learning_rate 0.000100047\n",
      "2019-01-29T02:32:09.664132: step 4335, loss 0.0715449, acc 1, learning_rate 0.000100047\n",
      "2019-01-29T02:32:09.796623: step 4336, loss 0.0677574, acc 1, learning_rate 0.000100047\n",
      "2019-01-29T02:32:09.922425: step 4337, loss 0.058636, acc 1, learning_rate 0.000100046\n",
      "2019-01-29T02:32:10.044448: step 4338, loss 0.0501596, acc 1, learning_rate 0.000100046\n",
      "2019-01-29T02:32:10.164897: step 4339, loss 0.0547044, acc 1, learning_rate 0.000100046\n",
      "2019-01-29T02:32:10.286069: step 4340, loss 0.0650348, acc 1, learning_rate 0.000100046\n",
      "2019-01-29T02:32:10.410253: step 4341, loss 0.0645337, acc 1, learning_rate 0.000100046\n",
      "2019-01-29T02:32:10.536696: step 4342, loss 0.0577079, acc 1, learning_rate 0.000100046\n",
      "2019-01-29T02:32:10.668375: step 4343, loss 0.0856604, acc 1, learning_rate 0.000100046\n",
      "2019-01-29T02:32:10.793455: step 4344, loss 0.052811, acc 1, learning_rate 0.000100046\n",
      "2019-01-29T02:32:10.914081: step 4345, loss 0.0648363, acc 1, learning_rate 0.000100045\n",
      "2019-01-29T02:32:11.041009: step 4346, loss 0.0580861, acc 1, learning_rate 0.000100045\n",
      "2019-01-29T02:32:11.129299: step 4347, loss 0.0757971, acc 1, learning_rate 0.000100045\n",
      "2019-01-29T02:32:11.258709: step 4348, loss 0.0712959, acc 1, learning_rate 0.000100045\n",
      "2019-01-29T02:32:11.383213: step 4349, loss 0.0599636, acc 1, learning_rate 0.000100045\n",
      "2019-01-29T02:32:11.471887: step 4350, loss 0.0777504, acc 0.983333, learning_rate 0.000100045\n",
      "2019-01-29T02:32:11.589315: step 4351, loss 0.0508227, acc 1, learning_rate 0.000100045\n",
      "2019-01-29T02:32:11.673282: step 4352, loss 0.0567508, acc 1, learning_rate 0.000100045\n",
      "2019-01-29T02:32:11.801320: step 4353, loss 0.0608203, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:11.923451: step 4354, loss 0.067313, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:12.047178: step 4355, loss 0.0607777, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:12.176865: step 4356, loss 0.0556333, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:12.280818: step 4357, loss 0.0583422, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:12.405237: step 4358, loss 0.0620461, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:12.529342: step 4359, loss 0.0718648, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:12.649095: step 4360, loss 0.0655523, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:12.736946: step 4361, loss 0.0559903, acc 1, learning_rate 0.000100044\n",
      "2019-01-29T02:32:12.857608: step 4362, loss 0.0695639, acc 1, learning_rate 0.000100043\n",
      "2019-01-29T02:32:12.966314: step 4363, loss 0.0743563, acc 0.984375, learning_rate 0.000100043\n",
      "2019-01-29T02:32:13.093392: step 4364, loss 0.0513859, acc 1, learning_rate 0.000100043\n",
      "2019-01-29T02:32:13.223692: step 4365, loss 0.0720708, acc 1, learning_rate 0.000100043\n",
      "2019-01-29T02:32:13.352023: step 4366, loss 0.0488331, acc 1, learning_rate 0.000100043\n",
      "2019-01-29T02:32:13.475067: step 4367, loss 0.0591792, acc 1, learning_rate 0.000100043\n",
      "2019-01-29T02:32:13.603058: step 4368, loss 0.0483985, acc 1, learning_rate 0.000100043\n",
      "2019-01-29T02:32:13.736031: step 4369, loss 0.0648693, acc 1, learning_rate 0.000100043\n",
      "2019-01-29T02:32:13.863658: step 4370, loss 0.100179, acc 0.984375, learning_rate 0.000100042\n",
      "2019-01-29T02:32:13.989777: step 4371, loss 0.0539467, acc 1, learning_rate 0.000100042\n",
      "2019-01-29T02:32:14.071669: step 4372, loss 0.0859354, acc 0.984375, learning_rate 0.000100042\n",
      "2019-01-29T02:32:14.192821: step 4373, loss 0.0581548, acc 1, learning_rate 0.000100042\n",
      "2019-01-29T02:32:14.321366: step 4374, loss 0.0632232, acc 1, learning_rate 0.000100042\n",
      "2019-01-29T02:32:14.408925: step 4375, loss 0.0645248, acc 1, learning_rate 0.000100042\n",
      "2019-01-29T02:32:14.497789: step 4376, loss 0.0675215, acc 0.984375, learning_rate 0.000100042\n",
      "2019-01-29T02:32:14.622884: step 4377, loss 0.0583396, acc 1, learning_rate 0.000100042\n",
      "2019-01-29T02:32:14.750921: step 4378, loss 0.0638541, acc 0.984375, learning_rate 0.000100042\n",
      "2019-01-29T02:32:14.871087: step 4379, loss 0.0853563, acc 0.984375, learning_rate 0.000100041\n",
      "2019-01-29T02:32:14.960163: step 4380, loss 0.0689744, acc 1, learning_rate 0.000100041\n",
      "2019-01-29T02:32:15.085263: step 4381, loss 0.0675072, acc 1, learning_rate 0.000100041\n",
      "2019-01-29T02:32:15.212983: step 4382, loss 0.0530946, acc 1, learning_rate 0.000100041\n",
      "2019-01-29T02:32:15.329763: step 4383, loss 0.0788568, acc 1, learning_rate 0.000100041\n",
      "2019-01-29T02:32:15.457650: step 4384, loss 0.0548295, acc 1, learning_rate 0.000100041\n",
      "2019-01-29T02:32:15.585665: step 4385, loss 0.0672081, acc 0.984375, learning_rate 0.000100041\n",
      "2019-01-29T02:32:15.708365: step 4386, loss 0.0633021, acc 0.984375, learning_rate 0.000100041\n",
      "2019-01-29T02:32:15.838773: step 4387, loss 0.0698063, acc 1, learning_rate 0.000100041\n",
      "2019-01-29T02:32:15.926278: step 4388, loss 0.068764, acc 1, learning_rate 0.00010004\n",
      "2019-01-29T02:32:16.048914: step 4389, loss 0.0557353, acc 1, learning_rate 0.00010004\n",
      "2019-01-29T02:32:16.174723: step 4390, loss 0.0698793, acc 1, learning_rate 0.00010004\n",
      "2019-01-29T02:32:16.265906: step 4391, loss 0.0700272, acc 1, learning_rate 0.00010004\n",
      "2019-01-29T02:32:16.388120: step 4392, loss 0.0748729, acc 0.984375, learning_rate 0.00010004\n",
      "2019-01-29T02:32:16.519634: step 4393, loss 0.0515518, acc 1, learning_rate 0.00010004\n",
      "2019-01-29T02:32:16.642112: step 4394, loss 0.0678568, acc 1, learning_rate 0.00010004\n",
      "2019-01-29T02:32:16.765075: step 4395, loss 0.0694869, acc 1, learning_rate 0.00010004\n",
      "2019-01-29T02:32:16.891160: step 4396, loss 0.0858781, acc 0.984375, learning_rate 0.00010004\n",
      "2019-01-29T02:32:17.020545: step 4397, loss 0.0611739, acc 1, learning_rate 0.00010004\n",
      "2019-01-29T02:32:17.107793: step 4398, loss 0.0683242, acc 1, learning_rate 0.000100039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:32:17.235912: step 4399, loss 0.0514672, acc 1, learning_rate 0.000100039\n",
      "2019-01-29T02:32:17.361660: step 4400, loss 0.0863845, acc 1, learning_rate 0.000100039\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:32:17.388711: step 4400, loss 0.67599, acc 0.752345\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-4400\n",
      "\n",
      "2019-01-29T02:32:17.725637: step 4401, loss 0.0670217, acc 1, learning_rate 0.000100039\n",
      "2019-01-29T02:32:17.815410: step 4402, loss 0.0513977, acc 1, learning_rate 0.000100039\n",
      "2019-01-29T02:32:17.939460: step 4403, loss 0.0596236, acc 1, learning_rate 0.000100039\n",
      "2019-01-29T02:32:18.071654: step 4404, loss 0.0924939, acc 0.984375, learning_rate 0.000100039\n",
      "2019-01-29T02:32:18.198876: step 4405, loss 0.0627389, acc 1, learning_rate 0.000100039\n",
      "2019-01-29T02:32:18.321845: step 4406, loss 0.0733346, acc 1, learning_rate 0.000100039\n",
      "2019-01-29T02:32:18.442687: step 4407, loss 0.0792355, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:18.569683: step 4408, loss 0.0762492, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:18.697272: step 4409, loss 0.0547124, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:18.832005: step 4410, loss 0.0789781, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:18.921026: step 4411, loss 0.0626141, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:19.047233: step 4412, loss 0.0604685, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:19.174491: step 4413, loss 0.0664544, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:19.297514: step 4414, loss 0.0708273, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:19.415643: step 4415, loss 0.0593124, acc 1, learning_rate 0.000100038\n",
      "2019-01-29T02:32:19.538689: step 4416, loss 0.0806513, acc 0.96875, learning_rate 0.000100038\n",
      "2019-01-29T02:32:19.625218: step 4417, loss 0.0571055, acc 1, learning_rate 0.000100037\n",
      "2019-01-29T02:32:19.743606: step 4418, loss 0.0608784, acc 1, learning_rate 0.000100037\n",
      "2019-01-29T02:32:19.872396: step 4419, loss 0.0631325, acc 1, learning_rate 0.000100037\n",
      "2019-01-29T02:32:19.996598: step 4420, loss 0.0627277, acc 1, learning_rate 0.000100037\n",
      "2019-01-29T02:32:20.122883: step 4421, loss 0.0610037, acc 1, learning_rate 0.000100037\n",
      "2019-01-29T02:32:20.250566: step 4422, loss 0.0718528, acc 1, learning_rate 0.000100037\n",
      "2019-01-29T02:32:20.378831: step 4423, loss 0.0672105, acc 0.984375, learning_rate 0.000100037\n",
      "2019-01-29T02:32:20.507950: step 4424, loss 0.057444, acc 1, learning_rate 0.000100037\n",
      "2019-01-29T02:32:20.630042: step 4425, loss 0.0668492, acc 1, learning_rate 0.000100037\n",
      "2019-01-29T02:32:20.752070: step 4426, loss 0.0779179, acc 0.984375, learning_rate 0.000100037\n",
      "2019-01-29T02:32:20.845607: step 4427, loss 0.0562553, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:20.969119: step 4428, loss 0.0671031, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:21.095246: step 4429, loss 0.0509008, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:21.217761: step 4430, loss 0.0735396, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:21.305495: step 4431, loss 0.0727305, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:21.432763: step 4432, loss 0.0719377, acc 0.984375, learning_rate 0.000100036\n",
      "2019-01-29T02:32:21.562252: step 4433, loss 0.0564443, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:21.693367: step 4434, loss 0.0682088, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:21.824159: step 4435, loss 0.0566447, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:21.956252: step 4436, loss 0.0621417, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:22.085638: step 4437, loss 0.0629724, acc 1, learning_rate 0.000100036\n",
      "2019-01-29T02:32:22.209073: step 4438, loss 0.0626077, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:22.329821: step 4439, loss 0.0780681, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:22.449153: step 4440, loss 0.0626361, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:22.556102: step 4441, loss 0.0648887, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:22.685022: step 4442, loss 0.0574303, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:22.812254: step 4443, loss 0.0544648, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:22.942303: step 4444, loss 0.0558737, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:23.047864: step 4445, loss 0.063432, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:23.171894: step 4446, loss 0.0765561, acc 0.96875, learning_rate 0.000100035\n",
      "2019-01-29T02:32:23.301263: step 4447, loss 0.0636153, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:23.428291: step 4448, loss 0.0551496, acc 1, learning_rate 0.000100035\n",
      "2019-01-29T02:32:23.560044: step 4449, loss 0.0671204, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:23.687240: step 4450, loss 0.0888269, acc 0.984375, learning_rate 0.000100034\n",
      "2019-01-29T02:32:23.814462: step 4451, loss 0.0591459, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:23.899746: step 4452, loss 0.0643976, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:24.024799: step 4453, loss 0.0531784, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:24.123205: step 4454, loss 0.0598948, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:24.254439: step 4455, loss 0.054386, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:24.341826: step 4456, loss 0.0543954, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:24.469063: step 4457, loss 0.0854186, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:24.590915: step 4458, loss 0.0671966, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:24.675311: step 4459, loss 0.0698828, acc 1, learning_rate 0.000100034\n",
      "2019-01-29T02:32:24.801225: step 4460, loss 0.0802588, acc 0.984375, learning_rate 0.000100033\n",
      "2019-01-29T02:32:24.927100: step 4461, loss 0.0722001, acc 1, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.047484: step 4462, loss 0.0926418, acc 0.984375, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.171844: step 4463, loss 0.0681461, acc 1, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.296343: step 4464, loss 0.0880895, acc 1, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.420297: step 4465, loss 0.0658111, acc 1, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.506334: step 4466, loss 0.0731866, acc 1, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.629351: step 4467, loss 0.0735555, acc 1, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.732304: step 4468, loss 0.0552013, acc 1, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.861474: step 4469, loss 0.056193, acc 1, learning_rate 0.000100033\n",
      "2019-01-29T02:32:25.985002: step 4470, loss 0.10149, acc 0.96875, learning_rate 0.000100033\n",
      "2019-01-29T02:32:26.077746: step 4471, loss 0.0660208, acc 0.984375, learning_rate 0.000100032\n",
      "2019-01-29T02:32:26.204765: step 4472, loss 0.0543488, acc 1, learning_rate 0.000100032\n",
      "2019-01-29T02:32:26.334976: step 4473, loss 0.061528, acc 1, learning_rate 0.000100032\n",
      "2019-01-29T02:32:26.422607: step 4474, loss 0.0551412, acc 1, learning_rate 0.000100032\n",
      "2019-01-29T02:32:26.551998: step 4475, loss 0.06564, acc 1, learning_rate 0.000100032\n",
      "2019-01-29T02:32:26.674807: step 4476, loss 0.0600164, acc 1, learning_rate 0.000100032\n",
      "2019-01-29T02:32:26.805938: step 4477, loss 0.0811573, acc 0.984375, learning_rate 0.000100032\n",
      "2019-01-29T02:32:26.929494: step 4478, loss 0.0526791, acc 1, learning_rate 0.000100032\n",
      "2019-01-29T02:32:27.051995: step 4479, loss 0.0917025, acc 0.984375, learning_rate 0.000100032\n",
      "2019-01-29T02:32:27.176672: step 4480, loss 0.0618936, acc 0.984375, learning_rate 0.000100032\n",
      "2019-01-29T02:32:27.301611: step 4481, loss 0.0564057, acc 1, learning_rate 0.000100032\n",
      "2019-01-29T02:32:27.431229: step 4482, loss 0.0590955, acc 1, learning_rate 0.000100032\n",
      "2019-01-29T02:32:27.559011: step 4483, loss 0.0546067, acc 1, learning_rate 0.000100031\n",
      "2019-01-29T02:32:27.691123: step 4484, loss 0.0651595, acc 1, learning_rate 0.000100031\n",
      "2019-01-29T02:32:27.777467: step 4485, loss 0.0643387, acc 1, learning_rate 0.000100031\n",
      "2019-01-29T02:32:27.905650: step 4486, loss 0.0588535, acc 1, learning_rate 0.000100031\n",
      "2019-01-29T02:32:28.034397: step 4487, loss 0.0585273, acc 1, learning_rate 0.000100031\n",
      "2019-01-29T02:32:28.162429: step 4488, loss 0.0516339, acc 1, learning_rate 0.000100031\n",
      "2019-01-29T02:32:28.289658: step 4489, loss 0.0567935, acc 1, learning_rate 0.000100031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-29T02:32:28.377602: step 4490, loss 0.0711091, acc 1, learning_rate 0.000100031\n",
      "2019-01-29T02:32:28.507523: step 4491, loss 0.0712087, acc 0.984375, learning_rate 0.000100031\n",
      "2019-01-29T02:32:28.597422: step 4492, loss 0.0629182, acc 0.984375, learning_rate 0.000100031\n",
      "2019-01-29T02:32:28.713575: step 4493, loss 0.0561833, acc 1, learning_rate 0.000100031\n",
      "2019-01-29T02:32:28.843968: step 4494, loss 0.0701849, acc 0.984375, learning_rate 0.000100031\n",
      "2019-01-29T02:32:28.969829: step 4495, loss 0.0693953, acc 0.984375, learning_rate 0.00010003\n",
      "2019-01-29T02:32:29.094083: step 4496, loss 0.0672024, acc 1, learning_rate 0.00010003\n",
      "2019-01-29T02:32:29.221711: step 4497, loss 0.0992232, acc 0.984375, learning_rate 0.00010003\n",
      "2019-01-29T02:32:29.349947: step 4498, loss 0.0529118, acc 1, learning_rate 0.00010003\n",
      "2019-01-29T02:32:29.477980: step 4499, loss 0.0814118, acc 0.984375, learning_rate 0.00010003\n",
      "2019-01-29T02:32:29.605040: step 4500, loss 0.0692148, acc 0.983333, learning_rate 0.00010003\n",
      "\n",
      "Evaluation:\n",
      "2019-01-29T02:32:29.631731: step 4500, loss 0.670814, acc 0.750469\n",
      "\n",
      "Saved model checkpoint to /root/My_Text_Classification_v3/runs/1548728574/checkpoints/model-4500\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py:2969: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
