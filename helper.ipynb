{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string, TREC=False):\n",
    "    '''\n",
    "    Create tokens, to lower case for all datasets but SST\n",
    "    '''\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
    "    string = re.sub(r\",\", \" , \", string) \n",
    "    string = re.sub(r\"!\", \" ! \", string) \n",
    "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
    "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
    "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    #Remove leading and trailing white spaces.\n",
    "    return string.strip() if TREC else string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_20newsgroup(subset='train', categories=None, shuffle=True, random_state=42):\n",
    "    \"\"\"\n",
    "    Retrieve data from 20 newsgroups\n",
    "    :param subset: train, test or all\n",
    "    :param categories: List of newsgroup name\n",
    "    :param shuffle: shuffle the list or not\n",
    "    :param random_state: seed integer to shuffle the dataset\n",
    "    :return: data and labels of the newsgroup\n",
    "    \"\"\"\n",
    "    datasets = fetch_20newsgroups(subset=subset, categories=categories, shuffle=shuffle, random_state=random_state)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_mrpolarity(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    \n",
    "    datasets = dict()\n",
    "    datasets['data'] = positive_examples + negative_examples\n",
    "    target = [0 for x in positive_examples] + [1 for x in negative_examples]\n",
    "    datasets['target'] = target\n",
    "    datasets['target_names'] = ['positive_examples', 'negative_examples']\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_localdata(container_path=None, categories=None, load_content=True,\n",
    "                       encoding='utf-8', shuffle=True, random_state=42):\n",
    "    \"\"\"\n",
    "    Load text files with categories as subfolder names.\n",
    "    Individual samples are assumed to be files stored a two levels folder structure.\n",
    "    :param container_path: The path of the container\n",
    "    :param categories: List of classes to choose, all classes are chosen by default (if empty or omitted)\n",
    "    :param shuffle: shuffle the list or not\n",
    "    :param random_state: seed integer to shuffle the dataset\n",
    "    :return: data and labels of the dataset\n",
    "    \"\"\"\n",
    "    datasets = load_files(container_path=container_path, categories=categories,\n",
    "                          load_content=load_content, shuffle=shuffle, encoding=encoding,\n",
    "                          random_state=random_state)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_labels(datasets):\n",
    "    \"\"\"\n",
    "    Load data and labels\n",
    "    :param datasets:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Split by words\n",
    "    x_text = datasets['data']\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    labels = []\n",
    "    for i in range(len(x_text)):\n",
    "        label = [0 for j in datasets['target_names']]\n",
    "        label[datasets['target'][i]] = 1\n",
    "        labels.append(label)\n",
    "    y = np.array(labels)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_vectors_word2vec(vocabulary, filename, binary):\n",
    "    # load embedding_vectors from the word2vec\n",
    "    encoding = 'utf-8'\n",
    "    with open(filename, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, vector_size = map(int, header.split())\n",
    "        # initial matrix with random uniform\n",
    "        embedding_vectors = np.random.uniform(-0.25, 0.25, (len(vocabulary), vector_size))\n",
    "        if binary:\n",
    "            binary_len = np.dtype('float32').itemsize * vector_size\n",
    "            for line_no in range(vocab_size):\n",
    "                word = []\n",
    "                while True:\n",
    "                    ch = f.read(1)\n",
    "                    if ch == b' ':\n",
    "                        break\n",
    "                    if ch == b'':\n",
    "                        raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "                    if ch != b'\\n':\n",
    "                        word.append(ch)\n",
    "                word = str(b''.join(word), encoding=encoding, errors='strict')\n",
    "                idx = vocabulary.get(word)\n",
    "                if idx != 0:\n",
    "                    embedding_vectors[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                else:\n",
    "                    f.seek(binary_len, 1)\n",
    "        else:\n",
    "            for line_no in range(vocab_size):\n",
    "                line = f.readline()\n",
    "                if line == b'':\n",
    "                    raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "                parts = str(line.rstrip(), encoding=encoding, errors='strict').split(\" \")\n",
    "                if len(parts) != vector_size + 1:\n",
    "                    raise ValueError(\"invalid vector on line %s (is this really the text format?)\" % (line_no))\n",
    "                word, vector = parts[0], list(map('float32', parts[1:]))\n",
    "                idx = vocabulary.get(word)\n",
    "                if idx != 0:\n",
    "                    embedding_vectors[idx] = vector\n",
    "        f.close()\n",
    "        return embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_vectors_glove(vocabulary, filename, vector_size):\n",
    "    # load embedding_vectors from the glove\n",
    "    # initial matrix with random uniform\n",
    "    embedding_vectors = np.random.uniform(-0.25, 0.25, (len(vocabulary), vector_size))\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        idx = vocabulary.get(word)\n",
    "        if idx != 0:\n",
    "            embedding_vectors[idx] = vector\n",
    "    f.close()\n",
    "    return embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str_sst(string):\n",
    "    '''\n",
    "    Create tokens, string cleaning for Stanford Sentiment Treebank (SST)\n",
    "    '''\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)   \n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
    "    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    pos_samples = [line.strip() for line in open(positive_data_file,'r', encoding='utf-8')]\n",
    "    neg_samples = [line.strip() for line in open(negative_data_file,'r', encoding='utf-8')]\n",
    "    \n",
    "    # Split by words\n",
    "    text = pos_samples + neg_samples\n",
    "    text = [clean_str(sentence) for sentence in text]\n",
    "    \n",
    "    # Generate labels\n",
    "    pos_labels = [[0, 1] for _ in pos_samples]\n",
    "    neg_labels = [[1, 0] for _ in neg_samples]\n",
    "    labels = np.concatenate([pos_labels, neg_labels], axis=0)\n",
    "    \n",
    "    return [text, labels]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
